{
    "docs": [
        {
            "location": "/index.html",
            "text": "In this lab we introduce the most interesting features of Oracle Solaris 11,\nbased on real life use cases. We will:\n\n\n\n\ncreate a disk pool and a file system with ZFS; expand it; snapshot and\n    clone it; use ZFS compression and deduplication; use ZFS migration for\n    backup: \nZFS Lab\n\n\ncreate a new boot environment as a backup, make our current system\n    unbootable because of some fatal mistakes, reboot the system using\n    backup BE: \nBoot Environments Lab\n\n\nuse new packaging system called IPS to search, install, verify and\n    fix packages; update the system using IPS and Boot Environments\n    together: \nIPS Lab\n\n\nuse new networking commands, configure virtual network interfaces: \nNetworking Lab\n\n\ncreate a couple of Solaris virtual environments (zones); install some\n    applications into them; clone a zone; use Resource Management with the zones: \nVirtualization Lab\n\n\nlearn how to install systems and zones with Automated Installer: \nAI Lab\n\n\ncompare two virtualization options available in Oracle Solaris: non-global\n    zones and kernel zones: \nKernel Zones Lab\n \n\n\n\n\nPrerequisites\n\n\nThis lab requires access to an Oracle Solaris 11 system either natively or in a\nVirtualBox virtual machine.\n\n\nUsing Virtual Box VM\n\n\nIf you're performing this lab on your laptop or desktop, we've provided\na VirtualBox appliance that contains all the software and configuration\nnecessary to complete this lab. All you need is VirtualBox software and\na modern (with a CPU supporting virtualization turned on, AMD-V or VT-x)\nlaptop/desktop with at least 3 GB of RAM and 10GB of free disk space.\n\n\n\n\nDownload and install the latest version of VirtualBox for your\n    platform (\nhttp://www.virtualbox.org/wiki/Downloads\n).\n\n\nDownload and install the latest version of VirtualBox Extensions\n    Pack (\nhttp://www.virtualbox.org/wiki/Downloads\n).\n\n\nDownload or find on the provided DVD/USB and import the Hands On Lab\n    machine into VirtualBox (File \n Import Appliance). You will have\n    to accept the OTN Oracle Solaris license to use the appliance.\n\n\n\n\nThe Environment\n\n\nIn this lab we are going to use Oracle Solaris 11 virtual appliance in\nOracle VirtualBox environment. If you are using lab machines, the\nappliance is already installed. You can also download the appliance from\nOracle Technology Network: (\nhttp://www.oracle.com/technetwork/server-storage/solaris11/downloads/vm-templates-2245495.html\n)\n\n\nBy default, VirtualBox assigns the IP address \n10.0.2.15\n to the Solaris\nglobal zone. We will be using also IP addresses \n10.0.2.21\n and \n10.0.2.22\n\nfor local zones. As we are using VirtualBox in NAT (network address\ntranslation) mode, this shouldn't interfere with your outside network\nenvironment.\n\n\nYou should login into Solaris desktop with the following credentials:\n\n\nUsername: \nlab\n Password: \noracle1\n\n\nAfter logging in, open a terminal window and assume the \nroot\n role:\n\n\nlab@solaris:~$ su - \n\n\n\nPassword for root is: \nsolaris1\n.\n\n\nNote: we don't recommend to log in as \nroot\n. In Oracle Solaris 11 it is\nprohibited by default; \nroot\n is only a role, not a login name.\n\n\nPutting It All Together\n\n\nThe whole idea of this lab is to show you some Oracle Solaris 11 features that\ncan be used to create a cloud infrastructure based on Solaris. You have\njust created storage pools and filesystems\nthink cloud storage. It\nwas fast, it was simple, it was flexible. You have created and cloned\nSolaris zones with applications within them\nthink \ncloud machine\ninstances\n. You have monitored and managed zones resources\nthink \ncloud\nelasticity, metering and chargeback\n. Add to that \nSolaris \nnetwork virtualization\n, Solaris \nsecurity\n, Solaris \nsoftware lifecycle\nmanagement\n and many other features which make Oracle Solaris 11 truly\ncloud-oriented operating system. Try them and learn more about Solaris 11!\n\n\nFinal Notes\n\n\nThe virtual appliance we used in this lab is configured to be able to\nperform zone installation without network access. Namely, we've\nconfigured an internal repository with just a small subset of packages\nnecessary for zone installations. If you are going to continue using\nthis appliance with open network access, you will need to change the\nrepository address to Oracle's standard Solaris repository.\n\n\nroot@solaris:~# pkg set-publisher -G '*' -M '*' -g http://pkg.oracle.com/solaris/release -P solaris \n\n\n\nFurther Oracle Solaris Education\n\n\nThis Hands-on Lab is just and introduction in Oracle Solaris 11 world.\nWe highly recommend to continue your education with Oracle University.\nThere is a full set of new courses covering Oracle Solaris 11:\n\n\n\n\nTransition to Oracle Solaris 11\n\n\nWhat's New in Oracle Solaris 11\n\n\nWhat's New in Oracle Solaris 11 (Self-Study)\n\n\nOracle Solaris 11 System Administration\n\n\nOracle Solaris 11 Advanced System Administration\n\n\n\n\nGet more details at the Oracle University page:\n\nhttp://bit.ly/OracleSolaris11Edu\n .\n\n\nIf you prefer to study Oracle Solaris 11 on your own, there are a lot of books\navailable: \nOracle Solaris 11 books on Amazon\n\n\nDon't miss Oracle Solaris channel on YouTube: \nhttps://www.youtube.com/OracleSolaris\n and especially the screencasts of most of the labs in this course:\n\n\n\n\nSeason 1: The Basics\n\n\nSeason 2: Cloud in a Box\n\n\nSeason 3: Tips and Tricks",
            "title": "Introduction"
        },
        {
            "location": "/index.html#prerequisites",
            "text": "This lab requires access to an Oracle Solaris 11 system either natively or in a\nVirtualBox virtual machine.",
            "title": "Prerequisites"
        },
        {
            "location": "/index.html#using-virtual-box-vm",
            "text": "If you're performing this lab on your laptop or desktop, we've provided\na VirtualBox appliance that contains all the software and configuration\nnecessary to complete this lab. All you need is VirtualBox software and\na modern (with a CPU supporting virtualization turned on, AMD-V or VT-x)\nlaptop/desktop with at least 3 GB of RAM and 10GB of free disk space.   Download and install the latest version of VirtualBox for your\n    platform ( http://www.virtualbox.org/wiki/Downloads ).  Download and install the latest version of VirtualBox Extensions\n    Pack ( http://www.virtualbox.org/wiki/Downloads ).  Download or find on the provided DVD/USB and import the Hands On Lab\n    machine into VirtualBox (File   Import Appliance). You will have\n    to accept the OTN Oracle Solaris license to use the appliance.",
            "title": "Using Virtual Box VM"
        },
        {
            "location": "/index.html#the-environment",
            "text": "In this lab we are going to use Oracle Solaris 11 virtual appliance in\nOracle VirtualBox environment. If you are using lab machines, the\nappliance is already installed. You can also download the appliance from\nOracle Technology Network: ( http://www.oracle.com/technetwork/server-storage/solaris11/downloads/vm-templates-2245495.html )  By default, VirtualBox assigns the IP address  10.0.2.15  to the Solaris\nglobal zone. We will be using also IP addresses  10.0.2.21  and  10.0.2.22 \nfor local zones. As we are using VirtualBox in NAT (network address\ntranslation) mode, this shouldn't interfere with your outside network\nenvironment.  You should login into Solaris desktop with the following credentials:  Username:  lab  Password:  oracle1  After logging in, open a terminal window and assume the  root  role:  lab@solaris:~$ su -   Password for root is:  solaris1 .  Note: we don't recommend to log in as  root . In Oracle Solaris 11 it is\nprohibited by default;  root  is only a role, not a login name.",
            "title": "The Environment"
        },
        {
            "location": "/index.html#putting-it-all-together",
            "text": "The whole idea of this lab is to show you some Oracle Solaris 11 features that\ncan be used to create a cloud infrastructure based on Solaris. You have\njust created storage pools and filesystems think cloud storage. It\nwas fast, it was simple, it was flexible. You have created and cloned\nSolaris zones with applications within them think  cloud machine\ninstances . You have monitored and managed zones resources think  cloud\nelasticity, metering and chargeback . Add to that \nSolaris  network virtualization , Solaris  security , Solaris  software lifecycle\nmanagement  and many other features which make Oracle Solaris 11 truly\ncloud-oriented operating system. Try them and learn more about Solaris 11!",
            "title": "Putting It All Together"
        },
        {
            "location": "/index.html#final-notes",
            "text": "The virtual appliance we used in this lab is configured to be able to\nperform zone installation without network access. Namely, we've\nconfigured an internal repository with just a small subset of packages\nnecessary for zone installations. If you are going to continue using\nthis appliance with open network access, you will need to change the\nrepository address to Oracle's standard Solaris repository.  root@solaris:~# pkg set-publisher -G '*' -M '*' -g http://pkg.oracle.com/solaris/release -P solaris",
            "title": "Final Notes"
        },
        {
            "location": "/index.html#further-oracle-solaris-education",
            "text": "This Hands-on Lab is just and introduction in Oracle Solaris 11 world.\nWe highly recommend to continue your education with Oracle University.\nThere is a full set of new courses covering Oracle Solaris 11:   Transition to Oracle Solaris 11  What's New in Oracle Solaris 11  What's New in Oracle Solaris 11 (Self-Study)  Oracle Solaris 11 System Administration  Oracle Solaris 11 Advanced System Administration   Get more details at the Oracle University page: http://bit.ly/OracleSolaris11Edu  .  If you prefer to study Oracle Solaris 11 on your own, there are a lot of books\navailable:  Oracle Solaris 11 books on Amazon  Don't miss Oracle Solaris channel on YouTube:  https://www.youtube.com/OracleSolaris  and especially the screencasts of most of the labs in this course:   Season 1: The Basics  Season 2: Cloud in a Box  Season 3: Tips and Tricks",
            "title": "Further Oracle Solaris Education"
        },
        {
            "location": "/zfs/zfs/index.html",
            "text": "What is ZFS?\n\n\nZFS (Zettabyte File System) was developed by Sun Microsystems and introduced in Solaris 10 in 2005. Since then it became very popular\nand not only in Solaris world. The main goal of ZFS design was to make sysadmin's life easier. You can quickly and easily:\n\n\n\n\ncreate disk pools and filesystems (it takes less than a second, no matter how big your pool is)\n\n\nsnapshot and clone your data for backup or dev/test purposes \n\n\nturn on compression and deduplication to save space\n\n\nmigrate Terabytes of your application data with minimal downtime\n\n\n\n\nIn addition to that, ZFS is extremely reliable: everything is covered by checksums, all the way along the path of the data. ZFS is scalable up to the numbers which are hard to imagine: file systems can have up to 256 quadrillion zettabytes of storage, directories can have up to 256 trillion entries.  \n\n\nYou can find more information about ZFS concepts and design strategy in this document: \nWhat is ZFS?\n\n\nLab Outline: Situations and Exercises\n\n\nWe have collected several practical situations where we can use some ZFS\nfeatures. It is not absolutely manadatory to perform all the exercises in the\nfollowing order, but sometimes we use results of the preceding tasks, like\nusing the ZFS pool we created in the first exercise. If you feel you are\nmissing something, try to look at the preceding exercises. \n\n\n\n\n\n\n\n\nSituation\n\n\nExercise\n\n\n\n\n\n\n\n\n\n\nYou have some disks to use for your new file system. Create a new disk pool and a file system on top of it.\n\n\nZFS Pools\n\n\n\n\n\n\nYou have to create home directories for your users; use file system quota to limit their space.\n\n\nZFS File systems\n\n\n\n\n\n\nYou are becoming low on your disk space. Add a couple more disks to your pool and expand your file system.\n\n\nZFS Compression\n\n\n\n\n\n\nUsers tend to keep a lot of similar files in their archives. Is it possible to save space by using deduplication?\n\n\nZFS Deduplication\n\n\n\n\n\n\nA user has accidentally deleted her file. How to restore it without getting to the backup?\n\n\nZFS Snapshots\n\n\n\n\n\n\nYou need a copy of your data to work on it separately (e.g. for test/dev environment). How to create a copy of your multi-terabyte dataset really fast?\n\n\nZFS Clones\n\n\n\n\n\n\nYour users want to be able to create and rollback their own snapshots (instead of asking you to do that\n\n\nZFS Rights Delegation\n\n\n\n\n\n\nHow do we backup and restore ZFS file systems?\n\n\nZFS Backup and Restore\n\n\n\n\n\n\nYou want to free up some space in your ZFS file system. You find several huge files, you delete them, but you don't see that you gained free space. What's happening?\n\n\nZFS Free Space\n\n\n\n\n\n\nAll ZFS file systems share the same pool. You want to make sure that some critical file systems are guaranteed to have enough space.\n\n\nZFS Reservations\n\n\n\n\n\n\nYou have received a new, fast storage array and you want to migrate your application data to it. How to do it with minimal downtime?\n\n\nZFS Shadow Migration",
            "title": "ZFS Intro"
        },
        {
            "location": "/zfs/zfs/index.html#what-is-zfs",
            "text": "ZFS (Zettabyte File System) was developed by Sun Microsystems and introduced in Solaris 10 in 2005. Since then it became very popular and not only in Solaris world. The main goal of ZFS design was to make sysadmin's life easier. You can quickly and easily:   create disk pools and filesystems (it takes less than a second, no matter how big your pool is)  snapshot and clone your data for backup or dev/test purposes   turn on compression and deduplication to save space  migrate Terabytes of your application data with minimal downtime   In addition to that, ZFS is extremely reliable: everything is covered by checksums, all the way along the path of the data. ZFS is scalable up to the numbers which are hard to imagine: file systems can have up to 256 quadrillion zettabytes of storage, directories can have up to 256 trillion entries.    You can find more information about ZFS concepts and design strategy in this document:  What is ZFS?",
            "title": "What is ZFS?"
        },
        {
            "location": "/zfs/zfs/index.html#lab-outline-situations-and-exercises",
            "text": "We have collected several practical situations where we can use some ZFS\nfeatures. It is not absolutely manadatory to perform all the exercises in the\nfollowing order, but sometimes we use results of the preceding tasks, like\nusing the ZFS pool we created in the first exercise. If you feel you are\nmissing something, try to look at the preceding exercises.      Situation  Exercise      You have some disks to use for your new file system. Create a new disk pool and a file system on top of it.  ZFS Pools    You have to create home directories for your users; use file system quota to limit their space.  ZFS File systems    You are becoming low on your disk space. Add a couple more disks to your pool and expand your file system.  ZFS Compression    Users tend to keep a lot of similar files in their archives. Is it possible to save space by using deduplication?  ZFS Deduplication    A user has accidentally deleted her file. How to restore it without getting to the backup?  ZFS Snapshots    You need a copy of your data to work on it separately (e.g. for test/dev environment). How to create a copy of your multi-terabyte dataset really fast?  ZFS Clones    Your users want to be able to create and rollback their own snapshots (instead of asking you to do that  ZFS Rights Delegation    How do we backup and restore ZFS file systems?  ZFS Backup and Restore    You want to free up some space in your ZFS file system. You find several huge files, you delete them, but you don't see that you gained free space. What's happening?  ZFS Free Space    All ZFS file systems share the same pool. You want to make sure that some critical file systems are guaranteed to have enough space.  ZFS Reservations    You have received a new, fast storage array and you want to migrate your application data to it. How to do it with minimal downtime?  ZFS Shadow Migration",
            "title": "Lab Outline: Situations and Exercises"
        },
        {
            "location": "/zfs/zfs_pools/index.html",
            "text": "Task:\n You have several disks to use for your new file system. Create\na new disk pool and a file system on top of it.\n\n\nLab:\n We will check the status of disk pools, create our own pool and\nexpand it.\n\n\nOur Solaris 11 installation already has a ZFS pool. It's your root file\nsystem. Check this:\n\n\nroot@solaris:~# zpool list \n\nNAME    SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nrpool  15.9G  5.64G  10.2G  35%  1.00x  ONLINE  -\n\n\n\nThis is our root system ZFS pool. In Solaris 11 the root file system\nmust be ZFS created on top of ZFS pool. What do we know about this pool?\n\n\nroot@solaris:~# zpool status rpool \npool: rpool\nstate: ONLINE\nscan: none requested\nconfig:\n\nNAME        STATE     READ WRITE CKSUM\nrpool       ONLINE       0     0     0\n  c1t0d0s1  ONLINE       0     0     0\n\nerrors: No known data errors\n\n\n\nIn our typical lab environment in the VirtualBox VM we don't have extra disks\nto experiment with. Let's use files instead. We will create a\nseparate directory and create files in it.\n\n\nroot@solaris:~# mkdir /devdsk\nroot@solaris:~# cd /devdsk\nroot@solaris:/devdsk# mkfile 200m c2d{0..11}\n\n\n\nNow we have 12 files which \nlook\n like disks and we will use them like\nif they were disks. Create a ZFS pool out of 4 disks using RAID-Z\nprotection:\n\n\nroot@solaris:/devdsk# zpool create labpool raidz /devdsk/c2d0 /devdsk/c2d1 /devdsk/c2d2 /devdsk/c2d3\n\n\n\nThat was easy, wasn't it? And fast, too! Check our ZFS pools again:\n\n\nroot@solaris:~# zpool list \nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool   748M   158K   748M   0%  1.00x  ONLINE  -\nrpool    15.6G  7.79G  7.83G  49%  1.00x  ONLINE  -\n\nCheck its status:\n\n\nroot@solaris:~# zpool status labpool\npool: labpool\nstate: ONLINE\nscan: none requested\nconfig:\n\nNAME        STATE     READ WRITE CKSUM\nlabpool     ONLINE       0     0     0\n  raidz1-0  ONLINE       0     0     0\n    c1t4d0  ONLINE       0     0     0\n    c1t5d0  ONLINE       0     0     0\n    c1t6d0  ONLINE       0     0     0\n    c1t7d0  ONLINE       0     0     0\n\nerrors: No known data errors\n\n\n\nBy the way, the file system was also created and mounted automatically:\n\n\nroot@solaris:~# zfs list labpool \nNAME      USED  AVAIL  REFER  MOUNTPOINT\nlabpool  97.2K   527M  44.9K  /labpool\n\n\n\nDo you need more space? Adding disks to the existing ZFS pool is as easy\nas creating it:\n\n\nroot@lab0:/devdsk# zpool add labpool raidz /devdsk/c2d4 /devdsk/c2d5 /devdsk/c2d6 /devdsk/c2d7\n\n\n\nCheck its size again:\n\n\nroot@solaris:~# zpool list labpool \nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G   134K  1.46G   0%  1.00x  ONLINE  -\nroot@solaris:~# zfs list labpool\nNAME     USED  AVAIL  REFER  MOUNTPOINT\nlabpool  100K  1.06G  44.9K  /labpool\n\n\n\nTake a note of the increased pool and file system sizes. Why are they\ndifferent? What do you think?\n\n\nCheck the pool status:\n\n\nroot@solaris:~# zpool status labpool\npool: labpool\nstate: ONLINE\nscan: none requested\nconfig:\n\nNAME         STATE     READ WRITE CKSUM\nlabpool      ONLINE       0     0     0\n  raidz1-0   ONLINE       0     0     0\n    c1t4d0   ONLINE       0     0     0\n    c1t5d0   ONLINE       0     0     0\n    c1t6d0   ONLINE       0     0     0\n    c1t7d0   ONLINE       0     0     0\n  raidz1-1   ONLINE       0     0     0\n    c1t8d0   ONLINE       0     0     0\n    c1t9d0   ONLINE       0     0     0\n    c1t10d0  ONLINE       0     0     0\n    c1t11d0  ONLINE       0     0     0\n\nerrors: No known data errors\n\n\n\nNote that there are two disk groups in this pool both protected with\nRAID-Z. ZFS has many options to protects your data, you can learn and\nexperiment with them later. Hint: learn more about RAID-Z2 and RAID-Z3\noptions and how they can protect your data.",
            "title": "ZFS Pools"
        },
        {
            "location": "/zfs/zfs_fs/index.html",
            "text": "Task:\n You have to create home directories for your users; use file\nsystem quota to limit their space.\n\n\nLab:\n We'll create a user \"\njoe\n\" and set a disk quota for him.\n\n\nCreating a user is pretty similar to most Unix/Linux systems. What's\ndifferent is what's going on behind the scenes.\n\n\nroot@solaris:~# useradd -m joe \nroot@solaris:~# passwd joe \nNew Password: oracle1 \nRe-enter new Password: oracle1\npasswd: password successfully changed for joe\n\n\n\nIn Solaris 11 behind the scenes we create a \nseparate\n ZFS file system\nfor the user (parameter \n-m\n) in \n/export/home\n. Check it:\n\n\nroot@solaris:~# zfs list\nNAME                              USED  AVAIL  REFER  MOUNTPOINT\nlabpool                           100K  1.06G  44.9K  /labpool\nrpool                            8.33G  7.05G  4.97M  /rpool\nrpool/ROOT                       4.73G  7.05G    31K  legacy\nrpool/ROOT/solaris               4.73G  7.05G  4.22G  /\nrpool/ROOT/solaris/var            409M  7.05G   198M  /var\nrpool/VARSHARE                    144K  7.05G    50K  /var/share\nrpool/VARSHARE/pkg                 63K  7.05G    32K  /var/share/pkg\nrpool/VARSHARE/pkg/repositories    31K  7.05G    31K  /var/share/pkg/repositories\nrpool/VARSHARE/zones               31K  7.05G    31K  /system/zones\nrpool/dump                        792M  7.08G   768M  -\nrpool/export                      906K  7.05G    32K  /export\nrpool/export/home                 874K  7.05G    33K  /export/home\nrpool/export/home/joe              35K  7.05G    35K  /export/home/joe\nrpool/export/home/lab             806K  7.05G   806K  /export/home/lab\nrpool/repo                       1.78G  7.05G  1.78G  /repo\nrpool/swap                       1.03G  7.09G  1.00G  -\n\n\n\nWhat does it mean for us, system administrators? That means we can use\nall kinds of ZFS features (compression, deduplication, encryption) on a\nper-user basis. We can create snapshots and perform rollbacks on a\nper-user basis. We can even give users rights to perform those\noperations themselves (look into Advanced labs folder). Now we'll set a\ndisk quota for \njoe\n's home directory.\n\n\nroot@solaris:~# zfs set quota=200m rpool/export/home/joe\n\n\n\nNow change user to \"\njoe\n\" and check how much space you can use:\n\n\nroot@solaris:# su - joe \njoe@solaris$ mkfile 150m file1 \n\n\n\nNow check the file system's available space again:\n\n\nroot@solarislab:~# zfs list rpool/export/home/joe\nNAME                   USED  AVAIL  REFER  MOUNTPOINT\nrpool/export/home/joe  150M  49.9M   150M  /export/home/joe\nand from Joe's perspective:\njoe@solarislab:~$ df -h $HOME\nFilesystem             Size   Used  Available Capacity  Mounted on\nrpool/export/home/joe\n                   200M   150M        50M    76%    /export/home/joe\n\n\n\nNow try to create another file:\n\n\njoe@solaris$ mkfile 150m file2 \n\n\n\nThis time we will get an error: \"Disk quota exceeded\". More than that,\neven root can't create another file in \n/export/home/joe\n directory. Try\nit!\n\n\nChange the quota for \njoe\n in the other window:\n\n\nroot@solaris:~# zfs set quota=300m rpool/export/home/joe \n\n\n\nThen change back to the \njoe\n's window and try again:\n\n\njoe@solaris$ rm file2 \njoe@solaris$ mkfile 150m file2 \n\n\n\nSuccess! As you can see, it's pretty easy to create and manage ZFS\nfilesystems. Remember, by default Solaris 11 creates a separate ZFS file\nsystem for each user.",
            "title": "ZFS File Systems"
        },
        {
            "location": "/zfs/zfs_compress/index.html",
            "text": "Task:\n You are getting low on your disk space. Now you know how to\nadd more disks to your pool and expand your file system. What other ZFS\nfeatures can help you to solve this problem?\n\n\nLab:\n In our lab we will compress our Solaris manuals directory and\nsee if we are able to use it after that. Create a separate filesystem\nfor this on our 'labpool' ZFS pool:\n\n\nroot@solaris:~# zfs create labpool/zman \nroot@solaris:~# zfs list | grep zman \nlabpool/zman                     44.9K   1.06G  44.9K  /labpool/zman\n\n\n\nSet compression to \"gzip\" (there are options to gzip and other\nalgorithms too--check the manual). You can do that also while creating\nthe filesystem.\n\n\nroot@solaris:~# zfs set compression=gzip labpool/zman \n\n\n\nCopy the first part of Solaris manuals there (it will take some time, be\npatient):\n\n\nroot@solaris:~# cp -rp /usr/share/man/man1 /labpool/zman/ \n\n\n\nCompare the sizes:\n\n\nroot@solaris:~# du -sh /usr/share/man/man1 /labpool/zman/man1\n13M   /usr/share/man/man1\n5.6M   /labpool/zman/man1\n\n\n\nWe just have saved about 57% of disk space. Not bad! Check if you are\nable to use the manuals after compression:\n\n\nroot@solaris:~# export MANPATH=/labpool/zman ; man ls \n\n\n\nInteresting to note: it may sound counterintuitive, but using\ncompression actually \nincreases\n file system's performance. You may\nthink: \"Compression uses extra CPU time, so it should slow down file\nsystem operations, right?\". But try to think further. Imagine a file\nthat takes two blocks on your disk. To write this file you have to write\ntwo blocks, right? If you compress this file by 50% you have to write\nonly one block. Now the question is: \"What is faster, your disk or your\nCPU?\". Of course, it takes much less time to compress a block of data\nthan to write it on the disk. OK, it's easy to explain, but is it\nconfirmed by practice? Yes, it is! Take a look at the blog of Don\nMacAsksill and see how he had confirmed that ZFS compression increases\nperformance:\n\nhttp://don.blogs.smugmug.com/2008/10/13/zfs-mysqlinnodb-compression-update/\n.\nNote that it works best when you use the default LZJB algorithm by using\nplain \"compression=on\" parameter. You might consider it a good default\npractice when creating ZFS file systems. There are exceptions, of\ncourse: image, video, encrypted and already compressed data will not\ngive you this advantage as they will not be compressed.",
            "title": "ZFS Compression"
        },
        {
            "location": "/zfs/zfs_dedup/index.html",
            "text": "Task:\n Users tend to keep a lot of similar files in their archives.\nIs it possible to save space by using deduplication?\n\n\nLab:\n We will create a ZFS file system with deduplication turned on\nand see if it helps.\n\n\nLet's model the following situation: we have a file system which is used\nas an archive. We'll create separate file systems for each user and\nimagine that they store similar files there.\n\n\nWe will use the ZFS pool called \nlabpool\n that we have created in the\nfirst exercise.\n\n\nCreate a file system with deduplication and compression:\n\n\nroot@solaris:~# zfs create -o dedup=on -o compression=gzip labpool/archive\n\n\n\nCreate users' file systems (we'll call them a, b, c, d for simplicity):\n\n\nroot@solaris:~# zfs create labpool/archive/a\nroot@solaris:~# zfs create labpool/archive/b\nroot@solaris:~# zfs create labpool/archive/c\nroot@solaris:~# zfs create labpool/archive/d\n\n\n\nCheck their \"dedup\" parameter:\n\n\nroot@solaris:~# zfs get dedup labpool/archive/a\nNAME               PROPERTY  VALUE          SOURCE\nlabpool/archive/a  dedup     on             inherited from labpool/archive\n\n\n\nChildren file systems inherit parameters from their parents.\n\n\nCreate an archive from /usr/share/man/man1, for example.\n\n\nroot@solaris:~# tar czf /tmp/man1.tar.gz /usr/share/man/man1\n\n\n\nAnd copy it four times to the file systems we've just created. Don't\nforget to check deduplication rate after each copy.\n\n\nroot@solaris:~# cd /labpool/archive\nroot@solaris:/labpool/archive# ls -lh /tmp/man1.tar.gz \n-rw-r--r--   1 root     root        3.2M Oct  3 15:30 /tmp/man1.tar.gz\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  7.99M  1.45G   0%  1.00x  ONLINE  -\nroot@solaris:/labpool/archive# cp /tmp/man1.tar.gz a/\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  12.6M  1.45G   0%  1.00x  ONLINE  -\nroot@solaris:/labpool/archive# cp /tmp/man1.tar.gz b/\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  12.7M  1.45G   0%  2.00x  ONLINE  -\nroot@solaris:/labpool/archive# cp /tmp/man1.tar.gz c/\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  12.7M  1.45G   0%  2.00x  ONLINE  -\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  12.5M  1.45G   0%  3.00x  ONLINE  -\nroot@solaris:/labpool/archive# cp /tmp/man1.tar.gz d/\nroot@solaris:/labpool/archive# zpool list labpool\nNAME      SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nlabpool  1.46G  12.5M  1.45G   0%  4.00x  ONLINE  -\n\n\n\nIt might take a couple of seconds for ZFS to commit those changes and\nreport the correct dedup ratio. Just repeat the command if you don't see\nthe results listed above.\n\n\nRemember, we set compression to \"on\" as well when we created the file\nsystem? Check the compression ratio:\n\n\nroot@solaris:/labpool/archive# zfs get compressratio labpool/archive\nNAME             PROPERTY       VALUE  SOURCE\nlabpool/archive  compressratio  1.00x  -\n\n\n\nThe reason is simple: we placed in the file system files that are\ncompressed already. Sometimes compression can save you some space,\nsometimes deduplication can help.\n\n\nIt's interesting to note that ZFS uses deduplication on a block level,\nnot on a file level. That means if you have a single file but with a lot\nof identical blocks, it will be deduplicatied too. Let's check this.\nCreate a new ZFS pool:\n\n\nroot@lab0:~# zpool create ddpool raidz /devdsk/c2d8 /devdsk/c2d9 /devdsk/c2d10 /devdsk/c2d11\n\n\n\nAs you remember, when we create a ZFS pool, by default a new ZFS\nfilesystem with the same name is created and mounted. We just have to\nturn deduplication on:\n\n\nroot@solaris:~# zfs set dedup=on ddpool\n\n\n\nNow let's create a big file that contains 1000 copies of the same block.\nIn the following commands we are figuring out the size of a ZFS block\nand creating a single file of that size. Then we are copying that file\n1000 times into our big file.\n\n\nroot@solaris:~# zfs get recordsize ddpool\nNAME    PROPERTY    VALUE  SOURCE\nddpool  recordsize  128K   default\nroot@solaris:~# mkfile 128k 128k-file\nroot@solaris:~# for i in {1..1000} ; do cat 128k-file \n 1000copies-file ; done\n\n\n\nNow we can copy this file to /ddpool and see the result:\n\n\nroot@solaris:~# cp 1000copies-file /ddpool\nroot@solaris:~# zpool list\nNAME      SIZE  ALLOC   FREE  CAP     DEDUP  HEALTH  ALTROOT\nddpool    748M   357K   748M   0%  1000.00x  ONLINE  -\nlabpool  1.46G  12.4M  1.45G   0%     4.00x  ONLINE  -\nrpool    15.6G  7.91G  7.72G  50%     1.00x  ONLINE  -\n\n\n\nHow can this help in real life? Imagine you have a policy which requires\ncreating and storing an archive every day. The archive's content doesn't\nchange a lot from day to day, but still you have to create it every day.\nMost of the blocks in the archive will be identical so it can be\ndeduplicated very efficiently. Let's demonstrate it using our system's\nmanual directories.\n\n\nroot@solaris:~# tar cvf /tmp/archive1.tar /usr/share/man/man1\nroot@solaris:~# tar cvf /tmp/archive2.tar /usr/share/man/man1 /usr/share/man/man2\n\n\n\nClean up our \n/ddpool\n file system and copy both files there:\n\n\nroot@solaris:~# rm /ddpool/*\nroot@solaris:~# cp /tmp/archive* /ddpool\nroot@solaris:~# zpool list ddpool\nNAME    SIZE  ALLOC  FREE  CAP  DEDUP  HEALTH  ALTROOT\nddpool  748M  18.2M  730M   2%  1.90x  ONLINE  -\n\n\n\nThink about your real life situations where deduplication could help.\nHomework exercise: compress both archive files with \ngzip\n, clean up the\n\n/ddpool\n and copy the compressed files again. Check if it affects\ndeduplication rate.",
            "title": "ZFS Deduplication:"
        },
        {
            "location": "/zfs/zfs_snap/index.html",
            "text": "Task:\n A user has accidentally deleted her file. How to restore it\nwithout getting to the tape backup?\n\n\nLab:\n Create a snapshot of our archive filesystem:\n\n\nroot@solaris:~# zfs snapshot -r labpool/archive@snap1 \n\n\n\nNote the \n-r\n parameter telling ZFS that we want to snapshot all\ndependent filesystems as well. Check your work:\n\n\nroot@solaris:~# zfs list -r -t all labpool\nNAME                      USED  AVAIL  REFER  MOUNTPOINT\nlabpool                  18.5M  1.10G  47.9K  /labpool\nlabpool/archive          12.7M  1.10G  52.4K  /labpool/archive\nlabpool/archive@snap1        0      -  52.4K  -\nlabpool/archive/a        3.17M  1.10G  3.17M  /labpool/archive/a\nlabpool/archive/a@snap1      0      -  3.17M  -\nlabpool/archive/b        3.17M  1.10G  3.17M  /labpool/archive/b\nlabpool/archive/b@snap1      0      -  3.17M  -\nlabpool/archive/c        3.17M  1.10G  3.17M  /labpool/archive/c\nlabpool/archive/c@snap1      0      -  3.17M  -\nlabpool/archive/d        3.17M  1.10G  3.17M  /labpool/archive/d\nlabpool/archive/d@snap1      0      -  3.17M  -\nlabpool/zman             5.41M  1.10G  5.41M  /labpool/zman\n\n\n\nImagine user \na\n had deleted her archive stored in /labpool/archive/a.\n\n\nroot@solaris:~# rm /labpool/archive/a/* \n\n\n\nAnd she comes to you asking for help. \"Can you restore my archive before\ntomorrow?\", she asks. Of course, you can! In a matter of seconds, not\nhours, her archive files will be back! Just rollback the snapshot!\n\n\nroot@solaris:~# zfs rollback labpool/archive/a@snap1 \n\n\n\nYou may ask \"How often should I make snapshots? Do snapshots take a lot\nof space? The answer is here:\n\n\nroot@solaris:~# zfs list -r -t all labpool \nNAME                    USED  AVAIL  REFER  MOUNTPOINT\nlabpool                18.5M  1.10G  47.9K  /labpool\nlabpool/archive        12.7M  1.10G  52.4K  /labpool/archive\nlabpool/archive@snap1      0      -  52.4K  -\nlabpool/archive/a      3.17M  1.10G  3.17M  /labpool/archive/a\nlabpool/archive/b      3.17M  1.10G  3.17M  /labpool/archive/b\nlabpool/archive/c      3.17M  1.10G  3.17M  /labpool/archive/c\nlabpool/archive/d      3.17M  1.10G  3.17M  /labpool/archive/d\nlabpool/zman           5.41M  1.10G  5.41M  /labpool/zman\n\n\n\nThe snapshot uses 0 bytes because we have not changed anything in your\nhome directory. When you make changes to your filesystem, it will take\nmore space. Try to change something in the /labpool/archive directory\nand check the sizes again. Learn more about how snapshots work from our\nOTN technical presentations and articles.\n\n\nFood for thought: How can snapshots be used in the real life\nenvironment? Backup is the first idea that comes to mind. What else?",
            "title": "ZFS Snapshops"
        },
        {
            "location": "/zfs/zfs_clones/index.html",
            "text": "Task:\n We need to create a copy of our transactional data to do some\nanalysis and modifications. In other words, we need a writeable\nsnapshot.\n\n\nLab:\n In this lab we will use ZFS cloning feature. Clones are similar\nto snapshots, but you can modify them. Similarly to snapshots, it takes\nseconds to create them and they take almost no space until you start\nchanging your files.\n\n\nClones can't be created from a live filesystem. To create a clone we\nhave to have a snapshot first. In this lab we can use a snapshot\n'@snap1' we have just created.\n\n\nroot@solaris:~# zfs clone labpool/archive/a@snap1 labpool/a_work\nroot@solaris:~# zfs list -r -t all labpool\nNAME                      USED  AVAIL  REFER  MOUNTPOINT\nlabpool                  18.6M  1.10G  49.4K  /labpool\nlabpool/a_work           26.9K  1.10G  3.17M  /labpool/a_work\nlabpool/archive          12.8M  1.10G  52.4K  /labpool/archive\nlabpool/archive@snap1        0      -  52.4K  -\nlabpool/archive/a        3.20M  1.10G  3.17M  /labpool/archive/a\nlabpool/archive/a@snap1  26.9K      -  3.17M  -\nlabpool/archive/b        3.17M  1.10G  3.17M  /labpool/archive/b\nlabpool/archive/b@snap1      0      -  3.17M  -\nlabpool/archive/c        3.17M  1.10G  3.17M  /labpool/archive/c\nlabpool/archive/c@snap1      0      -  3.17M  -\nlabpool/archive/d        3.17M  1.10G  3.17M  /labpool/archive/d\nlabpool/archive/d@snap1      0      -  3.17M  -\nlabpool/zman             5.41M  1.10G  5.41M  /labpool/zman\n\n\n\nCheck if the archive is in place in the clone filesystem:\n\n\nroot@solaris:~# cd /labpool/a_work\nroot@solaris:/labpool/a_work# ls\nman.tar.gz\n\n\n\nUnpack the archive and then check the original directory.\n\n\nroot@solaris:/labpool/a_work# tar xzvf man1.tar.gz \n....................\ntar: Removing leading '/' from '/usr/share/man/man1/tracker-services.1'\nx usr/share/man/man1/tracker-services.1, 1938 bytes, 4 tape blocks\nroot@solaris:/labpool/a_work# ls -l \ntotal 6413\n-rw-r--r--   1 root     root     3257177 Dec 13 17:05 man1.tar.gz\ndrwxr-xr-x   3 root     root           3 Dec 13 18:04 usr\nroot@solaris:/labpool/a_work# cd ../archive/a\nroot@solaris:/labpool/archive/a# ls  -l\ntotal 6409\n-rw-r--r--   1 root     root     3257177 Dec 13 17:05 man1.tar.gz\n\n\n\nThis powerful cloning feature can be used for your regular data. Oracle\nSolaris uses it internally to create boot environments and zone clones.\nThey will be described in the following lab exercises.",
            "title": "ZFS Clones"
        },
        {
            "location": "/zfs/zfs_rights/index.html",
            "text": "Remember, in our previous ZFS lab we created file system snapshots and\nthen used them to restore files we have \"accidentally\" deleted? It would\nbe great if it was possible to give your users rights to create and\nrestore snapshots on their own, without distracting you, sysadmin, from\nmore important tasks?\n\n\nYes, it's possible! You can delegate these rights to your users. Let's\ncreate a user Joe and give him rights to manage his own home directory,\ni.e. file system (remember, in Solaris 11 \nuseradd\n operation creates a\nZFS file system for the user, not just a home directory!).\n\n\nroot@solaris:~# useradd -c \nJoe User\n -m joe\n80 blocks\nroot@solaris:~# passwd joe\nNew Password:\nRe-enter new Password:\npasswd: password successfully changed for joe\nroot@solaris:~# zfs allow joe create,destroy,mount,snapshot rpool/export/home/joe\n\n\n\nNow become Joe and create a file. After that, create a snapshot and\n\"accidentally\" delete the file you have just created:\n\n\nroot@solaris:~# su - joe\nOracle Corporation SunOS 5.11 11.1 September 2012\njoe@solaris:~$\njoe@solaris:~$ vi firstfile.txt\njoe@solaris:~$ cat firstfile.txt\nThis is my first file.\njoe@solaris:~$ pwd\n/export/home/joe\njoe@solaris:~$ zfs snap rpool/export/home/joe@snap1\njoe@solaris:~$ rm firstfile.txt\njoe@solaris:~$ cat firstfile.txt\ncat: cannot open firstfile.txt: No such file or directory\n\n\n\nYes, the file is gone. But Joe is a smart guy, he has taken a snapshot\nafter he created the file. But he just forget the name of the\nsnapshot... Let's figure it out:\n\n\njoe@solaris:~$ zfs list -t all | grep joe\nrpool/export/home/joe 56K 8.52G 35.5K /export/home/joe\nrpool/export/home/joe@snap1 20.5K - 35.5K -\n\n\n\nOK, now Joe knows the name and tries to rollback the snapshot:\n\n\njoe@solaris:~$ zfs rollback rpool/export/home/joe@snap1\ncannot rollback 'rpool/export/home/joe': permission denied\n\n\n\nWhat? A-ha, we forgot to add rollback to the list of rights for Joe.\nLet's fix that:\n\n\njoe@solaris:~$ exit\nlogout\nroot@solaris:~# zfs allow joe rollback rpool/export/home/joe\nroot@solaris:~# su - joe\nOracle Corporation SunOS 5.11 11.1 September 2012\njoe@solaris:~$ zfs rollback rpool/export/home/joe@snap1\njoe@solaris:~$ ls\nfirstfile.txt local.cshrc local.login local.profile\njoe@solaris:~$ cat firstfile.txt\nThis is my first file.\n\n\n\nWhat a relief for Joe! And what a relief for you--now your users can\nmanage their filesystems on their own! Joe can even create new file\nsystems under his home directory. Try this as Joe to test if it's\npossible.",
            "title": "ZFS Rights Delegation"
        },
        {
            "location": "/zfs/zfs_backup/index.html",
            "text": "No data management system is complete without proper backup and restore\nfacility. Let's see what's available in ZFS.\n\n\nCreate a new file system and copy some data into it. In our case we'll\nuse system manuals again.\n\n\nroot@solaris:~# zfs create -o compression=lz4 -o mountpoint=/data rpool/data\nroot@solaris:~# cp -rp /usr/share/man/ /data/\nroot@solaris:~# zfs list rpool/data\nNAME         USED  AVAIL  REFER  MOUNTPOINT\nrpool/data  94.5M  19.8G  94.5M  /data\n\n\n\nTry \nls -R /data\n and see a lot of manual files listed in this\ndirectory. Everything is good.\n\n\nBefore creating a backup we have to take a snapshot of our data. (Note\nthat we don't have to stop applications that might be accessing the\ndata.) In the command below we use the '\ndate(1)\n' command to produce a\ntimestamp on the backup snapshot. Feel free to use the date format which\nis suitable for you.\n\n\nroot@solaris:~# zfs snapshot rpool/data@backup-`date +%Y-%m-%d`\nroot@solaris:~# zfs list -r -t filesystem,snapshot rpool/data\nNAME                           USED  AVAIL  REFER  MOUNTPOINT\nrpool/data                    94.5M  19.5G  94.5M  /data\nrpool/data@backup-2016-03-28      0      -  94.5M  -\n\n\n\nNow let's create a separate ZFS pool to store our backups. In our lab\nenvironment we'll use plain files instead of actual disks, but in real\nlife most likely you will use a separate storage array.\n\n\nroot@solaris:~# cd /devdsk\nroot@solaris:/devdsk# mkfile 300M backupdisk\nroot@solaris:/devdsk# cd\nroot@solaris:~# zpool create backuppool /devdsk/backupdisk \n\n\n\nAnd now we are ready to send the snapshot we've just created to that\n\"separate storage array\".\n\n\nroot@solaris:~# zfs send rpool/data@backup-2016-03-28 | zfs recv backuppool/backup\nroot@solaris:~# zfs list -r -t filesystem,snapshot backuppool/backup\nNAME                                 USED  AVAIL  REFER  MOUNTPOINT\nbackuppool/backup                    161M   101M   161M  /backuppool/backup\nbackuppool/backup@backup-2016-03-28     0      -   161M  -\n\n\n\nThis is all good, but why the backup file system takes more space than\nthe original? Of course, we didn't specify the compression option! Can\nwe do it while receiving the snapshot? Of course! Repeat the command but\nnow with the compression option (don't forget to destroy the backup\nfirst):\n\n\nroot@solaris:~# zfs destroy -r backuppool/backup\nroot@solaris:~# zfs send rpool/data@backup-2016-03-28 | zfs recv -o compression=lz4 backuppool/backup\nroot@solaris:~# zfs list -r -t filesystem,snapshot backuppool/backup\nNAME                                  USED  AVAIL  REFER  MOUNTPOINT\nbackuppool/backup                    94.3M   167M  94.3M  /backuppool/backup\nbackuppool/backup@backup-2016-03-28      0      -  94.3M  -\n\n\n\nNow we'll use this backup to restore our file system. Take a look at\nyour data one last time and destroy the file system.\n\n\nroot@solaris:~# ls -R /data\n... (long list of files follows. Feel free to stop it with Ctrl-C)....\nroot@solaris:~# zfs destroy -r rpool/data\nroot@solaris:~# ls -R /data\n/data:\n\n\n\nNow receive your data back from the backup and check if everything is\nOK.\n\n\nroot@solaris:~# zfs send backuppool/backup@backup-2016-03-28 | zfs recv -o compression=lz4 rpool/data\nroot@solaris:~# ls -R /data\n/data:\n\n\n\nWhat? Where is our data?? Try 'zfs list':\n\n\nroot@solaris:~# zfs list rpool/data\nNAME         USED  AVAIL  REFER  MOUNTPOINT\nrpool/data  94.3M  19.2G  94.3M  /rpool/data\n\n\n\nOf course! The file system is there, but it's not mounted under /data as\nit was before. Let's receive the backup again, but now specify the\nmountpoint.\n\n\nroot@solaris:~# zfs destroy -r rpool/data\nroot@solaris:~# zfs send backuppool/backup@backup-2016-03-28 | zfs recv -o compression=lz4 -o mountpoint=/data rpool/data\nroot@solaris:~# ls -R /data\n...(a lot of files)....\n\n\n\nWell, now everything is back!\n\n\nYou might have noticed that we used Unix \npipe\n to send and receive ZFS\ndatasets. That means that \nzfs send\n command produces a stream and\nsends it to standard output. So, instead of \"piping\" that stream into\nanother command we can just redirect it to a file, like this:\n\n\nroot@solaris:~# zfs send rpool/data@backup-2016-03-28 \n backup\nroot@solaris:~# file backup\nbackup:     ZFS snapshot stream\nroot@solaris:~# ls -lh backup\n-rw-r--r--   1 root     root        181M Mar 29 12:26 backup\n\n\n\nWhat can we do with this backup file? We can store it in some safe\nlocation, we can copy it and store in several locations, just in case.\nWhen we need to restore from it, we use \nzfs recv\n command and send this\nfile to its standard input. It's pretty easy and very much in \"Unix\nspirit\", isn't it?\n\n\nroot@solaris:~# zfs destroy -r rpool/data\nroot@solaris:~# zfs recv -o compression=lz4 -o mountpoint=/data rpool/data \n backup\n\n\n\nAlso that means that we can send the backup stream to another machine\nvia ssh tunnel. If you have another system available (as in our Oracle\nSolution Center lab environment), try to send it there:\n\n\nroot@solaris:~# zfs send rpool/data@backup-2016-03-28 | ssh  zfs recv -o compression=lz4 backuppool/backup\n\n\n\nThere are a lot more topics in ZFS sending and receiving: full and\nincremental streams, stream packages, recursive stream packages, etc.\nFeel free to open \"Managing ZFS File Systems in Oracle Solaris\" manual\nin the official documentation set and practice using your VirtualBox lab\nenvironment.",
            "title": "ZFS Backup and Restore"
        },
        {
            "location": "/zfs/zfs_free/index.html",
            "text": "One day you have noticed that you don't have enough free space on your\nZFS file system. Of course, you know how easy it is to expand your ZFS\npool: just add more disks. But you don't have any extra disks available\nright now. It's time to use good old method: clean up some garbage. So\nyou go and look for temporary files, downloaded ISO images, old logs and\nother stuff you stored \"just temporarily\" several months ago and forgot\nabout it. You delete them all and check your free space again. But...\nnothing has changed here. You still have shortage of disk space. Why?\n\n\nThe short answer is: snapshots. Remember you took several snapshots on\nyour file system? Remember you were told that they don't occupy any disk\nspace? Yes, that's true. UNTIL you start making changes to your file\nsystem.\n\n\nLet's perform an experiment. Create a file which will represent a disk\ndevice and then create a ZFS pool, which will automatically create and\nmount a new file system.\n\n\nroot@solaris:~# mkfile 1g /var/tmp/disk1\nroot@solaris:~# zpool create test1 /var/tmp/disk1\nroot@solaris:~# zpool list\nNAME    SIZE  ALLOC   FREE  CAP  DEDUP  HEALTH  ALTROOT\nrpool  15.6G  10.3G  5.33G  65%  1.00x  ONLINE  -\ntest1  1016M   152K  1016M   0%  1.00x  ONLINE  -\nroot@solaris:~# zfs list test1\nNAME   USED  AVAIL  REFER  MOUNTPOINT\ntest1   85K   984M    31K  /test1\n\n\n\nNow create a file in this file system and check available space.\n\n\nroot@solaris:~# mkfile 100m /test1/file1\nroot@solaris:~# zfs list test1\nNAME   USED  AVAIL  REFER  MOUNTPOINT\ntest1  100M   884M   100M  /test1\n\n\n\nSo far so good. Exactly 100 MB of space is taken by our file. Of course,\nif we delete this file right now, we get all the space back.\n\n\nroot@solaris:~# rm /test1/file1\nroot@solaris:~# zfs list test1\nNAME   USED  AVAIL  REFER  MOUNTPOINT\ntest1  265K   984M    31K  /test1\n\n\n\nCreate the file again and take a snapshot this time.\n\n\nroot@solaris:~# mkfile 100m /test1/file1\nroot@solaris:~# zfs list test1\nNAME   USED  AVAIL  REFER  MOUNTPOINT\ntest1  100M   884M   100M  /test1\nroot@solaris:~# zfs snapshot test1@snap1\n\n\n\nCheck the sizes of both the file system and the snapshot:\n\n\nroot@solaris:~# zfs list -r -t all test1\nNAME         USED  AVAIL  REFER  MOUNTPOINT\ntest1        100M   884M   100M  /test1\ntest1@snap1     0      -   100M  -\n\n\n\nYou see: the snapshot's size is exactly zero, as you were told already.\nNow delete the file and check the sizes again.\n\n\nroot@solaris:~# rm /test1/file1\nroot@solaris:~# ls /test1\nroot@solaris:~# zfs list -r -t all test1\nNAME         USED  AVAIL  REFER  MOUNTPOINT\ntest1        100M   884M    31K  /test1\ntest1@snap1  100M      -   100M  -\n\n\n\nNow the snapshot takes exactly 100 Megabytes--the size of your deleted\nfile! You don't see the file with \nls(1)\n command, but it is still\nthere, in the snapshot. File is successfully deleted, but your free\nspace is still the same: 884 Megabytes. It's interesting to note that\n\ndf(1M)\n command can produce confusing output in this case:\n\n\noot@solaris:~# df -h /test1\nFilesystem             Size   Used  Available Capacity  Mounted on\ntest1                  984M    31K       884M     1%    /test1\n\n\n\nFrom this output it's hard to figure out where 100 Megabytes have gone.\nSo now you understand why it's recommended to use native \nzfs(1M)\n\ncommands when working with ZFS file systems.\n\n\nIt's all good, but we didn't answer the original question: how to get\nmore free storage space when you need it? In most cases, your file\nsystems will have snapshots, perhaps many of them. Deleting one or\nseveral snapshots can really help in getting more storage space. In our\ncase:\n\n\nroot@solaris:~# zfs destroy test1@snap1\nroot@solaris:~# zfs list -r -t all test1\nNAME   USED  AVAIL  REFER  MOUNTPOINT\ntest1  110K   984M    31K  /test1\n\n\n\nAll free space is back! Congratulations!",
            "title": "ZFS Free Space"
        },
        {
            "location": "/zfs/zfs_reservations/index.html",
            "text": "Do you remember how much space was available to the file systems? You\nare right, all the space that is available in the whole pool, is\navailable to all file systems that are created on that pool. Is it good?\nOf course! You don't have to carefully calculate how much space you will\nneed for this particular file system, you just use it! And when you need\nmore, you just add more disks to the pool and more space is immediately\navailable to all file systems!\n\n\nYes, it's good and very convenient, but... All file systems share the\nsame pool and those of them who grab their space faster, eventually will\ntake over the whole pool and no space will remain available to other\nfile systems who are not that greedy. Yes, you can add disks, but it\ntakes time and your users don't want to wait. What can be done to\nprevent those greedy file systems from hijacking the whole ZFS pool? You\ncan use quotas to limit their appetite, but there is better solution.\nYou can do just the opposite to quotas: instead of setting the maximum\nspace allocation, you can set the minimum, guaranteed space for the most\nimportant file systems.\n\n\nImagine you have a Big Boss who wants to have a guaranteed space for his\nprojects and files in a file system. And you have a Little Boss who is\nnot that demanding. We will use this example to demonstrate how ZFS can\nhandle this situation. Let's start with creating a ZFS pool where we are\ngoing to store data from both bosses. First, figure out what disks are\navailable on your system.\n\n\nroot@solaris:~# echo | format (this command prevents format from going into interactive mode)\nSearching for disks...done\nAVAILABLE DISK SELECTIONS:\n0. c7d0\n/pci@0,0/pci-ide@1,1/ide@0/cmdk@0,0\n1. c7d1\n/pci@0,0/pci-ide@1,1/ide@0/cmdk@1,0\nSpecify disk (enter its number): Specify disk (enter its number):\nroot@solaris:~#\n\n\n\nOK, two disks are installed in the system. Which one is taken by the\nroot pool and which one is available to create another ZFS pool?\n\n\nroot@solaris:~# zpool status\npool: rpool\nstate: ONLINE\nscan: none requested\nconfig:\nNAME STATE READ WRITE CKSUM\nrpool ONLINE 0 0 0\nc7d0 ONLINE 0 0 0\nerrors: No known data errors\n\n\n\nThat means that \nc7d0\n is taken by rpool and we can use \nc7d1\n to create\nan additional ZFS pool. Let's do that.\n\n\nroot@solaris:~# zpool create labpool c7d1\n\n\n\n\nNow we have our own pool; let's create a file system for our Little\nBoss. He is not very demanding, so we create a default file system.\n\n\nroot@solaris:~# zfs create labpool/littleboss\nroot@solaris:~# zfs list\nNAME USED AVAIL REFER MOUNTPOINT\nlabpool 124K 976M 32K /labpool\nlabpool/littleboss 31K 976M 31K /labpool/littleboss\nrpool 5.88G 9.50G 4.90M /rpool\nrpool/ROOT 4.07G 9.50G 31K legacy\nrpool/ROOT/solaris 4.07G 9.50G 3.77G /\nrpool/ROOT/solaris/var 201M 9.50G 196M /var\nrpool/VARSHARE 52.5K 9.50G 52.5K /var/share\nrpool/dump 792M 9.53G 768M -\nrpool/export 868K 9.50G 32K /export\nrpool/export/home 836K 9.50G 32K /export/home\nrpool/export/home/lab 804K 9.50G 804K /export/home/lab\nrpool/swap 1.03G 9.53G 1.00G -\nLet's create another file system, this time for Big Boss.\nroot@solaris:~# zfs create labpool/bigboss\nroot@solaris:~# zfs list\nNAME USED AVAIL REFER MOUNTPOINT\nlabpool 162K 976M 33K /labpool\nlabpool/bigboss 31K 976M 31K /labpool/bigboss\nlabpool/littleboss 31K 976M 31K /labpool/littleboss\n. . .\n\n\n\nYou see: available space in both file systems is the same and it's equal\nto the space available in the whole pool. Try to create a big file in\nBoss' file system:\n\n\nroot@solaris:~# mkfile 200m /labpool/littleboss/bigfile\nroot@solaris:~# zfs list\nNAME USED AVAIL REFER MOUNTPOINT\nlabpool 200M 776M 33K /labpool\nlabpool/bigboss 31K 776M 31K /labpool/bigboss\nlabpool/littleboss 200M 776M 200M /labpool/littleboss\n. . .\n\n\n\n(it may take a second to get exactly 200M. If the number is smaller, try\nzfs list again) You see: Little Boss' file just has taken 200 MB of\nspace - from both file systems! Big Boss might not like it. He wants to\nmake sure that he has at least 500 MB of space for his files! OK, let's\nuse ZFS reservation:\n\n\nroot@solaris:~# zfs set reservation=500m labpool/bigboss\nroot@solaris:~# zfs list\nNAME USED AVAIL REFER MOUNTPOINT\nlabpool 700M 276M 33K /labpool\nlabpool/bigboss 31K 776M 31K /labpool/bigboss\nlabpool/littleboss 200M 276M 200M /labpool/littleboss\n. . .\n\n\n\nNow Big Boss has the same amount available, 776 MB, but we just have cut\n500 MB of space from Little Boss. And this space is reserved for Big\nBoss. He is happy now. After you are done with this, destroy both\nfilesystems to clean up the pool for future exercises.\n\n\nroot@solaris:~# zfs destroy labpool/bigboss\nroot@solaris:~# zfs destroy labpool/littleboss",
            "title": "ZFS Reservations"
        },
        {
            "location": "/zfs/zfs_shadow/index.html",
            "text": "You may have to migrate your data to a new location. For instance, you\njust have connected a new disk array with really fast disks and you want\nto move your data from the old array with slow disks. Or, you may want\nto turn on compression on the file system and you know that compression\nonly works for future data, not for existing data. To make all data\ncompressed you have to re-write all your data. If your dataset is large,\nit may take significant time. You don't want to wait, you want to start\nusing your data as if it was already in the new location. ZFS Shadow\nMigration is created exactly for this situation.\n\n\nFor this exercise we will use our manual pages directory. First, we will\ncreate a separate file system and copy all manual pages there. We'll\nchange the MANPATH variable to make sure we are using manual files from\nthat file system. After that, we will create a new file system on our\nZFS pool labpool and configure it for shadow migration. We will change\nMANPATH again to point to that new file system and check if we can read\nsystem manuals while the data is being migrated. Shadow migration is\ncreated as a separate package and separate service. To use it, we have\nto install the package and enable the service.\n\n\nroot@solaris:~# pkg install shadow-migration\nPackages to install: 1\nCreate boot environment: No\nCreate backup boot environment: No\nServices to change: 1\nDOWNLOAD PKGS FILES XFER (MB) SPEED\nCompleted 1/1 14/14 0.2/0.2 734k/s\nPHASE ITEMS\nInstalling new actions 39/39\nUpdating package state database Done\nUpdating image state Done\nCreating fast lookup database Done\nroot@solaris:~# svcadm enable shadowd\nStart with creating a file system and copying the manuals there:\nroot@solaris:~# zfs create rpool/mancopy\nroot@solaris:~# cp -rp /usr/share/man/* /rpool/mancopy\nSet MANPATH and try to read a manual page:\nroot@solaris:~# export MANPATH=/rpool/mancopy\nroot@solaris:~# man ls\nReformatting page. Please Wait... done\n\n\n\nHint: If you want to be absolutely sure that the man utility uses your\nfile system instead of the default one, use this powerful script from\nDTrace Toolkit (open another terminal window, become root, run the\nfollowing command and then in your first terminal window run \nman ls\n):\n\n\nroot@solaris:~# /usr/dtrace/DTT/opensnoop -n man\n. . .\n0 2785 man 5 /rpool/mancopy/man1/ls.1\n. . .\n\n\n\nNow create a new file system on labpool and set it as a shadow of our\nmancopy. Before doing that, change rpool/mancopy to read-only. It's a\nrequirement for shadow migration.\n\n\nroot@solaris:~# zfs set readonly=on rpool/mancopy\nroot@solaris:~# zfs create -o shadow=file:///rpool/mancopy labpool/shadowman\nUse the 'shadowstat' command to watch the migration progress.\nroot@solaris:~# shadowstat\nEST\nBYTES BYTES ELAPSED\nDATASET XFRD LEFT ERRORS TIME\nlabpool/shadowman 18.8M - - 00:01:10\n^Croot@solaris:~#\n\n\n\nAnd now, before the migration process has finished (we have a little bit\nover 100MB to copy), change MANPATH to the new file system and try 'man\nls' again.\n\n\nroot@solaris:~# export MANPATH=/labpool/shadowman\nroot@solaris:~# man ls\n\n\n\nAgain, just to check if you are really accessing the new location, in\nanother window run the \nopensnoop\n script. You may want to watch the\nprocess using \nshadowstat\n until you see \nNo migrations in progress\n.\n\n\nInteresting to note that the new filesystem (\nlabpool/shadowman\n) is not\nread-only, you can use it for reading and writing right after it was\ncreated.\n\n\nroot@solaris:~# zfs get readonly labpool/shadowman\nNAME PROPERTY VALUE SOURCE\nlabpool/shadowman readonly off default\n\n\n\nYou can read more about ZFS Shadow migration here:\n\nhttp://www.oracle.com/technetwork/articles/servers-storage-admin/howto-migrate-s11-datashadow-%0A1866521.html\n \n\n\nhttps://blogs.oracle.com/eschrock/entry/shadow_migration\n \n\n\nhttps://blogs.oracle.com/eschrock/entry/shadow_migration_internals",
            "title": "ZFS Shadow Migration"
        },
        {
            "location": "/zfs/zfs_cleanup/index.html",
            "text": "After we have finished this ZFS Lab, let's clean up what we have created\nso far. After all, our ZFS skill set won't be complete without knowing\nhow to destroy ZFS pools.\n\n\nroot@solaris:~# zpool destroy ddpool\nroot@solaris:~# zpool destroy labpool\nroot@solaris:~# zpool destroy backuppool\n\n\n\nTake a note how easy it is to destroy a ZFS pool. Use extreme care when\nusing the \"destroy\" command.",
            "title": "Lab Cleanup"
        },
        {
            "location": "/be/be/index.html",
            "text": "Boot Environments Intro\n\n\nBoot Environments feature helps you to have several separate environments with different Solaris versions (e.g. 11.2.7, 11.3.12. etc.), different package versions, different system configurations. Typically they are used to update Solaris systems (most of the work is done by IPS behind the scenes, but you can learn the mechanics of the process in the \nIPS Lab\n.\n\n\nBoot environments use ZFS snapshot/clone technology so they can be created quickly and they don't consume storage space until you make changes. You can have as many Boot Environments as you want and keep different system configurations in them. By default, every time you update Solaris OS, a new boot environment is created (you can specify its name).  \n\n\nProtection from Human Errors\n\n\nTask:\n You want to make updates to your system, but you want to be\nable to return back to the previous state.\n\n\nLab:\n We will use a Solaris 11 feature called Boot Environments.\nWe'll create an extra boot environment as a backup (think saving your\nstate in a shooting game). Then we'll make some fatal mistakes which\nmake our system unbootable. After we failed to boot our default boot\nenvironment, we'll boot into the backup BE.\n\n\nThe only command you want to know to work with Boot Environments is\n\nbeadm(1M)\n. Start with showing all boot environments in the system:\n\n\nroot@solaris:~# beadm list\nBE      Active Mountpoint Space Policy Created          \n--      ------ ---------- ----- ------ -------          \nsolaris NR     /          6.87G static 2011-11-14 13:13 \n\n\n\nCreate a new boot environment:\n\n\nroot@solaris:~# beadm create solaris-backup\n\n\n\nCheck the status again:\n\n\nroot@solaris:~# beadm list\nBE          Active Mountpoint Space   Policy Created          \n--          ------ ---------- -----   ------ -------          \nsolaris       NR    /          7.13G  static 2011-11-14 13:13 \nsolaris-backup      -        126.14M  static 2011-11-15 11:09 \n\n\n\nNow pretend you are making some changes in the system configuration,\ncreating and removing directories, and... Somebody has distracted you\nand instead of removing a temporary directory, you have typed:\n\n\nroot@solaris:~# rm -rf /usr \n\n\n\nWhat??? You just have destroyed the whole \n/usr\n directory! You have\nkilled your system! Try \nls\n or \nps\n or any normal command. What do\nyou see? Even if you try to reboot your system using \ninit 6\n, the\nresult is the same: \nCommand not found\n. Try to \"Power Off\" your virtual\nmachine and reboot it\nyou'll see it won't boot. Don't wait too long,\n3\n5 minutes is enough to be sure that it doesn't boot. Hint: While\nwaiting for the system to reboot, press the \nESC\n key to see the error\nmessages.\n\n\nNo worries! We have a backup! And you don't have to go and find the\nbackup tape in your fireproof cabinet, go through all the hassles of\nrestoring the unbootable system... Just when your VirtualBox VM shows\nyou the GRUB menu, choose the \nsolaris-backup\n item instead of the\n\nOracle Solaris 11.3\n which is the default.\n\n\n\n\nSPARC version\nIf you are using a SPARC system for this lab (for\nexample, in Oracle Solutions Center we use logical domains for that)\nthen you need console access to be able to choose a different Boot\nEnvironment. In the case of LDoms, you have to login in the Control\nDomain and stop your guest domain with \nldm stop lab0\n (or whatever\ndomain name you are using). Then make sure the domain will not\nauto-boot: \nldm set-var auto-boot\\?=false lab0\n and start the domain\nagain: \nldm start lab0\n. Now you can access the domain's console via\n\ntelnet localhost 5000\n. You will see the familiar \nok\n prompt. Then\nuse \nboot -L\n to list existing Boot Environments.\n\n\n{0} ok boot -L\nBoot device: /virtual-devices@100/channel-devices@200/disk@0:a  File and args: -L\n1 solaris\n2 solaris-backup\nSelect environment to boot: [ 1 - 2 ]: 2\n\nTo boot the selected entry, invoke:\nboot [] -Z rpool/ROOT/solaris-backup\n\nProgram terminated\n{0} ok boot -Z rpool/ROOT/solaris-backup\n\n\n\n\n\n\n\nIt boots again! What a relief!\n\n\nWhat now? List your boot environments again:\n\n\nroot@solaris:~# beadm list\nBE          Active Mountpoint Space   Policy Created          \n--          ------ ---------- -----   ------ -------          \nsolaris        R    /          7.13G  static 2011-11-14 13:13 \nsolaris-backup N      -        126.14M  static 2011-11-15 11:09 \n\n\n\nWhat does it mean? It means that you are now running \nsolaris-backup\n\nboot environment, but at the next reboot the system will try to boot the\n\nsolaris\n boot environment, which is corrupted. Let's fix that. The plan\nis: activate your current BE, delete the \nsolaris\n BE (as it's corrupted\nanyway), create a new \nsolaris\n BE and activate it.\n\n\nroot@solaris:~# beadm activate solaris-backup\nroot@solaris:~# beadm destroy solaris\nroot@solaris:~# beadm create solaris\nroot@solaris:~# beadm activate solaris\n\n\n\nCheck your work:\n\n\nroot@solaris:~# beadm list\nBE             Active Mountpoint Space  Policy Created          \n--             ------ ---------- -----  ------ -------          \nsolaris        R      -          6.05G  static 2012-11-14 16:35 \nsolaris-backup N      /          275.5K static 2012-11-14 16:22 \n\n\n\nNow you've got everything back. You can safely reboot into \nsolaris\n BE\nor you can continue working in \nsolaris-backup\n until next reboot.\n\n\nBoot Environments have many applications: you can update your system,\ninstalling packages into inactive boot environment; create boot\nenvironment snapshots etc. We leave it for your homework.",
            "title": "Boot Environments"
        },
        {
            "location": "/be/be/index.html#boot-environments-intro",
            "text": "Boot Environments feature helps you to have several separate environments with different Solaris versions (e.g. 11.2.7, 11.3.12. etc.), different package versions, different system configurations. Typically they are used to update Solaris systems (most of the work is done by IPS behind the scenes, but you can learn the mechanics of the process in the  IPS Lab .  Boot environments use ZFS snapshot/clone technology so they can be created quickly and they don't consume storage space until you make changes. You can have as many Boot Environments as you want and keep different system configurations in them. By default, every time you update Solaris OS, a new boot environment is created (you can specify its name).",
            "title": "Boot Environments Intro"
        },
        {
            "location": "/be/be/index.html#protection-from-human-errors",
            "text": "Task:  You want to make updates to your system, but you want to be\nable to return back to the previous state.  Lab:  We will use a Solaris 11 feature called Boot Environments.\nWe'll create an extra boot environment as a backup (think saving your\nstate in a shooting game). Then we'll make some fatal mistakes which\nmake our system unbootable. After we failed to boot our default boot\nenvironment, we'll boot into the backup BE.  The only command you want to know to work with Boot Environments is beadm(1M) . Start with showing all boot environments in the system:  root@solaris:~# beadm list\nBE      Active Mountpoint Space Policy Created          \n--      ------ ---------- ----- ------ -------          \nsolaris NR     /          6.87G static 2011-11-14 13:13   Create a new boot environment:  root@solaris:~# beadm create solaris-backup  Check the status again:  root@solaris:~# beadm list\nBE          Active Mountpoint Space   Policy Created          \n--          ------ ---------- -----   ------ -------          \nsolaris       NR    /          7.13G  static 2011-11-14 13:13 \nsolaris-backup      -        126.14M  static 2011-11-15 11:09   Now pretend you are making some changes in the system configuration,\ncreating and removing directories, and... Somebody has distracted you\nand instead of removing a temporary directory, you have typed:  root@solaris:~# rm -rf /usr   What??? You just have destroyed the whole  /usr  directory! You have\nkilled your system! Try  ls  or  ps  or any normal command. What do\nyou see? Even if you try to reboot your system using  init 6 , the\nresult is the same:  Command not found . Try to \"Power Off\" your virtual\nmachine and reboot it you'll see it won't boot. Don't wait too long,\n3 5 minutes is enough to be sure that it doesn't boot. Hint: While\nwaiting for the system to reboot, press the  ESC  key to see the error\nmessages.  No worries! We have a backup! And you don't have to go and find the\nbackup tape in your fireproof cabinet, go through all the hassles of\nrestoring the unbootable system... Just when your VirtualBox VM shows\nyou the GRUB menu, choose the  solaris-backup  item instead of the Oracle Solaris 11.3  which is the default.   SPARC version If you are using a SPARC system for this lab (for\nexample, in Oracle Solutions Center we use logical domains for that)\nthen you need console access to be able to choose a different Boot\nEnvironment. In the case of LDoms, you have to login in the Control\nDomain and stop your guest domain with  ldm stop lab0  (or whatever\ndomain name you are using). Then make sure the domain will not\nauto-boot:  ldm set-var auto-boot\\?=false lab0  and start the domain\nagain:  ldm start lab0 . Now you can access the domain's console via telnet localhost 5000 . You will see the familiar  ok  prompt. Then\nuse  boot -L  to list existing Boot Environments.  {0} ok boot -L\nBoot device: /virtual-devices@100/channel-devices@200/disk@0:a  File and args: -L\n1 solaris\n2 solaris-backup\nSelect environment to boot: [ 1 - 2 ]: 2\n\nTo boot the selected entry, invoke:\nboot [] -Z rpool/ROOT/solaris-backup\n\nProgram terminated\n{0} ok boot -Z rpool/ROOT/solaris-backup    It boots again! What a relief!  What now? List your boot environments again:  root@solaris:~# beadm list\nBE          Active Mountpoint Space   Policy Created          \n--          ------ ---------- -----   ------ -------          \nsolaris        R    /          7.13G  static 2011-11-14 13:13 \nsolaris-backup N      -        126.14M  static 2011-11-15 11:09   What does it mean? It means that you are now running  solaris-backup \nboot environment, but at the next reboot the system will try to boot the solaris  boot environment, which is corrupted. Let's fix that. The plan\nis: activate your current BE, delete the  solaris  BE (as it's corrupted\nanyway), create a new  solaris  BE and activate it.  root@solaris:~# beadm activate solaris-backup\nroot@solaris:~# beadm destroy solaris\nroot@solaris:~# beadm create solaris\nroot@solaris:~# beadm activate solaris  Check your work:  root@solaris:~# beadm list\nBE             Active Mountpoint Space  Policy Created          \n--             ------ ---------- -----  ------ -------          \nsolaris        R      -          6.05G  static 2012-11-14 16:35 \nsolaris-backup N      /          275.5K static 2012-11-14 16:22   Now you've got everything back. You can safely reboot into  solaris  BE\nor you can continue working in  solaris-backup  until next reboot.  Boot Environments have many applications: you can update your system,\ninstalling packages into inactive boot environment; create boot\nenvironment snapshots etc. We leave it for your homework.",
            "title": "Protection from Human Errors"
        },
        {
            "location": "/ips/ips/index.html",
            "text": "One of the main new features in Oracle Solaris 11 is a new packaging\nsystem, called IPS (Image Packaging System). In this lab we will explore\nits capabilities and learn how to work with packages from System\nAdministrator's perspective.\n\n\nIPS uses a network-based repository model which is also used by modern Linux distributions (\nyum/rpm\n in Red Hat Enterprise Linux and Oracle Linux, \napt\n in Debian and Ubuntu, etc.). All the information about package and its dependencies is stored in the package's metadata. IPS is responsible for packages' and overall system's integrity, so it's not a sysadmin's headache anymore. \n\n\nAnother good news for sysadmins is that in IPS there are no \"patches\". No more long README files, no more guessing which patch to install first. Everything is done by a single command (\npkg update\n) and IPS takes care of everything.\n\n\nYou can read more about IPS concepts in the official \nOracle Solaris 11 documentation\n\n\nIn this lab we will address the following typical situations.\n\n\n\n\n\n\n\n\nSituation\n\n\nExercise\n\n\n\n\n\n\n\n\n\n\nYou have an ISO file with Solaris 11 repository and you have to install it and share for your local network\n\n\nIPS Repository\n\n\n\n\n\n\nYou need a package and you want to find it in the repository and install it\n\n\nIPS Packages\n\n\n\n\n\n\nYou want to make sure that your packages weren't changed (maliciously or by mistake)\n\n\nIPS Verify\n\n\n\n\n\n\nYou want to install a new package, but in a separate Boot Environment\n\n\nIPS and BE\n\n\n\n\n\n\nYou have in-house development and you want to use Solaris mechanism to install and update your local applications\n\n\nIPS for Developers\n\n\n\n\n\n\n\n\nAfter you are done with the exercises you can check you IPS history:\n\n\nroot@solaris:~# pkg history\n\n\n\nAnd the verbose history:\n\n\nroot@solaris:~# pkg history -l|more\n\n\n\nThis might be helpful if you have a team of sysadmins and you want to know what was done before you.",
            "title": "IPS Intro"
        },
        {
            "location": "/ips/ips_repo/index.html",
            "text": "Repository installation\n\n\n\n\nNote\n\n\nYou don't have to perform this lab, if you have access to the\nonline repository at oracle.com. To test that, type:\n\npkg list -a | wc -l\n. If the output shows some large number (more\nthan 3500) then you have access to the full Solaris repository and can\nskip this step. Even in this case you can choose to perform this\nexercise for training purposes.\n\n\n\n\nBefore we started working with IPS we have to install a repository into\nour Oracle Solaris instance. It's a preparatory exercise, but you may\nuse the same commands to install an internal repository in your network.\nFor this lab we have created a small repository, just big enough to\nperform our lab exercises. In real life you will download and install\nthe full repository from oracle.com. With a proper support contract from\nOracle you will get access to the support repository and be able to\nupdate your local repository periodically.\n\n\nOpen the USB stick provided by your instructor, or the local folder\nwhere you have copied all the files to. There is a file called\n\nrepo.iso\n. We have to add it as a virtual DVD to the lab virtual\nmachine. First, shutdown the Solaris instance in the VM. Then go to the\nLab VM's settings and click on \nStorage\n.\n\n\n\n\nThen click on \nAdd optical drive\n icon:\n\n\n\n\nAnd on the next dialog click on \nChoose disk\n:\n\n\n\n\nThen find the \nrepo.iso\n file in your file system and add it to the\nvirtual machine.\n\n\n\n\nAfter this is done, start the virtual machine the usual way. After you\nlogged in as \nlab\n, check if you see the mounted CD-ROM on your desktop.\n\n\n\n\nIf everything is OK, open the terminal, become root and configure the\nrepository. Start with creating a separate ZFS file system for that.\n\n\nroot@solarislab:~# zfs create -o mountpoint=/repo -o compress=on rpool/repo\n\n\n\nCheck if you have your repository on that CDROM file:\n\n\nroot@solarislab:~# ls /media/CDROM\npkg5.repository  publisher\n\n\n\nCopy all the files from the repository to the new file system:\n\n\nroot@solarislab:~# rsync -aP /media/CDROM/* /repo\n\n\n\nAnd then update the repository index and configure the IPS server:\n\n\nroot@solarislab:~# pkgrepo rebuild -s /repo\nroot@solarislab:~# svccfg -s application/pkg/server setprop pkg/inst_root=/repo\nroot@solarislab:~# svcadm refresh application/pkg/server\nroot@solarislab:~# svcadm enable application/pkg/server\nroot@solarislab:~# pkg set-publisher -G '*' -M '*' -g http://10.0.2.15/ solaris\n\n\n\nCheck available packages. There should be about 900 of then (as we said,\nit's a small repository). If you see something significantly different,\ndiscuss it with your instructor and decide on how to move ahead.\n\n\nroot@solarislab:~# pkg list -a | wc\n 977    2932   79174\n\n\n\nWell, now the system is ready for the IPS and Virtualization labs.",
            "title": "IPS Lab Preparation"
        },
        {
            "location": "/ips/ips_repo/index.html#repository-installation",
            "text": "Note  You don't have to perform this lab, if you have access to the\nonline repository at oracle.com. To test that, type: pkg list -a | wc -l . If the output shows some large number (more\nthan 3500) then you have access to the full Solaris repository and can\nskip this step. Even in this case you can choose to perform this\nexercise for training purposes.   Before we started working with IPS we have to install a repository into\nour Oracle Solaris instance. It's a preparatory exercise, but you may\nuse the same commands to install an internal repository in your network.\nFor this lab we have created a small repository, just big enough to\nperform our lab exercises. In real life you will download and install\nthe full repository from oracle.com. With a proper support contract from\nOracle you will get access to the support repository and be able to\nupdate your local repository periodically.  Open the USB stick provided by your instructor, or the local folder\nwhere you have copied all the files to. There is a file called repo.iso . We have to add it as a virtual DVD to the lab virtual\nmachine. First, shutdown the Solaris instance in the VM. Then go to the\nLab VM's settings and click on  Storage .   Then click on  Add optical drive  icon:   And on the next dialog click on  Choose disk :   Then find the  repo.iso  file in your file system and add it to the\nvirtual machine.   After this is done, start the virtual machine the usual way. After you\nlogged in as  lab , check if you see the mounted CD-ROM on your desktop.   If everything is OK, open the terminal, become root and configure the\nrepository. Start with creating a separate ZFS file system for that.  root@solarislab:~# zfs create -o mountpoint=/repo -o compress=on rpool/repo  Check if you have your repository on that CDROM file:  root@solarislab:~# ls /media/CDROM\npkg5.repository  publisher  Copy all the files from the repository to the new file system:  root@solarislab:~# rsync -aP /media/CDROM/* /repo  And then update the repository index and configure the IPS server:  root@solarislab:~# pkgrepo rebuild -s /repo\nroot@solarislab:~# svccfg -s application/pkg/server setprop pkg/inst_root=/repo\nroot@solarislab:~# svcadm refresh application/pkg/server\nroot@solarislab:~# svcadm enable application/pkg/server\nroot@solarislab:~# pkg set-publisher -G '*' -M '*' -g http://10.0.2.15/ solaris  Check available packages. There should be about 900 of then (as we said,\nit's a small repository). If you see something significantly different,\ndiscuss it with your instructor and decide on how to move ahead.  root@solarislab:~# pkg list -a | wc\n 977    2932   79174  Well, now the system is ready for the IPS and Virtualization labs.",
            "title": "Repository installation"
        },
        {
            "location": "/ips/ips_install/index.html",
            "text": "Task:\n You want to find and install a package from Solaris\nrepository.\n\n\nLab:\n We will learn some basic IPS commands used to look for a\npackage, inquire about its content etc. When we found the package we\nneeded, we install it.\n\n\nLet's imagine we want to do some network load testing, so we want to run\nthe utility called \niperf\n. Try to run this command and find out that\nit's not installed (don't be surprised by \ncommand not found\n message:\n\n\nroot@solaris:~# iperf\n\n\n\nSo the first thing we do is show our current publisher. The publisher is\nwhere the IPS repository is located. It can be a local directory, an NFS\nmount point, an internal http ot https server, or an Internet repository\nlike \nhttp://pkg.oracle.com/solaris\n. You system can have several\npublishers configured.\n\n\nroot@solaris:~# pkg publisher\n\n\n\nOK, it seems we are going to use our local publisher installed in our\nSolaris system.\n\n\nNow we list our installed packages\n\n\nroot@solaris:~# pkg list | more\n\n\n\nLet's see how many packages are installed\n\n\nroot@solaris:~# pkg list | wc\n\n\n\nNow let's see how many packages are available in the repository\n\n\nroot@solaris:~# pkg list -a | wc\n\n\n\nAs you can see, our local repository is pretty small, it contains only\nthe packages we need for this lab. If you have changed the publisher to\n\nhttp://pkg.oracle.com/solaris/release\n as it's described in the\nintroduction, you would see many more packages.\n\n\nLet's now do a local search for iperf (among the installed packages). It\nwill find nothing:\n\n\nroot@solaris:~# pkg search -l iperf\n\n\n\nYou may get the following message after this command:\n\n\n    pkg: Search performance is degraded.\n    Run 'pkg rebuild-index' to improve search speed. \n\n\n\nSo, if you really want to improve search speed, then go ahead and\nrebuild the index as instructed. However, it is not critical for the\nrest of the lab.\n\n\nNow do the same search, without the \n-l\n flag so it goes to the\nrepository:\n\n\nroot@solaris:~# pkg search iperf\n\n\n\nNext we get some information about the package like date of creation,\nversion number, etc.\n\n\nroot@solaris:~# pkg info -r iperf\n\n\n\nAnd we can see exactly what files make up the package:\n\n\nroot@solaris:~# pkg contents -r iperf\nroot@solaris:~# pkg search benchmark/iperf:depend::\n\n\n\nNow, after we have learned everything about the package we are about to\ninstall, we can try a \"dry run\" before actually installing it:\n\n\nroot@solaris:~# pkg install -n iperf\nroot@solaris:~# pkg install -nv iperf\n\n\n\nIf we are satisfied with the results, we can install the \niperf\n\npackage:\n\n\nroot@solaris:~# pkg install iperf\n\n\n\nHere we demonstrate what kind of metadata is kept in IPS:\n\n\nroot@solaris:~# pkg contents -t file -o owner,group,mode,pkg.size,path iperf\n\n\n\nNow we run iperf to show it's now found:\n\n\nroot@solaris:~# iperf",
            "title": "IPS Package Installation"
        },
        {
            "location": "/ips/ips_verify/index.html",
            "text": "Show how we can verify a package has not been compromised:\n\n\nroot@solaris:~# pkg verify iperf\n\n\n\nLet's check the permission bits of \niperf\n:\n\n\nroot@solaris:~# ls -l /bin/iperf\n\n\n\nLet's say someone changed the permissions bits like this:\n\n\nroot@solaris:~# chmod 777 /bin/iperf\n\n\n\nLet's verify again, and it will return an error:\n\n\nroot@solaris:~# pkg verify iperf\n\n\n\nNow let's fix this package:\n\n\nroot@solaris:~# pkg fix iperf\n\n\n\nThe next few lines show an interesting way of getting a package name\nfrom a arbitrary file. Let's take \nvi\n editor for test. We first get a\nSHA1 digest of \n/usr/bin/vi\n\n\nroot@solaris:~# digest -a sha1 /usr/bin/vi\n\n\n\nSince the digest is saved in the package DB, we can search for that hash\nand see what matches it:\n\n\nroot@solaris:~# pkg search -l f2495fa19fcc4b8a403e0bd4fef809d031296c68\n\n\n\nHow can we use it? Imagine someone has renamed some important file to\nhide his tracks. Using this method we can find out the original name of\nthe file.",
            "title": "IPS Package Verification"
        },
        {
            "location": "/ips/ips_be/index.html",
            "text": "Another important feature of IPS is the ability to perform all package\nrelated operations not only on the current boot environment, but also on\na mounted one. Imagine you want to update your system, but you want to\nkeep your current state untouched to be able to return back to safety in\ncase something goes wrong. Also you want to minimize the downtime.\n\n\nCreate a new boot environment for the updated system:\n\n\nroot@solaris:~# beadm create solaris-updated\n\n\n\nIn real life you might want to use some naming policy for the BEs, like\ntimestamping them.\n\n\nNow mount this boot environment in your file system:\n\n\nroot@solaris:~# beadm mount solaris-updated /mnt\n\n\n\nNow you can perform any package operations with this mounted boot\nenvironment. As we don't have updates in our repository, we just install\na package (the same iperf package), check that it's not available in our\ncurrent BE and then reboot the system with the updated BE and make sure\nthe package is installed there.\n\n\nroot@solaris:~# pkg -R /mnt install iperf\n\n\n\nNow make sure the new boot environment is active on reboot:\n\n\nroot@solaris:~# beadm activate solaris-updated\n\n\n\nThen reboot the system and check if \niperf\n is available.\n\n\nImagine how much downtime you can save when using this method to update\nyour systems!",
            "title": "IPS and Boot Environments"
        },
        {
            "location": "/ips/ips_dev/index.html",
            "text": "Application packaging with IPS\n\n\nThe Image Packaging System (IPS) is the network based package management\nsystem included in Oracle Solaris 11. It provides a framework for\ncomplete software lifecycle management such as installation, upgrade and\nremoval of software packages. IPS takes advantage of ZFS using ZFS boot\nenvironments, such that administrators can update a system while running\nproduction services and then boot into the new boot environment when a\nplanned maintenance window comes along. IPS uses package repositories to\ninstall software over the network, and provides for automatic package\ndependencies.\n\n\nIn this session we will be taking the zoneplot, a command line utility\nfor measuring and plotting zone statistics, and creating an IPS package\nfor it that we can install onto the system. This is a very simple\nexample to give you an overview of the packaging process, but it's\nrecommended you take at look at the \nIPS Developer\nGuide\n.\n\n\nFor this package we will need two scripts. First accepts a Unix pipe\nstream and shows its data using gnuplot. Second script converts the\nzonestat outpout into a stream readable by the first script. Both\nscripts are publicly available. Download them from the Internet:\n\n\nroot@solaris:~# wget http://www.lysium.de/sw/driveGnuPlotStreams.pl\nroot@solaris:~# wget http://zoneplot.googlecode.com/files/zoneplot\n\n\n\n\nIPS uses package manifests to describe how a package is assembled - what\nfiles or directories are included, what dependencies the package has,\nand basic information about a package such as name, version and\ndescription. Fortunately we can generate much of this manifest using IPS\nand pointing it to a location, or proto area, which contains all the\nfiles and directories that we want to include in this package. We'll\nstart by creating this proto area and laying things out how we'd expect\nthe package to look like:\n\n\nroot@solaris:~# mkdir -p PROTO/usr/bin\n\n\n\n\nIn this case we're creating a PROTO directory in $HOME. Within that\ndirectory, we have a /usr/bin layout. We'll copy our scripts into this\nproto area and make them executable:\n\n\nroot@solaris:~# cp zoneplot driveGnuPlotStreams.pl PROTO/usr/bin\nroot@solaris:~# chmod a+x PROTO/usr/bin/*\n\n\n\n\nWe can see in the following graphic how our directory is now laid out\n\n\n zoneplot     driveGnuPlotStreams.pl\n      \\                   /\n              bin\n               |\n              usr\n               |\n               PROTO\n\n\n\n\nLet's generate an initial manifest:\n\n\nroot@solaris:~# pkgsend generate PROTO \n zoneplot.p5m.gen\n\n\n\n\nLet's look at the contents:\n\n\nroot@solaris:~# cat zoneplot.p5m.gen\ndir group=bin mode=0755 owner=root path=usr\ndir group=bin mode=0755 owner=root path=usr/bin\nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot\n\n\n\n\nAs you'll see here there are two sets of 'actions' - two directory\nactions that describe usr and usr/bin directories, and two file actions\nthat describe \nzoneplot\n and \ndriveGnuPlotStreams.pl\n. These are the\ncontents of our package. Usual IPS practice encourages us to not include\ndirectory actions that are already defined on the system. We will remove\nthem.\n\n\nroot@solaris:~# vi zoneplot.p5m.gen\nroot@solaris:~# cat zoneplot.p5m.gen\nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot\n\n\n\n\nThe next thing that we need to do is to add some basic meta-information\nabout the package - including package name, package version and package\ndescription. Add the following lines to our manifest:\n\n\nroot@solaris:~# vi zoneplot.p5m.gen\nroot@solaris:~# cat zoneplot.p5m.gen\nset name=pkg.fmri value=zoneplot@1.0,5.11-0\nset name=pkg.summary value=\nzoneplot utility\n\nset name=pkg.description value=\nUtility to plot the output of zone statistics\n\nset name=info.classification value=\norg.opensolaris.category.2008:Applications/System Utilities\n\nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot\n\n\n\n\nIPS has automatic package dependency checking for all package management\noperations on the system. For example, if a package requires other\npackages to work it will automatically install these. We will do the\nsame for zoneplot. We will first try and detect what package\ndependencies this package might have using the pkgdepend generate\ncommand:\n\n\nroot@solaris:~# pkgdepend generate -md PROTO zoneplot.p5m.gen \n zoneplot.p5m.dep\nroot@solaris:~# cat zonepot.p5m.dep\nset name=pkg.fmri value=zoneplot@1.0,5.11-0\nset name=pkg.summary value=\nzoneplot utility\n\nset name=pkg.description value=\nUtility to plot the output of zone statistics\n\nset name=info.classification value=\norg.opensolaris.category.2008:Applications/System Utilities\n\nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=__TBD pkg.debug.depend.file=perl pkg.debug.depend.path=usr/bin pkg.debug.depend.reason=usr/bin/driveGnuPlotStreams.pl pkg.debug.depend.type=script type=require\ndepend fmri=__TBD pkg.debug.depend.file=bash pkg.debug.depend.path=usr/bin pkg.debug.depend.reason=usr/bin/zoneplot pkg.debug.depend.type=script type=require\n\n\n\n\nYou'll see that we have detected that there are two files in this\npackage (zoneplot and driveGnuPlotStreams.pl) that have external\ndependencies. zoneplot is a bash script and thus depends on bash.\ndriveGnuPlotStreams.pl is a perl script and thus depends on perl. While\nboth of these are already installed by default on Oracle Solaris 11,\nit's good practice to make sure we define these dependencies.\n\n\nThe next step is resolving these dependencies into the packages they are\ndelivered in. We need to determine what packages bash and perl are a\npart of. We use the pkgdepend resolve command to achieve this.\n\n\nroot@solaris:~# pkgdepend resolve -m zoneplot.p5m.dep\nroot@solaris:~# cat zoneplot.p5m.dep.res (NOTE the .res extension!)\nset name=pkg.fmri value=zoneplot@1.1,5.11-0\nset name=pkg.summary value=\nzoneplot utility\n\nset name=pkg.description value=\nUtility to plot the output of zone statistics\n\nset name=info.classification value=\norg.opensolaris.category.2008:Applications/System Utilities\n\nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=pkg:/runtime/perl-512@5.12.4-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/shell/bash@4.1.9-0.175.1.0.0.24.0 type=require\n\n\n\n\nWe can see that these dependencies have been resolved to\npkg:/runtime/perl-512 and pkg:/shell/bash. While IPS does a good job at\ntrying to detect dependencies by looking at files (ELF headers, #!\nscript definitions, etc.), sometimes we will have to manually provide\nadditional dependencies. In this case, we know that this utility depends\non gnuplot to plot the statistics graphically, so we will add this to\nour manifest.\n\n\nFirst we have to figure out the gnuplot's FMRI:\n\n\nroot@solaris:~# pkg search -o pkg.shortfmri gnuplot\nPKG.SHORTFMRI\npkg:/image/gnuplot@4.6.0-0.175.1.0.0.24.0\n\n\n\n\nNow copy this FMRI into the \nzoneplot.p5m.dep.res\n file as following:\n\n\nroot@solaris:~# vi zoneplot.p5m.dep.res\nroot@solaris:~# cat zoneplot.p5m.dep.res\nset name=pkg.fmri value=zoneplot@1.1,5.11-0\nset name=pkg.summary value=\nzoneplot utility\n\nset name=pkg.description value=\nUtility to plot the output of zone statistics\n\nset name=info.classification value=\norg.opensolaris.category.2008:Applications/System Utilities\n\nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=pkg:/runtime/perl-512@5.12.4-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/shell/bash@4.1.9-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/image/gnuplot@4.6.0,5.11-0.175.1.0.0.24.0 type=require\n\n\n\n\nNow that we have our PROTO area and manifest completed, it is now time\nto publish this package to an IPS repository. First we will need to\ncreate a repository to host this package - it's best practice not to\npublish to the existing solaris default publishers even if you have a\ncopy of them locally.\n\n\nLet's first create a ZFS data set to host our repository:\n\n\nroot@solaris:~# zfs create -o mountpoint=/repository rpool/repository\n\n\n\n\nAnd now create an IPS package repository there:\n\n\nroot@solaris:~# pkgrepo create /repository\n\n\n\n\nWe will need to set the publisher prefix of this repository so that we\ncan add it to our publisher configuration on our system. We will call it\n'zoneplot':\n\n\nroot@solaris:~# pkgrepo -s /repository set publisher/prefix=zoneplot\n\n\n\n\nNow we are ready to publish our package:\n\n\nroot@solaris:~# pkgsend -s /repository publish -d PROTO zoneplot.p5m.dep.res\npkg://zoneplot/zoneplot@1.0,5.11-0:20130524T160043Z\nPUBLISHED\n\n\n\n\nWe can confirm this by taking a look at the status of the repository:\n\n\nroot@solaris:~# pkgrepo -s /repository info\nPUBLISHER  PACKAGES  STATUS           UPDATED\nzoneplot         1                      online                2013-05-24T16:00:43.695049Z\n\n\n\n\nWe can query the package now within the repository:\n\n\nroot@solaris:~# pkg info -g /repository zoneplot\n          Name: zoneplot\n       Summary: zoneplot utility\n   Description: Utility to plot the output of zone statistics\n      Category: Applications/System Utilities\n         State: Not installed\n     Publisher: zoneplot\n       Version: 1.0\n Build Release: 5.11\n        Branch: 0\nPackaging Date: May 24, 2013 04:00:43 PM\n          Size: 6.04 kB\n          FMRI: pkg://zoneplot/zoneplot@1.2,5.11-0:20130524T160043Z\n\n\n\n\nLet's go ahead and add this publisher to our configuration:\n\n\nroot@solaris:~# pkg set-publisher -p /repository\nroot@solaris:~# pkg publisher\nPUBLISHER            TYPE      STATUS P LOCATION\nsolaris                        origin     online      F http://pkg.oracle.com/solaris/release\nzoneplot                       origin     online      F file:///repository\n\n\n\n\nAnd now finally install zoneplot (you will see that it will pull in a\nnumber of package dependencies):\n\n\nroot@solaris:~# pkg install zoneplot",
            "title": "IPS for Developers"
        },
        {
            "location": "/ips/ips_dev/index.html#application-packaging-with-ips",
            "text": "The Image Packaging System (IPS) is the network based package management\nsystem included in Oracle Solaris 11. It provides a framework for\ncomplete software lifecycle management such as installation, upgrade and\nremoval of software packages. IPS takes advantage of ZFS using ZFS boot\nenvironments, such that administrators can update a system while running\nproduction services and then boot into the new boot environment when a\nplanned maintenance window comes along. IPS uses package repositories to\ninstall software over the network, and provides for automatic package\ndependencies.  In this session we will be taking the zoneplot, a command line utility\nfor measuring and plotting zone statistics, and creating an IPS package\nfor it that we can install onto the system. This is a very simple\nexample to give you an overview of the packaging process, but it's\nrecommended you take at look at the  IPS Developer\nGuide .  For this package we will need two scripts. First accepts a Unix pipe\nstream and shows its data using gnuplot. Second script converts the\nzonestat outpout into a stream readable by the first script. Both\nscripts are publicly available. Download them from the Internet:  root@solaris:~# wget http://www.lysium.de/sw/driveGnuPlotStreams.pl\nroot@solaris:~# wget http://zoneplot.googlecode.com/files/zoneplot  IPS uses package manifests to describe how a package is assembled - what\nfiles or directories are included, what dependencies the package has,\nand basic information about a package such as name, version and\ndescription. Fortunately we can generate much of this manifest using IPS\nand pointing it to a location, or proto area, which contains all the\nfiles and directories that we want to include in this package. We'll\nstart by creating this proto area and laying things out how we'd expect\nthe package to look like:  root@solaris:~# mkdir -p PROTO/usr/bin  In this case we're creating a PROTO directory in $HOME. Within that\ndirectory, we have a /usr/bin layout. We'll copy our scripts into this\nproto area and make them executable:  root@solaris:~# cp zoneplot driveGnuPlotStreams.pl PROTO/usr/bin\nroot@solaris:~# chmod a+x PROTO/usr/bin/*  We can see in the following graphic how our directory is now laid out   zoneplot     driveGnuPlotStreams.pl\n      \\                   /\n              bin\n               |\n              usr\n               |\n               PROTO  Let's generate an initial manifest:  root@solaris:~# pkgsend generate PROTO   zoneplot.p5m.gen  Let's look at the contents:  root@solaris:~# cat zoneplot.p5m.gen\ndir group=bin mode=0755 owner=root path=usr\ndir group=bin mode=0755 owner=root path=usr/bin\nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot  As you'll see here there are two sets of 'actions' - two directory\nactions that describe usr and usr/bin directories, and two file actions\nthat describe  zoneplot  and  driveGnuPlotStreams.pl . These are the\ncontents of our package. Usual IPS practice encourages us to not include\ndirectory actions that are already defined on the system. We will remove\nthem.  root@solaris:~# vi zoneplot.p5m.gen\nroot@solaris:~# cat zoneplot.p5m.gen\nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot  The next thing that we need to do is to add some basic meta-information\nabout the package - including package name, package version and package\ndescription. Add the following lines to our manifest:  root@solaris:~# vi zoneplot.p5m.gen\nroot@solaris:~# cat zoneplot.p5m.gen\nset name=pkg.fmri value=zoneplot@1.0,5.11-0\nset name=pkg.summary value= zoneplot utility \nset name=pkg.description value= Utility to plot the output of zone statistics \nset name=info.classification value= org.opensolaris.category.2008:Applications/System Utilities \nfile usr/bin/driveGnuPlotStreams.pl group=bin mode=0755 owner=root path=usr/bin/driveGnuPlotStreams.pl\nfile usr/bin/zoneplot group=bin mode=0755 owner=root path=usr/bin/zoneplot  IPS has automatic package dependency checking for all package management\noperations on the system. For example, if a package requires other\npackages to work it will automatically install these. We will do the\nsame for zoneplot. We will first try and detect what package\ndependencies this package might have using the pkgdepend generate\ncommand:  root@solaris:~# pkgdepend generate -md PROTO zoneplot.p5m.gen   zoneplot.p5m.dep\nroot@solaris:~# cat zonepot.p5m.dep\nset name=pkg.fmri value=zoneplot@1.0,5.11-0\nset name=pkg.summary value= zoneplot utility \nset name=pkg.description value= Utility to plot the output of zone statistics \nset name=info.classification value= org.opensolaris.category.2008:Applications/System Utilities \nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=__TBD pkg.debug.depend.file=perl pkg.debug.depend.path=usr/bin pkg.debug.depend.reason=usr/bin/driveGnuPlotStreams.pl pkg.debug.depend.type=script type=require\ndepend fmri=__TBD pkg.debug.depend.file=bash pkg.debug.depend.path=usr/bin pkg.debug.depend.reason=usr/bin/zoneplot pkg.debug.depend.type=script type=require  You'll see that we have detected that there are two files in this\npackage (zoneplot and driveGnuPlotStreams.pl) that have external\ndependencies. zoneplot is a bash script and thus depends on bash.\ndriveGnuPlotStreams.pl is a perl script and thus depends on perl. While\nboth of these are already installed by default on Oracle Solaris 11,\nit's good practice to make sure we define these dependencies.  The next step is resolving these dependencies into the packages they are\ndelivered in. We need to determine what packages bash and perl are a\npart of. We use the pkgdepend resolve command to achieve this.  root@solaris:~# pkgdepend resolve -m zoneplot.p5m.dep\nroot@solaris:~# cat zoneplot.p5m.dep.res (NOTE the .res extension!)\nset name=pkg.fmri value=zoneplot@1.1,5.11-0\nset name=pkg.summary value= zoneplot utility \nset name=pkg.description value= Utility to plot the output of zone statistics \nset name=info.classification value= org.opensolaris.category.2008:Applications/System Utilities \nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=pkg:/runtime/perl-512@5.12.4-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/shell/bash@4.1.9-0.175.1.0.0.24.0 type=require  We can see that these dependencies have been resolved to\npkg:/runtime/perl-512 and pkg:/shell/bash. While IPS does a good job at\ntrying to detect dependencies by looking at files (ELF headers, #!\nscript definitions, etc.), sometimes we will have to manually provide\nadditional dependencies. In this case, we know that this utility depends\non gnuplot to plot the statistics graphically, so we will add this to\nour manifest.  First we have to figure out the gnuplot's FMRI:  root@solaris:~# pkg search -o pkg.shortfmri gnuplot\nPKG.SHORTFMRI\npkg:/image/gnuplot@4.6.0-0.175.1.0.0.24.0  Now copy this FMRI into the  zoneplot.p5m.dep.res  file as following:  root@solaris:~# vi zoneplot.p5m.dep.res\nroot@solaris:~# cat zoneplot.p5m.dep.res\nset name=pkg.fmri value=zoneplot@1.1,5.11-0\nset name=pkg.summary value= zoneplot utility \nset name=pkg.description value= Utility to plot the output of zone statistics \nset name=info.classification value= org.opensolaris.category.2008:Applications/System Utilities \nfile usr/bin/zoneplot path=usr/bin/zoneplot owner=root group=bin mode=0755\nfile usr/bin/driveGnuPlotStreams.pl path=usr/bin/driveGnuPlotStreams.pl owner=root group=bin mode=0755\ndepend fmri=pkg:/runtime/perl-512@5.12.4-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/shell/bash@4.1.9-0.175.1.0.0.24.0 type=require\ndepend fmri=pkg:/image/gnuplot@4.6.0,5.11-0.175.1.0.0.24.0 type=require  Now that we have our PROTO area and manifest completed, it is now time\nto publish this package to an IPS repository. First we will need to\ncreate a repository to host this package - it's best practice not to\npublish to the existing solaris default publishers even if you have a\ncopy of them locally.  Let's first create a ZFS data set to host our repository:  root@solaris:~# zfs create -o mountpoint=/repository rpool/repository  And now create an IPS package repository there:  root@solaris:~# pkgrepo create /repository  We will need to set the publisher prefix of this repository so that we\ncan add it to our publisher configuration on our system. We will call it\n'zoneplot':  root@solaris:~# pkgrepo -s /repository set publisher/prefix=zoneplot  Now we are ready to publish our package:  root@solaris:~# pkgsend -s /repository publish -d PROTO zoneplot.p5m.dep.res\npkg://zoneplot/zoneplot@1.0,5.11-0:20130524T160043Z\nPUBLISHED  We can confirm this by taking a look at the status of the repository:  root@solaris:~# pkgrepo -s /repository info\nPUBLISHER  PACKAGES  STATUS           UPDATED\nzoneplot         1                      online                2013-05-24T16:00:43.695049Z  We can query the package now within the repository:  root@solaris:~# pkg info -g /repository zoneplot\n          Name: zoneplot\n       Summary: zoneplot utility\n   Description: Utility to plot the output of zone statistics\n      Category: Applications/System Utilities\n         State: Not installed\n     Publisher: zoneplot\n       Version: 1.0\n Build Release: 5.11\n        Branch: 0\nPackaging Date: May 24, 2013 04:00:43 PM\n          Size: 6.04 kB\n          FMRI: pkg://zoneplot/zoneplot@1.2,5.11-0:20130524T160043Z  Let's go ahead and add this publisher to our configuration:  root@solaris:~# pkg set-publisher -p /repository\nroot@solaris:~# pkg publisher\nPUBLISHER            TYPE      STATUS P LOCATION\nsolaris                        origin     online      F http://pkg.oracle.com/solaris/release\nzoneplot                       origin     online      F file:///repository  And now finally install zoneplot (you will see that it will pull in a\nnumber of package dependencies):  root@solaris:~# pkg install zoneplot",
            "title": "Application packaging with IPS"
        },
        {
            "location": "/net/net/index.html",
            "text": "Oracle Solaris 11 Networking Lab\n\n\nIn Solaris 11 several new networking commands were added, some\nmanagement practices have changed to make network administration easier\nand more robust. In this lab we will learn some new networking commands,\ncompare them to the old ones and also work with network virtualization\nfeatures, which are brand new in Solaris 11.\n\n\nExercise N.1: Solaris 11 Networking Basics\n\n\nTask:\n You have to configure network interfaces and network services\n(DNS) in Solaris.\n\n\nLab:\n We have configured our Solaris virtual machine initially to use\nAutomatic network configuration. That means that it was configured using\nVirtualBox's internal DHCP server. In real life usually it's not the\ncase. Usually you configure your Solaris servers using manual mode. We\nwill learn how to do that. We will study the default IP and DNS\nconfiguration and then use it in the manual mode. We will use a new\nfeature called Vanity Naming which allows you to give network interfaces\nany names you want. Note that when we use these new Solaris 11 commands,\nall the changes are persistent and will sustain a reboot.\n\n\nWe assume that you have used the 'Automatic' network option mode during\nthe initial system configuration for your virtual machine. You have\nrecieved your network configuration from the VirtualBox's internal DHCP\nserver. Check if you can access the Internet:\n\n\nroot@solaris:~# ping oracle.com\noracle.com is alive\n\n\n\nIf you are behind a firewall, most likely you will not be able to ping\nthe outside network. If this is the case, try to ping one of your\ninternal sites (e.g. your internal DNS server). Or, try \nping 10.0.2.2\n.\nIt's the address of your host machine as seen from inside the VM.\n\n\nCheck your current configuration and record it to use in the future,\nwhen we switch to the manual mode. Enter the following commands and\nobserve the results.\n\n\nroot@solaris:~# dladm show-link\nroot@solaris:~# dladm show-phys\nroot@solaris:~# dladm show-ether\n\n\n\nWhat did you learn from those commands? That you have one physical\nEthernet interface, with the name \nnet0\n, using device \ne1000g0\n, with\nnominal speed 1Gbps. Big change in Solaris 11: all network interfaces by\ndefault now have unified generic names like \nnet0\n, \nnet1\n etc. More than\nthat: you can even use your own names for network interfaces! More about\nthis later.\n\n\nThis is our datalink level inventory. Let's move up, on the IP level.\nEnter the following commands to figure out your current IP\nconfiguration.\n\n\nroot@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         ok           --         --\nnet0/v4        dhcp       ok           --         10.0.2.15/24\nnet0/v6        addrconf   ok           --         fe80::a00:27ff:fec0:3b0a/10\n\n\n\nOK, we've got the usual loopback interface and the \nnet0\n interface with\nIP address \n10.0.2.15/24\n which was assigned by the DHCP server. Let's\ntake a more detailed look at \nnet0\n.\n\n\nroot@solaris:~# ipadm show-ifprop net0\nIFNAME      PROPERTY        PROTO PERM CURRENT    PERSISTENT DEFAULT    POSSIBLE\nnet0        arp             ipv4  rw   on         --         on         on,off\nnet0        forwarding      ipv4  rw   off        --         off        on,off\nnet0        metric          ipv4  rw   0          --         0          --\nnet0        mtu             ipv4  rw   1500       --         1500       68-1500\nnet0        exchange_routes ipv4  rw   on         --         on         on,off\nnet0        usesrc          ipv4  rw   none       --         none       --\nnet0        forwarding      ipv6  rw   off        --         off        on,off\nnet0        metric          ipv6  rw   0          --         0          --\nnet0        mtu             ipv6  rw   1500       --         1500       1280-1500\nnet0        nud             ipv6  rw   on         --         on         on,off\nnet0        exchange_routes ipv6  rw   on         --         on         on,off\nnet0        usesrc          ipv6  rw   none       --         none       --\nnet0        group           ip    rw   --         --         --         --\nnet0        standby         ip    rw   off        --         off        on,off\n\n\n\nA lot of information about IP properties of this \nnet0\n interface. You can\nlearn about these network parameters later. Consider that your homework\nassignment. For now let's move on.\n\n\nWhat about routing table and DNS settings? We will need them when\nconfiguring our interfaces in manual mode.\n\n\nroot@solaris:~# netstat -nr\n\nRouting Table: IPv4\nDestination           Gateway           Flags  Ref     Use     Interface \n-------------------- -------------------- ----- ----- ---------- --------- \ndefault              10.0.2.2             UG        4       1778 net0      \n10.0.2.0             10.0.2.15            U         3          0 net0      \n127.0.0.1            127.0.0.1            UH        2        796 lo0       \n\nRouting Table: IPv6\nDestination/Mask            Gateway                   Flags Ref   Use    If   \n--------------------------- --------------------------- ----- --- ------- ----- \n::1                         ::1                         UH      2       8 lo0   \nfe80::/10                   fe80::a00:27ff:fec0:3b0a    U       2       0 net0  \n\nroot@solaris:~# cat /etc/resolv.conf\n\n#\n# _AUTOGENERATED_FROM_SMF_V1_\n#\n# WARNING: THIS FILE GENERATED FROM SMF DATA.\n#   DO NOT EDIT THIS FILE.  EDITS WILL BE LOST.\n# See resolv.conf(4) for details.\n\nnameserver  192.168.1.1\n\n\n\nNote the warning in the \nresolv.conf\n file. There are some changes in\nDNS configuration in Solaris 11, we'll talk about them later. Now, just\nwrite down your default router IP address (\n10.0.2.2\n in case of\nVirtualBox installation) and your DNS server address(es) (most likely,\nyours are different from \n192.168.1.1\n).\n\n\nNow, we are ready to change network management to the manual mode:\n\n\nroot@solaris:~# netadm enable -p ncp DefaultFixed\n\n\n\nCheck again if you can access the Internet (again, replace \noracle.com\n\nwith one of your internal hosts if you are behind a firewall):\n\n\nroot@solaris:~# ping oracle.com\nping: unknown host oracle.com\n\n\n\nMost likely, the reason for this error message is that we can't access\nany DNS servers or they are not configured at all. Check the DNS\nserver's IP address (replace \n192.168.1.1\n with what you have recorded\nwhile in Automatic mode):\n\n\nroot@solaris:~# ping 192.168.1.1\nping: sendto No route to host\n\n\n\nRouting is not configured. OK, the default gateway was \n10.0.2.2\n\n(internal VirtualBox address). Let's try it:\n\n\nroot@solaris:~# ping 10.0.2.2\nping: sendto No route to host\n\n\n\nNothing works! Let's start from the beginning. Check if the same\nphysical links are available:\n\n\nroot@solaris:~# dladm show-phys\nLINK              MEDIA                STATE      SPEED  DUPLEX    DEVICE\nnet0              Ethernet             unknown    1000   full      e1000g0\n\n\n\nOK, physical link is in place. What about IP links?\n\n\nroot@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\n\n\n\nOnly loopback is available. Time to create an IP link from scratch:\n\n\nroot@solaris:~# ipadm create-ip net0\nroot@solaris:~# ipadm\nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         down         --         --\n\n\n\nIP link is there, but there is no IP address assigned to it. Let's fix\nthat.\n\n\nroot@solaris:~# ipadm create-addr -a 10.0.2.15/24 net0\nnet0/v4\nroot@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         ok           --         --\nnet0/addr      static     ok           --         10.0.2.15/24\n\n\n\nMuch better. Try pinging some addresses:\n\n\nroot@solaris:~# ping oracle.com\nping: unknown host oracle.com\nroot@solaris:~# ping 10.0.2.2\n10.0.2.2 is alive\n\n\n\nFirst ping failure tells us that most likely DNS is not avalable. Second\nping shows that we can at least access our default gateway. Let's\ncontinue moving further and ping our DNS server.\n\n\nPing the network again:\n\n\nroot@solaris:~# ping 192.168.1.1 (replace 192.168.1.1 with your DNS server IP address)\nping: sendto No route to host\n\n\n\nRouting is not configured. Check:\n\n\nroot@solaris:~# netstat -nr\n\nRouting Table: IPv4\nDestination           Gateway           Flags  Ref     Use     Interface \n-------------------- -------------------- ----- ----- ---------- --------- \n10.0.2.0             10.0.2.15            U         3          2 net0      \n127.0.0.1            127.0.0.1            UH        2       1214 lo0       \n\nRouting Table: IPv6\nDestination/Mask            Gateway                   Flags Ref   Use    If   \n--------------------------- --------------------------- ----- --- ------- ----- \n::1                         ::1                         UH      2      12 lo0   \n\n\n\nYes, indeed. There is no default gateway. Add the default gateway and\ncheck again:\n\n\nroot@solaris:~# route -p add default 10.0.2.2\nadd net default: gateway 10.0.2.2\nadd persistent net default: gateway 10.0.2.2\nroot@solaris:~# ping 192.168.1.1 (replace 192.168.1.1 with your DNS server IP address)\n192.168.1.1 is alive\nroot@solaris:~# ping oracle.com (replace oracle.com with your internal site)\nping: unknown host oracle.com\n\n\n\nWe can reach our DNS server, but our system is not configured to use it.\nIf you think that editing your /etc/resolv.conf is enough, remember the\nwarning in that file:\n\n\n    # WARNING: THIS FILE GENERATED FROM SMF DATA.\n    #   DO NOT EDIT THIS FILE.  EDITS WILL BE LOST.\n\n\n\nThat means that in Solaris 11 name service configuration is different\nfrom what you used before. To use DNS we have to configure the\n\ndns/client\n service and also the \nname-service/switch\n service which\nused to be configured via \n/etc/nsswitch.conf\n. Yes, it's a little bit\nmore complicated, but it's more robust and manageable. It's a general\ndirection in Solaris: most of the services are configured via SMF\nframework, not via config files. Here are the commands:\n\n\nroot@solaris:~# svccfg -s dns/client 'setprop config/nameserver = net_address: 192.168.1.1'\nroot@solaris:~# svccfg -s dns/client 'setprop config/domain = astring: \nexample.com\n ' (replace example.com with your local default domain name or skip this step)\nroot@solaris:~# svccfg -s name-service/switch 'setprop config/host = astring: \nfiles dns\n '\nroot@solaris:~# svcadm refresh name-service/switch\nroot@solaris:~# svcadm refresh dns/client\n\n\n\nAlternatively, you can edit the usual files \n/etc/resolv.conf\n and\n\n/etc/nsswitch.conf\n, but you have to import them into the naming service\nconfiguration:\n\n\nroot@solaris:~# nscfg import -f svc:/system/name-service/switch:default\nroot@solaris:~# nscfg import -f svc:/network/dns/client:default\nroot@solaris:~# svcadm refresh dns/client\n\n\n\nNow our ping finally reaches the Internet:\n\n\nroot@solaris:~# ping oracle.com (replace oracle.com with one of your internal hosts)\noracle.com is alive\n\n\n\nNew names.\n Do you remember the days when you were a junior Solaris\nsystem administrator and wondered why all network interfaces in Solaris\nhave these funny names? \nle\n, \nbge\n, \nce\n, \nxge\n, \ne1000g\n.... Now, as you can\nsee, they all are called \nnet0\n, \nnet1\n, \nnet2\n, ... Much simpler, right? Even\nmore than that: you can give your interfaces your own names. Here is the\nexample. Show what we've got now:\n\n\nroot@solaris:~# dladm\nroot@solaris:~# ipadm\n\n\n\nImagine we want to use our network interfaces for different services on\nour Solaris box. We have web server, application server etc. We can name\nour network interfaces \nweb1\n, \napp0\n, \ndb1\n etc. Start by deleting the \nnet0\n\nIP interface\n\n\nroot@solaris:~# ipadm delete-ip net0\n\n\n\n...now rename the NIC\n\n\nroot@solaris:~# dladm rename-link net0 web1\nroot@solaris:~# dladm \n\n\n\nAdd back in the IP interface and its address:\n\n\nroot@solaris:~# ipadm create-ip web1\nroot@solaris:~# ipadm create-addr -a 10.0.2.15/24 web1\n\n\n\nCleaning up... Undo it all\n\n\nroot@solaris:~# ipadm delete-ip web1\nroot@solaris:~# dladm rename-link web1 net0\nroot@solaris:~# ipadm create-ip net0\nroot@solaris:~# ipadm create-addr -a 10.0.2.15/24 net0\nroot@solaris:~# ipadm\n\n\n\nYou may need to restart your DNS client service after this exercise:\n\n\nroot@solaris:~# svcadm disable dns/client\nroot@solaris:~# svcadm enable dns/client\n\n\n\nOne word of advice: having this kind of freedom, please try to avoid\nlong discussions about network interface naming, similar to what you\nhave already had regarding host naming policies. :-)\n\n\nExercise N.2: Network Virtualization\n\n\nTask:\n You want to create Virtual Network Interface Cards (VNICs) to\nuse them with your Zones. You want to build and manage your\napplication's network infrastructure completely inside the box for\ndevelopment and testing purposes.\n\n\nLab:\n We will create VNICs, assign IP addresses to them and learn how\nto limit bandwidth on them.\n\n\nFirst we show the links. Links can be physical or virtual. Note that for\nphysical NICs, we use a new naming scheme \nnet0\n, \nnet1\n, etc. that hides\nthe actual device name.\n\n\nroot@solaris:~# dladm show-link\n\n\n\nShow only the physical ethernet NICs:\n\n\nroot@solaris:~# dladm show-ether\n\n\n\nAnd to see the actual hardware devices used for the netX NICs:\n\n\nroot@solaris:~# dladm show-phys\n\n\n\nThe next command shows a bit more information like the physical\nlocation:\n\n\nroot@solaris:~# dladm show-phys -L\n\n\n\nSo now we create a VNIC that we call vnic1, using \nnet0\n as its\nunderlying datalink. Note that VNICs are first-class NICs in terms of\nvisibility (e.g. \nsnoop\n)\n\n\nroot@solaris:~# dladm create-vnic -l net0 vnic1\n\n\n\nShow the VNICs:\n\n\nroot@solaris:~# dladm show-vnic\n\n\n\nWe can easily limit bandwith on a VNIC:\n\n\nroot@solaris:~# dladm set-linkprop -p maxbw=40 vnic1\nroot@solaris:~# dladm show-vnic\n\n\n\nNow we create an IP interface. This is analgous to plumbing the\ninterface:\n\n\nroot@solaris:~# ipadm create-ip vnic1\n\n\n\nNow we assign a persistent IP address to the VNIC:\n\n\nroot@solaris:~# ipadm create-addr -a 10.2.3.4/24 vnic1\n\n\n\nPing the VNIC:\n\n\nroot@solaris:~# ping 10.2.3.4\n\n\n\nShow all available datalinks, both physical and virtual\n\n\nroot@solaris:~# dladm show-link\n\n\n\nFinally list all IP addresses:\n\n\nroot@solaris:~# ipadm show-addr\n\n\n\nNow we tear down what we've just created:\n\n\nroot@solaris:~# ipadm delete-addr vnic1/v4\nroot@solaris:~# ipadm delete-ip vnic1\nroot@solaris:~# dladm delete-vnic vnic1\nroot@solaris:~# dladm show-link\n\n\n\nNow you see how new networking commands work. Of course, you can still\nuse the old-style \nifconfig\n, but the new commands are easier to use\nand, most importantly, they make presistent changes.\n\n\nFind more food for thought and inspiration here:\n\n\n\n\nStrategies for Network Administration in Oracle Solaris 11.2\n\n\nhttp://docs.oracle.com/cd/E36784_01/html/E37473/index.html\n  \n\n\nHow to get started configuring your network in Oracle Solaris 11\n\n\nhttp://www.oracle.com/technetwork/articles/servers-storage-dev/s11-network-config-1632927.html\n  \n\n\nHow to Script Oracle Solaris 11 Zones Creation for a Network-In-a-Box Configuration\n\n\nhttp://www.oracle.com/technetwork/articles/servers-storage-admin/o11-118-s11-script-zones-524499.html\n  \n\n\nHow to Restrict Your Application Traffic Using Oracle Solaris 11\n\n    Network Virtualization and Resource Management\n\n\nhttp://www.oracle.com/technetwork/articles/servers-storage-admin/o11-095-s11-app-traffic-525038.html",
            "title": "Networking"
        },
        {
            "location": "/net/net/index.html#oracle-solaris-11-networking-lab",
            "text": "In Solaris 11 several new networking commands were added, some\nmanagement practices have changed to make network administration easier\nand more robust. In this lab we will learn some new networking commands,\ncompare them to the old ones and also work with network virtualization\nfeatures, which are brand new in Solaris 11.",
            "title": "Oracle Solaris 11 Networking Lab"
        },
        {
            "location": "/net/net/index.html#exercise-n1-solaris-11-networking-basics",
            "text": "Task:  You have to configure network interfaces and network services\n(DNS) in Solaris.  Lab:  We have configured our Solaris virtual machine initially to use\nAutomatic network configuration. That means that it was configured using\nVirtualBox's internal DHCP server. In real life usually it's not the\ncase. Usually you configure your Solaris servers using manual mode. We\nwill learn how to do that. We will study the default IP and DNS\nconfiguration and then use it in the manual mode. We will use a new\nfeature called Vanity Naming which allows you to give network interfaces\nany names you want. Note that when we use these new Solaris 11 commands,\nall the changes are persistent and will sustain a reboot.  We assume that you have used the 'Automatic' network option mode during\nthe initial system configuration for your virtual machine. You have\nrecieved your network configuration from the VirtualBox's internal DHCP\nserver. Check if you can access the Internet:  root@solaris:~# ping oracle.com\noracle.com is alive  If you are behind a firewall, most likely you will not be able to ping\nthe outside network. If this is the case, try to ping one of your\ninternal sites (e.g. your internal DNS server). Or, try  ping 10.0.2.2 .\nIt's the address of your host machine as seen from inside the VM.  Check your current configuration and record it to use in the future,\nwhen we switch to the manual mode. Enter the following commands and\nobserve the results.  root@solaris:~# dladm show-link\nroot@solaris:~# dladm show-phys\nroot@solaris:~# dladm show-ether  What did you learn from those commands? That you have one physical\nEthernet interface, with the name  net0 , using device  e1000g0 , with\nnominal speed 1Gbps. Big change in Solaris 11: all network interfaces by\ndefault now have unified generic names like  net0 ,  net1  etc. More than\nthat: you can even use your own names for network interfaces! More about\nthis later.  This is our datalink level inventory. Let's move up, on the IP level.\nEnter the following commands to figure out your current IP\nconfiguration.  root@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         ok           --         --\nnet0/v4        dhcp       ok           --         10.0.2.15/24\nnet0/v6        addrconf   ok           --         fe80::a00:27ff:fec0:3b0a/10  OK, we've got the usual loopback interface and the  net0  interface with\nIP address  10.0.2.15/24  which was assigned by the DHCP server. Let's\ntake a more detailed look at  net0 .  root@solaris:~# ipadm show-ifprop net0\nIFNAME      PROPERTY        PROTO PERM CURRENT    PERSISTENT DEFAULT    POSSIBLE\nnet0        arp             ipv4  rw   on         --         on         on,off\nnet0        forwarding      ipv4  rw   off        --         off        on,off\nnet0        metric          ipv4  rw   0          --         0          --\nnet0        mtu             ipv4  rw   1500       --         1500       68-1500\nnet0        exchange_routes ipv4  rw   on         --         on         on,off\nnet0        usesrc          ipv4  rw   none       --         none       --\nnet0        forwarding      ipv6  rw   off        --         off        on,off\nnet0        metric          ipv6  rw   0          --         0          --\nnet0        mtu             ipv6  rw   1500       --         1500       1280-1500\nnet0        nud             ipv6  rw   on         --         on         on,off\nnet0        exchange_routes ipv6  rw   on         --         on         on,off\nnet0        usesrc          ipv6  rw   none       --         none       --\nnet0        group           ip    rw   --         --         --         --\nnet0        standby         ip    rw   off        --         off        on,off  A lot of information about IP properties of this  net0  interface. You can\nlearn about these network parameters later. Consider that your homework\nassignment. For now let's move on.  What about routing table and DNS settings? We will need them when\nconfiguring our interfaces in manual mode.  root@solaris:~# netstat -nr\n\nRouting Table: IPv4\nDestination           Gateway           Flags  Ref     Use     Interface \n-------------------- -------------------- ----- ----- ---------- --------- \ndefault              10.0.2.2             UG        4       1778 net0      \n10.0.2.0             10.0.2.15            U         3          0 net0      \n127.0.0.1            127.0.0.1            UH        2        796 lo0       \n\nRouting Table: IPv6\nDestination/Mask            Gateway                   Flags Ref   Use    If   \n--------------------------- --------------------------- ----- --- ------- ----- \n::1                         ::1                         UH      2       8 lo0   \nfe80::/10                   fe80::a00:27ff:fec0:3b0a    U       2       0 net0  \n\nroot@solaris:~# cat /etc/resolv.conf\n\n#\n# _AUTOGENERATED_FROM_SMF_V1_\n#\n# WARNING: THIS FILE GENERATED FROM SMF DATA.\n#   DO NOT EDIT THIS FILE.  EDITS WILL BE LOST.\n# See resolv.conf(4) for details.\n\nnameserver  192.168.1.1  Note the warning in the  resolv.conf  file. There are some changes in\nDNS configuration in Solaris 11, we'll talk about them later. Now, just\nwrite down your default router IP address ( 10.0.2.2  in case of\nVirtualBox installation) and your DNS server address(es) (most likely,\nyours are different from  192.168.1.1 ).  Now, we are ready to change network management to the manual mode:  root@solaris:~# netadm enable -p ncp DefaultFixed  Check again if you can access the Internet (again, replace  oracle.com \nwith one of your internal hosts if you are behind a firewall):  root@solaris:~# ping oracle.com\nping: unknown host oracle.com  Most likely, the reason for this error message is that we can't access\nany DNS servers or they are not configured at all. Check the DNS\nserver's IP address (replace  192.168.1.1  with what you have recorded\nwhile in Automatic mode):  root@solaris:~# ping 192.168.1.1\nping: sendto No route to host  Routing is not configured. OK, the default gateway was  10.0.2.2 \n(internal VirtualBox address). Let's try it:  root@solaris:~# ping 10.0.2.2\nping: sendto No route to host  Nothing works! Let's start from the beginning. Check if the same\nphysical links are available:  root@solaris:~# dladm show-phys\nLINK              MEDIA                STATE      SPEED  DUPLEX    DEVICE\nnet0              Ethernet             unknown    1000   full      e1000g0  OK, physical link is in place. What about IP links?  root@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128  Only loopback is available. Time to create an IP link from scratch:  root@solaris:~# ipadm create-ip net0\nroot@solaris:~# ipadm\nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         down         --         --  IP link is there, but there is no IP address assigned to it. Let's fix\nthat.  root@solaris:~# ipadm create-addr -a 10.0.2.15/24 net0\nnet0/v4\nroot@solaris:~# ipadm \nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\nlo0/v4         static     ok           --         127.0.0.1/8\nlo0/v6         static     ok           --         ::1/128\nnet0              ip         ok           --         --\nnet0/addr      static     ok           --         10.0.2.15/24  Much better. Try pinging some addresses:  root@solaris:~# ping oracle.com\nping: unknown host oracle.com\nroot@solaris:~# ping 10.0.2.2\n10.0.2.2 is alive  First ping failure tells us that most likely DNS is not avalable. Second\nping shows that we can at least access our default gateway. Let's\ncontinue moving further and ping our DNS server.  Ping the network again:  root@solaris:~# ping 192.168.1.1 (replace 192.168.1.1 with your DNS server IP address)\nping: sendto No route to host  Routing is not configured. Check:  root@solaris:~# netstat -nr\n\nRouting Table: IPv4\nDestination           Gateway           Flags  Ref     Use     Interface \n-------------------- -------------------- ----- ----- ---------- --------- \n10.0.2.0             10.0.2.15            U         3          2 net0      \n127.0.0.1            127.0.0.1            UH        2       1214 lo0       \n\nRouting Table: IPv6\nDestination/Mask            Gateway                   Flags Ref   Use    If   \n--------------------------- --------------------------- ----- --- ------- ----- \n::1                         ::1                         UH      2      12 lo0     Yes, indeed. There is no default gateway. Add the default gateway and\ncheck again:  root@solaris:~# route -p add default 10.0.2.2\nadd net default: gateway 10.0.2.2\nadd persistent net default: gateway 10.0.2.2\nroot@solaris:~# ping 192.168.1.1 (replace 192.168.1.1 with your DNS server IP address)\n192.168.1.1 is alive\nroot@solaris:~# ping oracle.com (replace oracle.com with your internal site)\nping: unknown host oracle.com  We can reach our DNS server, but our system is not configured to use it.\nIf you think that editing your /etc/resolv.conf is enough, remember the\nwarning in that file:      # WARNING: THIS FILE GENERATED FROM SMF DATA.\n    #   DO NOT EDIT THIS FILE.  EDITS WILL BE LOST.  That means that in Solaris 11 name service configuration is different\nfrom what you used before. To use DNS we have to configure the dns/client  service and also the  name-service/switch  service which\nused to be configured via  /etc/nsswitch.conf . Yes, it's a little bit\nmore complicated, but it's more robust and manageable. It's a general\ndirection in Solaris: most of the services are configured via SMF\nframework, not via config files. Here are the commands:  root@solaris:~# svccfg -s dns/client 'setprop config/nameserver = net_address: 192.168.1.1'\nroot@solaris:~# svccfg -s dns/client 'setprop config/domain = astring:  example.com  ' (replace example.com with your local default domain name or skip this step)\nroot@solaris:~# svccfg -s name-service/switch 'setprop config/host = astring:  files dns  '\nroot@solaris:~# svcadm refresh name-service/switch\nroot@solaris:~# svcadm refresh dns/client  Alternatively, you can edit the usual files  /etc/resolv.conf  and /etc/nsswitch.conf , but you have to import them into the naming service\nconfiguration:  root@solaris:~# nscfg import -f svc:/system/name-service/switch:default\nroot@solaris:~# nscfg import -f svc:/network/dns/client:default\nroot@solaris:~# svcadm refresh dns/client  Now our ping finally reaches the Internet:  root@solaris:~# ping oracle.com (replace oracle.com with one of your internal hosts)\noracle.com is alive  New names.  Do you remember the days when you were a junior Solaris\nsystem administrator and wondered why all network interfaces in Solaris\nhave these funny names?  le ,  bge ,  ce ,  xge ,  e1000g .... Now, as you can\nsee, they all are called  net0 ,  net1 ,  net2 , ... Much simpler, right? Even\nmore than that: you can give your interfaces your own names. Here is the\nexample. Show what we've got now:  root@solaris:~# dladm\nroot@solaris:~# ipadm  Imagine we want to use our network interfaces for different services on\nour Solaris box. We have web server, application server etc. We can name\nour network interfaces  web1 ,  app0 ,  db1  etc. Start by deleting the  net0 \nIP interface  root@solaris:~# ipadm delete-ip net0  ...now rename the NIC  root@solaris:~# dladm rename-link net0 web1\nroot@solaris:~# dladm   Add back in the IP interface and its address:  root@solaris:~# ipadm create-ip web1\nroot@solaris:~# ipadm create-addr -a 10.0.2.15/24 web1  Cleaning up... Undo it all  root@solaris:~# ipadm delete-ip web1\nroot@solaris:~# dladm rename-link web1 net0\nroot@solaris:~# ipadm create-ip net0\nroot@solaris:~# ipadm create-addr -a 10.0.2.15/24 net0\nroot@solaris:~# ipadm  You may need to restart your DNS client service after this exercise:  root@solaris:~# svcadm disable dns/client\nroot@solaris:~# svcadm enable dns/client  One word of advice: having this kind of freedom, please try to avoid\nlong discussions about network interface naming, similar to what you\nhave already had regarding host naming policies. :-)",
            "title": "Exercise N.1: Solaris 11 Networking Basics"
        },
        {
            "location": "/net/net/index.html#exercise-n2-network-virtualization",
            "text": "Task:  You want to create Virtual Network Interface Cards (VNICs) to\nuse them with your Zones. You want to build and manage your\napplication's network infrastructure completely inside the box for\ndevelopment and testing purposes.  Lab:  We will create VNICs, assign IP addresses to them and learn how\nto limit bandwidth on them.  First we show the links. Links can be physical or virtual. Note that for\nphysical NICs, we use a new naming scheme  net0 ,  net1 , etc. that hides\nthe actual device name.  root@solaris:~# dladm show-link  Show only the physical ethernet NICs:  root@solaris:~# dladm show-ether  And to see the actual hardware devices used for the netX NICs:  root@solaris:~# dladm show-phys  The next command shows a bit more information like the physical\nlocation:  root@solaris:~# dladm show-phys -L  So now we create a VNIC that we call vnic1, using  net0  as its\nunderlying datalink. Note that VNICs are first-class NICs in terms of\nvisibility (e.g.  snoop )  root@solaris:~# dladm create-vnic -l net0 vnic1  Show the VNICs:  root@solaris:~# dladm show-vnic  We can easily limit bandwith on a VNIC:  root@solaris:~# dladm set-linkprop -p maxbw=40 vnic1\nroot@solaris:~# dladm show-vnic  Now we create an IP interface. This is analgous to plumbing the\ninterface:  root@solaris:~# ipadm create-ip vnic1  Now we assign a persistent IP address to the VNIC:  root@solaris:~# ipadm create-addr -a 10.2.3.4/24 vnic1  Ping the VNIC:  root@solaris:~# ping 10.2.3.4  Show all available datalinks, both physical and virtual  root@solaris:~# dladm show-link  Finally list all IP addresses:  root@solaris:~# ipadm show-addr  Now we tear down what we've just created:  root@solaris:~# ipadm delete-addr vnic1/v4\nroot@solaris:~# ipadm delete-ip vnic1\nroot@solaris:~# dladm delete-vnic vnic1\nroot@solaris:~# dladm show-link  Now you see how new networking commands work. Of course, you can still\nuse the old-style  ifconfig , but the new commands are easier to use\nand, most importantly, they make presistent changes.  Find more food for thought and inspiration here:   Strategies for Network Administration in Oracle Solaris 11.2  http://docs.oracle.com/cd/E36784_01/html/E37473/index.html     How to get started configuring your network in Oracle Solaris 11  http://www.oracle.com/technetwork/articles/servers-storage-dev/s11-network-config-1632927.html     How to Script Oracle Solaris 11 Zones Creation for a Network-In-a-Box Configuration  http://www.oracle.com/technetwork/articles/servers-storage-admin/o11-118-s11-script-zones-524499.html     How to Restrict Your Application Traffic Using Oracle Solaris 11 \n    Network Virtualization and Resource Management  http://www.oracle.com/technetwork/articles/servers-storage-admin/o11-095-s11-app-traffic-525038.html",
            "title": "Exercise N.2: Network Virtualization"
        },
        {
            "location": "/virt/virt_pre/index.html",
            "text": "Before starting this lab make sure that you either:\n\n\n\n\nhave access to the external Oracle package repository (the output of\n    \npkg list -a | wc -l\n should give your more than 3500 packages), \nor\n\n\nhave completed the \nRepository Installation\n exercise from IPS Lab and installed a small local repository",
            "title": "Virtualization Lab Prerequisites"
        },
        {
            "location": "/virt/zone_install/index.html",
            "text": "Task:\n Your development team wants a separate environment to develop\nand test their new application.\n\n\nLab:\n We are going to use Solaris virtualization technology called\nSolaris Zones.\n\n\n\n\n60 seconds of theory\n\n\nSolaris Zones are isolated operating environments,\nwhich run inside a single Solaris instance. Each Solaris Zone has its\nown users, set of processes and applications, hostname and IP address.\nThere are two types of zones: kernel zones and non-kernel zones. In this\nlab we will be working with non-kernel zones. They use the same kernel\nthat is used by the host Solaris instance, also called \"global zone\".\nEach zone has it's own dataset ('zone root') where all system files are\ninstalled. Zones can use additional datasets for applications and users'\ndata. By default in Solaris 11 zones use virtual network interface cards\n(VNICs) and \"exclusive IP\" which means that networking can be managed\nboth from inside and outside the zone.\n\n\n\n\nWhen creating a zone we have to define two most important parameters:\nthe zone root's location and the network configuration. In the simplest\npossible case both of them can be left to their default values. Zone\nroot by default will be located in \n/system/zones/zonename\n. Of course,\nyou can change that, but in this lab we'll leave it at default.\nNetworking by default is configured as an \"automatic VNIC\" which is\ncreated automatically at the zone's boot time and destroyed after zone\nhalts. By default the zone's IP address is defined inside the zone and\nquite possibly by a different system administrator. In most datacenter\nsituations though, you would like to control yourself the IP addresses\nassigned to your zones. This is what we are going to do in this lab.\n\n\nStart with this simple command:\n\n\nroot@solaris:~# zonecfg -z zone1 \nzone1: No such zone configured \nUse 'create' to begin configuring a new zone. \nOK, if you say so...\nzonecfg:zone1\n create \ncreate: Using system default template 'SYSdefault'\n\n\n\nLet's look at what can be configured and what is already configured:\n\n\nzonecfg:zone1\n info\nzonename: zone1\nzonepath.template: /system/zones/%{zonename}\nzonepath: /system/zones/zone1\nbrand: solaris\nautoboot: false\nautoshutdown: shutdown\nbootargs: \nfile-mac-profile: \npool: \nlimitpriv: \nscheduling-class: \nip-type: exclusive\nhostid: \ntenant: \nfs-allowed: \nanet:\n      linkname: net0\n      lower-link: auto\n      allowed-address not specified\n      configure-allowed-address: true\n      defrouter not specified\n      allowed-dhcp-cids not specified\n      link-protection: mac-nospoof\n      mac-address: auto\n      mac-prefix not specified\n      mac-slot not specified\n      vlan-id not specified\n      priority not specified\n      rxrings not specified\n      txrings not specified\n      mtu not specified\n      maxbw not specified\n      rxfanout not specified\n      vsi-typeid not specified\n      vsi-vers not specified\n      vsi-mgrid not specified\n      etsbw-lcl not specified\n      cos not specified\n      pkey not specified\n      linkmode not specified\n      evs not specified\n      vport not specified\n\n\n\nAs you can see, \nzonepath\n is configured by default and one networking\ninterface, \nanet\n, is already there. To make sure the zone's IP address\nis configured properly, we'll define it here, in \nzonecfg\n, along with the\ndefault router. In that case we can be sure that it can't be changed\nfrom inside the zone (maliciously or by mistake).\n\n\nzonecfg:zone1\n select anet linkname=net0\n(In spite of having only one anet, we still have to specify which one we select for configuration) \nzonecfg:zone1:anet\n set allowed-address=10.0.2.21/24 \n(Use the IP address assigned by your instructor)\nzonecfg:zone1:anet\n set defrouter=10.0.2.2 \n(Your instructor will give you the default gateway address)\nzonecfg:zone1:anet\n end\nzonecfg:zone1\n exit\n\n\n\nTo check the status of our newly created zone:\n\n\nroot@solaris:~# zoneadm list -cv \nID NAME             STATUS     PATH                           BRAND    IP    \n0 global           running    /                              solaris  shared\n- zone1            configured /zones/zone1                   solaris  excl  \n\n\n\nThe zone is configured, we can install and boot it right now. But before\nthe installation we'll configure a profile for the Solaris instance\nwhich will be running inside the zone. By doing that we are avoiding\nconfiguring the zone interactively during the first boot. Our zone will\nbe ready for use immediately after start.\n\n\nroot@solaris:~# sysconfig create-profile -o /root/zone1-profile\n\n\n\nThis command will bring you to the interactive dialog very similar to\nthe standard Solaris installaion. Use \nF2\n to confirm your choices and\nmove from screen to screen. If \nF2\n doesn't work for you, \nuse \nEsc\n-\n2\n (press\nand release \nEsc\n and then \n2\n). You will have to enter:\n\n\n\n\nComputer Name (hostname for the zone): \nzone1\n\n\nNetwork configuration: choose \nAutomatically\n\n\nTime zone: choose your time zone from the list\n\n\nDate: confirm the current date\n\n\nRoot password: \nsolaris1\n\n\nNew user account details: real name, login name and password. This\n    will be the first user of the zone. We have entered \nZone User\n,\n    \nzuser\n, \noracle1\n\n\nOther options leave to defaults\n\n\n\n\nNow, when the zone's profile is created, we can install the zone and\ninitialize it using this profile.\n\n\nroot@solaris:~# zoneadm -z zone1 install -c /root/zone1-profile \nA ZFS file system has been created for this zone.\nProgress being logged to /var/log/zones/zoneadm.20111113T200358Z.zone1.install\n   Image: Preparing at /zones/zone1/root.\n\nInstall Log: /system/volatile/install.4418/install_log\nAI Manifest: /tmp/manifest.xml.NVaaNi\nSC Profile: /root/zone1-profile.xml\nZonename: zone1\nInstallation: Starting ...\n\n\n\nHere you can take a break. The installation will take about 8-10\nminutes, depending on your computer.\n\n\n...Long output is skipped... \nNext Steps: Boot the zone, then log into the zone console (zlogin -C) \nto complete the configuration process. \n\n\n\nCheck the status again:\n\n\nroot@solaris:~# zoneadm list -cv \nID NAME             STATUS     PATH                           BRAND    IP    \n0 global           running    /                              solaris  shared\n- zone1            installed  /zones/zone1                   solaris  excl  \n\n\n\nIt's time to boot our zone:\n\n\nroot@solaris:~# zoneadm -z zone1 boot \nroot@solaris:~# zoneadm list -cv\nID NAME             STATUS     PATH                           BRAND    IP    \n0 global           running    /                              solaris  shared\n1 zone1            running    /zones/zone1                   solaris  excl  \n\n\n\nNote the zone's status has changed to \nrunning\n.\n\n\nNow log into our zone's console (note \n-C\n). You will have to wait a\ncouple of minutes while the system is initializing services for the\nfirst time. While waiting for the zone to boot completely, you can open\nanother terminal window, become root (\nsu -\n) and login into the zone\ndirectly with \nzlogin zone1\n This way you don't have to wait for all the\nservices to start, but you can watch the booting process in real time.\nRun \nprstat\n and watch various system services starting one after\nanother.\n\n\nroot@solaris:~# zlogin -C zone1 \n[Connected to zone 'zone1' console] \n\n\n\nYou will get the standard Solaris login prompt (you might need to press\n\nEnter\n one more time). Congratulations! You've just configured\n\"virtualization within virtualization\" using Oracle technologies:\nSolaris zones within Oracle VirtualBox (or within OVM for SPARC a.k.a.\nLogical Domains).\n\n\nTry to login using \nroot\n's credentials (\nroot/solaris1\n). Here is the\nresult:\n\n\nzone1 console login: root\nPassword: \nRoles can not login directly\nLogin incorrect\nNov 13 15:23:07 zone1 login: login account failure: Permission denied\n\n\n\nA-ha! This is a new Solaris 11 security feature called \"root as a role\".\nThat means that you can't login into a system as \nroot\n. You have to use\nnormal user's credentials and only then you will be able to use \nsudo\n\nor \npfexec\n according to your roles and privileges.\n\n\nTry to login again with \nzuser/oracle1\n.\n\n\nOracle Corporation      SunOS 5.11      11.1    September 2012\nzuser@zone1:~$\n\n\n\nSuccess!\n\n\nNote: to escape from the zone's console first type \nexit\n to close the\nsession and then at the console prompt use: \n~\n \n.\n (tilde period).",
            "title": "Zone Installation"
        },
        {
            "location": "/virt/zone_inside/index.html",
            "text": "Task:\n You have to install some application packages in the zone and\ncreate users.\n\n\nLab:\n Log in in the zone, create a user and install a web server\napplication.\n\n\nroot@solaris:~# zlogin zone1 \nroot@zone1:~# \n\n\n\nPlay around with the usual sysadmin commands. How can you tell if you\nare in a zone or not? First, try \nps -ef\n. Do you see anything unusual?\nYes, you are right, the process IDs don't start with 0, but with some\nbig number. Other than that, no visible difference between the normal\nSolaris installation and the zone. Try \nuname -a\n, \npsrinfo\n, \ncat\n/etc/release\n... Check if you can access the Internet by pinging\n\noracle.com\n.\n\n\nNow let's do something useful with the zone. Like running a web server,\nfor example. Let's install and run Apache.\n\n\nroot@zone1:~# pkg list -a *apache* \n. . .Skipped. . . \nweb/server/apache-22 2.2.22-0.175.1.0.0.24.0  ---. \n. .Skipped. . . \nroot@zone1:~# pkg install apache-22 \n. . .Skipped. . . \n\n\n\nWe've installed it successfully, but it's not running yet.\n\n\nroot@zone1:~# svcs -a | grep apache \ndisabled 6:31:42 svc:/network/http:apache22 \n\n\n\nStart the Apache web server:\n\n\nroot@zone1:~# svcadm enable apache22 \nroot@zone1:~# svcs -a | grep apache \nonline 6:34:03 svc:/network/http:apache22 \n\n\n\nCheck if it's working from your global Solaris zone (your Solaris\ndesktop): start Firefox and enter your zone's IP address into the URL\nfield: \n10.0.2.21\n. \nIt works!\n -- the page usually reads. \n\n\nCheck if it's your zone who is talking. Go back to the zone's terminal\nwindow and change your web server homepage (I'm using \nvi\n here, as we\ndon't have many choices in a freshly installed zone. If you are not\nfamiliar with \nvi\n, check our Vi Quick Reference below):\n\n\nroot@zone1:~# vi /var/apache2/2.2/htdocs/index.html \n\n\n\nWrite here something like \"This is Zone1 and it works!\" and save the\nfile. Make sure you use \nw!\n (with the exclamation sign) to save the\nread-only file. Now reload the page in Firefox in your Solaris desktop.\nDid it work? Congratulations!\n\n\n\n\nVi Quick Reference\n\n\nIf you're unfamiliar with vi, following are a few common\nkeyboard commands to get you through this exercise:\n\ni = switch to Insert mode\n\nUse Insert mode to type in your text.\n\nEsc = switch to Command mode\n\nIn Command mode use:\n\nk = up\n\nj = down\n\nw = right or forward one word\n\nb = left or back one word\n\nl = right 1 char\n\nh = left 1 char\n\nx = delete 1 char\n\nu = undo\n\ndd = delete entire current line\n\n:w = write (save) the current file\n\n:wq = write and quit\n\n:w! = write to a read-only file\n\n:q! = quite ignoring changes (do not write)  \n\n\n\n\nWhat else do we need? Try to create users in the zone.\n\n\nroot@zone1:~# useradd -m jack \nroot@zone1:~# passwd jack\nNew Password: oracle1 (will not be displayed) \nRe-enter new Password: oracle1 (will not be displayed) \npasswd: password successfully changed for jack \nroot@zone1:~# su - jack \nOracle Corporation  SunOS 5.11  11.0    November 2011\njack@zone1:~$ ls\nlocal.cshrc    local.login    local.profile\njack@zone1:~$ \n\n\n\nLooks good! Try to login from your global zone (open another window on\nyour Solaris desktop):\n\n\nlab@solaris:~$ ssh -l jack 10.0.2.21\n\n\n\n(It's a small letter L here, not the digit 'one')\n\n\nExit from the \nssh\n session and return back to the global zone. Let's\nsee how zones look from the global zone's perspective. From here you can\nwatch processes in non-global zones by using \n-Z\n command line argument in\n\nps(1)\n. Try this:\n\n\nroot@solaris:~# ps -efZ\n.....Skipped long output...\nzone1     root  4807     1   0 11:47:33 ?           0:00 /usr/lib/ssh/sshd\nzone1     root  4132     1   0 11:47:13 ?           0:00 /usr/lib/rad/rad -sp\nzone1     root  4736     1   0 11:47:30 ?           0:00 /usr/lib/autofs/automountd\nzone1     root  4737  4736   0 11:47:30 ?           0:00 /usr/lib/autofs/automountd\nglobal     root  4921  1636   0 12:25:04 pts/1       0:00 ps -efZ\nzone1     root  4869     1   0 11:47:37 ?           0:00 /sbin/dhcpagent\n\n\n\nAll processes are tagged with a zone name: it's either \nzone1\n or\n\nglobal\n. Remember to try this command again when you have more zones\nrunning (in our next exercises).\n\n\nNow login again into the zone and try '\nps -efZ\n' inside it. Check if\nyou can see global zone processes from inside the zone. Remember to\ncheck this again when you have more zones running.\n\n\nYou may also try the \nprstat(1M)\n command with \n-Z\n argument and see\nwhat happens.\n\n\nFor your homework: compare global and non-global zones installations.\nHow many packages are installed in both?How many services are running?\nCheck if you can login into the global zone with the zone user's (\njack\n)\ncredentials. Check if you can use your zone's root password in the\nglobal zone (of course, if they are different).",
            "title": "Inside the Zone"
        },
        {
            "location": "/virt/zone_cloning/index.html",
            "text": "Task:\n Your development team wants a copy of this environment for\ntesting purposes.\n\n\nLab:\n We will configure a new zone ('\nzone2\n') and then clone it from\nthe existing \nzone1\n.\n\n\nFirst, configure \nzone2\n the same way you've configured \nzone1\n:\n\n\nroot@solaris:~# zonecfg -z zone2 \nzone2: No such zone configured \nUse 'create' to begin configuring a new zone. \nzonecfg:zone2\n create \ncreate: Using system default template 'SYSdefault'\nzonecfg:zone2\n select anet linkname=net0\n(In spite of having only one anet, we still have to specify which one we select for configuration) \nzonecfg:zone2:anet\n set allowed-address=10.0.2.22/24 \n(Use the IP address assigned by your instructor)\nzonecfg:zone2:anet\n set defrouter=10.0.2.2 \n(Your instructor will give you the default gateway address)\nzonecfg:zone2:anet\n end\nzonecfg:zone2\n exit\n\n\n\nCheck:\n\n\nroot@solaris:~# zoneadm list -cv \n  ID NAME             STATUS     PATH                           BRAND    IP    \n   0 global           running    /                              solaris  shared\n   1 zone1            running    /zones/zone1                   solaris  excl  \n   - zone2            configured /zones/zone2                   solaris  excl  \n\n\n\nThen we create the new zone's profile. Start the System Configuration\nTool and repeat all the configuration steps you did for \nzone1\n. Just\nchange Computer Name to \nzone2\n.\n\n\nroot@solaris:~# sysconfig create-profile -o /root/zone2-profile\n\n\n\nBefore cloning we have to shutdown our running \nzone1\n:\n\n\nroot@solaris:~# zoneadm -z zone1 shutdown \n\n\n\nNow clone \nzone1\n and configure \nzone2\n automatically using this\nprofile:\n\n\nroot@solaris:~# zoneadm -z zone2 clone -c /root/zone2-profile zone1 \nroot@solaris:~# zoneadm list -cv \n  ID NAME             STATUS     PATH                           BRAND    IP    \n   0 global           running    /                              solaris  shared\n   1 zone1            installed  /zones/zone1                   solaris  excl  \n   2 zone2            installed  /zones/zone2                   solaris  excl  \n\n\n\nNow boot both zones:\n\n\nroot@solaris:~# zoneadm -z zone1 boot \nroot@solaris:~# zoneadm -z zone2 boot \nroot@solaris:~# zoneadm list -cv \n  ID NAME             STATUS     PATH                           BRAND    IP    \n   0 global           running    /                              solaris  shared\n   1 zone1            running    /zones/zone1                   solaris  excl  \n   2 zone2            running    /zones/zone2                   solaris  excl  \n\n\n\nSuccess! And it was faster than the initial installation, wasn't it?\n\n\nAfter it's done, login into zone2.\n\n\nroot@solaris:~# zlogin zone2 \n\n\n\nFirst of all, what about our Apache server?\n\n\nroot@zone2:~# pkg list '*apache*' \nNAME (PUBLISHER)                                  VERSION                    IFO\nweb/server/apache-22                              2.2.27-0.175.2.0.0.42.1    i--\n\n\n\nGreat! It's installed already! Check if it's running:\n\n\nroot@zone2:~# svcs *apache* \nonline 11:48:47 svc:/network/http:apache22 \n\n\n\nTry the \nzone2's\n address (\n10.0.2.22\n or whatever IP you used in\n\nzonecfg\n step) in Firefox in the global zone.\n\n\nThis is Zone1 and it works!\n - of course, we have cloned not only\nthe installed applications, but also all the files. Change it to \nZone2\n,\njust for consistency sake. Now you know how to use the vi editor, don't\nyou?\n\n\nroot@zone2:~# vi /var/apache2/2.2/htdocs/index.html",
            "title": "Zone Cloning"
        },
        {
            "location": "/virt/zone_monitoring/index.html",
            "text": "Task:\n Your departments want to know how much resources do they use\nto pay their fair share for the datacenter infrastructure.\n\n\nLab:\n Some familiar Solaris commands now include a \n-Z\n parameter to\nhelp you to monitor zones behavior. Try \nps -efZ\n and \nprstat -Z\n to take a\nlook. Try also a new command \nzonestat(1)\n to show zone statistics.\n\n\nroot@solaris:~# zonestat -z zone1,zone2 5 \n\nCollecting data for first interval...\nInterval: 1, Duration: 0:00:05\nSUMMARY                   Cpus/Online: 1/1   PhysMem: 2047M  VirtMem: 3071M\n                    ---CPU----  --PhysMem-- --VirtMem-- --PhysNet--\n               ZONE  USED %PART  USED %USED  USED %USED PBYTE %PUSE\n            [total]  0.05 5.45%  968M 47.3% 1251M 40.7%     0 0.00%\n           [system]  0.01 1.51%  287M 14.0%  735M 23.9%     -     -\n              zone1  0.00 0.16% 73.8M 3.60% 66.3M 2.16%     0 0.00%\n              zone2  0.00 0.13% 73.9M 3.61% 67.2M 2.18%     0 0.00%\n\n\n\nNote the parameters you can observe with \nzonestat\n: CPU utilization,\nphysical and virtual memory usage, network bandwidth utilization.",
            "title": "Zone Monitoring"
        },
        {
            "location": "/virt/zone_resource/index.html",
            "text": "Task:\n You want to control the zones' resource usage. You want to\nassign certain amount of processing power to each zone.\n\n\nLab:\n We now know how to create and clone zones. Now let's try to cap\nCPU resources in one zone to demonstrate the basics of resource\nmanagement in Solaris.\n\n\nFirst, run a simple CPU-consuming script in the \nzone1\n:\n\n\nroot@solaris:~# zlogin zone1 \nbash -c 'while true ; do date \n /dev/null ; done'\n \n\n\n\nNote that we are simply using \nzlogin\n to pass the command to the zone.\n\n\nWhat's going on in the global zone? Open another window, become root and\ncheck:\n\n\nroot@solaris:~# vmstat 5 \n\n\n\nIdle is 0, system time is around 70%. Not good.\n\n\nroot@solaris:~# zonestat 5 \n\n\n\nzone1\n consumes 70-80% of total resources, the rest is spent in global\nzone (most likely serving \nzone1\n's requests). We decided to reduce the\n\nzone1\n's resource consumption and give it only 50% of our CPU cycles. You\ncan leave the window with \nzonestat\n running open and start another\nterminal session to change the zone's parameters. This way you'll be\nable to see the changes in real time.\n\n\nroot@solaris:~# zonecfg -z zone1 \nzonecfg:zone1\n add capped-cpu \nzonecfg:zone1:capped-cpu\n set ncpus=0.5 \nzonecfg:zone1:capped-cpu\n end \nzonecfg:zone1\n exit \nroot@solaris:~# zoneadm -z zone1 apply \nzone 'zone1': Checking: Adding rctl name=zone.cpu-cap\nzone 'zone1': Applying the changes\n\n\n\nLook into another window where you have \nzonestat\n running (or run\n\nzonestat 5\n again). You should see the CPU utilization by \nzone1\n dropped\ndown to 50%.\n\n\nYou can check the updated \nzone1\n's configuration and see that it's CPU\ncap is now set to 50%.\n\n\nroot@solaris:~# zonecfg -z zone1 info\n. . . \ncapped-cpu:\n    [ncpus: 0.50]\nrctl:\n    name: zone.cpu-cap\n    value: (priv=privileged,limit=50,action=deny)\n\n\n\nThat means next time you run zone1 it will receive 50% of one CPU. It's\nimportant to note: it's \nnot\n 50% of ALL CPUs, it's 50% of only one\nvirtual CPU. In our case in VirtualBox VM we have only one CPU. In most\nreal life cases you will have tens and hundreds of virtual CPUs. Set\nyour values accordingly.\n\n\nIs it also possible to change this CPU cap parameter on the fly. This\nchange will be temporary, only until the reboot.\n\n\nroot@solaris:~# prctl -n zone.cpu-cap -r -v 25 -i zone zone1 \n\n\n\nCheck if it works looking in the window with \nzonestat\n or running it\nagain:\n\n\nroot@solaris:~# zonestat 5 \n\n\n\nDon't forget to stop the infinite loop in your zone! Or simply halt the\nzone.\n\n\nroot@solaris:~# zoneadm -z zone1 halt \n\n\n\nOther resources can be capped this way as well: memory, swap, number of\nthreads etc. Again, think about how it can be used in real life\nsituations?",
            "title": "Zone Resource Management"
        },
        {
            "location": "/ai/ai_intro/index.html",
            "text": "In this lab we will learn how to use the new installation method\nintroduced in Oracle Solaris 11: Automated Installer. We will review\nseveral typical situations and discuss how to solve them using Automated\nInstaller (AI). This lab includes the following scenarios:\n\n\n\n\nSimple installation: one CPU architecture, one Solaris version, one\n    set of packages;\n\n\nSeveral Solaris versions to be installed from one AI server;\n\n\nDifferent package configurations: small-server, large-server,\n    additional packages;\n\n\nDifferent host configurations: hostnames, IP addresses, etc.;\n\n\nZone installation from AI server;\n\n\nInstalling from Unified Archives\n\n\n\n\nBut before we start, 60 seconds of theory.\n\n\n\n\n60 seconds of theory\n\n\nAutomated Installer has three major components: install services,\nmanifests and system profiles. Here is what they do:\n\n\n\n\nInstall Services:\n they provide bootable media for clients and\n    work in conjunction with IPS repositories. You need one install\n    service per each CPU architecture and OS version.\n\n\nAI Manifests:\n they describe installation configuration (disk\n    layout etc.) and package sets. You need one manifest per each set of\n    packages (small-server, large-server, additional packages) and\n    disk layout. You can modify them dynamically at the time\n    of installation.\n\n\nSystem Configuration (SC) Profiles:\n they describe host OS\n    configuration including hostname, network parameters, time zone,\n    passwords etc. They are optional: if you don't have one, you'll have\n    to specify all these parameters at first boot.\n\n\n\n\n\n\nAutomated Installer works in conjunction with IPS repository, so we will\nneed one for our exercises. Luckily, we have one installed in our lab.\nYou instructor will give you all necessary information.",
            "title": "Automated Installer Intro"
        },
        {
            "location": "/ai/ai_simple/index.html",
            "text": "Let's start with the simplest possible case: we have systems with only\none CPU architecture, we want to install the same Solaris version and\nthe same set of packages on every system. We have to install the AI\npackage and create the first install service, which is going to be the\ndefault one.\n\n\nStart with checking if \ninstalladm(1M)\n is installed on the system:\n\n\nroot@solarislab:~# pkg list -a installadm\nNAME (PUBLISHER)                                  VERSION                    IFO\ninstall/installadm                                0.5.11-0.175.2.8.0.1.2     i--\n\n\n\nLetter \n'i'\n in the last column mean that it's installed. If it's not,\njust install it with \npkg install install/installadm\n. Now let's check\nif we have any install services already (who knows, somebody might have\nconfigured them before us).\n\n\nroot@solarislab:~# installadm list\nThere are no services configured on this server.\n\n\n\nSo far, so good. We can create our first install service right now.\n\n\nroot@solarislab:~# installadm create-service\nOK to use subdir of /export/auto_install to store image? [y|N]: y\n  0% : Service svc:/network/dns/multicast:default is not online.  Installation services will not be advertised via multicast DNS.\n  0% : Creating service from: pkg:/install-image/solaris-auto-install\n  0% : Using publisher(s):\n  0% :     solaris: http://10.80.11.34:81/\n  5% : Refreshing Publisher(s)\n 15% : Planning Phase\n 24% : Download Phase\n 62% : Actions Phase\n 91% : Finalize Phase\n 91% : Creating sparc service: solaris11_3-sparc\n 91% : Image path: /export/auto_install/solaris11_3-sparc\n 91% : Setting \nsolaris\n publisher URL in default manifest to:\n 91% :  http://10.80.11.34:81/\n 91% : Creating default-sparc alias\n 91% : Setting \nsolaris\n publisher URL in default manifest to:\n 91% :  http://10.80.11.34:81/\n100% : Created Service: 'solaris11_3-sparc'\n100% : Refreshing SMF service svc:/system/install/server:default\n100% : Enabling SMF service svc:/network/dhcp/server:ipv4\n100% : Warning: mDNS registry of service 'solaris11_3-sparc' could not be verified.\n100% : Warning: mDNS registry of service 'default-sparc' could not be verified.\n\n\n\nLet's look more closely at the output. First of all, we have to confirm\nthat we are OK with storing boot image in the default\n\n/export/auto_install\n directory (of course, you can change that if you\nwant). Then, as soon as it's just our local lab exercise, we are not\ngoing to use multicast DNS to advertise it widely. Three lines after\nthat tell us that we are going to use a special package that contains\nSolaris boot image and it's located on our local IPS repository.\n\n\nIf you look further, you see that \ninstalladm\n creates a SPARC service\nand from its name we can guess that it's based on Oracle Solaris 11.3\nversion. Why? Because the last version we have in the repository is\n11.3. You can check it:\n\n\nroot@solarislab:~# pkg list -af entire | head\nNAME (PUBLISHER)                                  VERSION                    IFO\nentire                                            0.5.11-0.175.3.0.0.29.0    ---\nentire                                            0.5.11-0.175.3.0.0.28.0    ---\nentire                                            0.5.11-0.175.2.14.0.5.0    ---\nentire                                            0.5.11-0.175.2.13.0.6.0    ---\nentire                                            0.5.11-0.175.2.12.0.6.0    i--\nentire                                            0.5.11-0.175.2.12.0.5.0    ---\nentire                                            0.5.11-0.175.2.11.0.5.0    ---\nentire                                            0.5.11-0.175.2.10.0.5.0    ---\nentire                                            0.5.11-0.175.2.9.0.5.0     ---\n\n\n\nYou see: our current installed version is \n11.2.12\n, but the latest\navailable is \n11.3.0.0.29\n. So \ninstalladm\n uses it by default.\n\n\nWhat else can we learn from the output? That \ninstalladm\n has also\ncreated a default manifest for this service and set the publisher in\nthat manifest to our current publisher with local address \n10.80.11.34\n.\nIt also has enabled a DHCP service, but we don't need it for SPARC\ninstallation so we'd better disable it right now:\n\n\nroot@solarislab:~# svcadm disable network/dhcp/server:ipv4\n\n\n\nNow the most interesting part: let's try to install a system from this\nAI server. To do that we have to get to the console of a client system.\nIn our lab environment we have created several guest domains, but we\ndidn't install Solaris in them. You instructor will give you all\nnecessary information how to access your domain's console. Usually it's\n\ntelnet localhost 5000\n (or other port number). When you get to OpenBoot\n\nok\n prompt enter the following \nsetenv\n command. Make sure that\neverything that goes after \nnetwork-boot-arguments\n doesn't have any\nspaces. Your instructor will provide you with the information about\nhostnames and IP addresses\nthey are marked in italic in the following command.\n\n\n\n{0} ok setenv network-boot-arguments hostname=\nai-client\n,host-ip=\n10.80.10.91\n,router-ip=\n10.80.11.254\n,subnet-mask=255.255.254.0,file=http://10.80.11.34:5555/cgi-bin/wanboot-cgi\n\n\n\n\nAfter that enter the boot command and watch the installation process. We\nhave skipped most of the output, but you are encouraged to read and try\nto interpret as much as possible. You instructor will help you in that.\n\n\n\n{0} \nok boot net - install\n\nnetwork-boot-arguments =  hostname=ai-client,host-ip=10.80.10.92,router-ip=10.80.11.254,subnet-mask=255.255.254.0,file=http://10.80.11.34:5555/cgi-bin/wanboot-cgi\nNOTICE: Entering OpenBoot.\nNOTICE: Fetching Guest MD from HV.\nNOTICE: Starting additional cpus.\nNOTICE: Initializing LDC services.\nNOTICE: Probing PCI devices.\nNOTICE: Finished PCI probing.\n\nSPARC T4-2, No Keyboard\nCopyright (c) 1998, 2014, Oracle and/or its affiliates. All rights reserved.\nOpenBoot 4.36.1, 4.0000 GB memory available, Serial #83607872.\nEthernet address 0:14:4f:fb:c1:40, Host ID: 84fbc140.\n\n\n\nBoot device: /virtual-devices@100/channel-devices@200/network@0  File and args: - install\n\n. . . . . . . .\nWed Sep 16 21:19:40 wanboot info: miniroot: Download complete\nSunOS Release 5.11 Version 11.3 64-bit\nCopyright (c) 1983, 2015, Oracle and/or its affiliates. All rights reserved.\nRemounting root read/write\nProbing for device nodes ...\nPreparing network image for use\nDownloading solaris.zlib\n. . . . . . . . . . .\nDone mounting image\nConfiguring devices.\nHostname: ai-client\nService discovery phase initiated\nService name to look up: default-sparc\nService discovery over multicast DNS failed\nService default-sparc located at 10.80.11.34:5555 will be used\nService discovery finished successfully\nProcess of obtaining install manifest initiated\nUsing the install manifest obtained via service discovery\n\nai-client console login:\nAutomated Installation started\nThe progress of the Automated Installation will be output to the console\nDetailed logging is in the logfile at /system/volatile/install_log\nPress RETURN to get a login prompt at any time.\n\n21:22:41    Install Log: /system/volatile/install_log\n21:22:41    Using Derived Script: /system/volatile/ai.xml\n21:22:41    Using profile specification: /system/volatile/profile\n21:22:41    Using service list file: /var/run/service_list\n21:22:41    Starting installation.\n. . . . .\n21:33:19    91% update-filesystem-owner-group completed.\n21:33:19    92% transfer-ai-files completed.\n21:33:19    100% create-snapshot completed.\n21:33:19    100% None\n21:33:19    Automated Installation succeeded.\n21:33:20    You may wish to reboot the system at this time.\nAutomated Installation finished successfully\nThe system can be rebooted now\nPlease refer to the /system/volatile/install_log file for details\nAfter reboot it will be located at /var/log/install/install_log\n\n\n\n\nWe skipped a lot of messages about the installation progress, but you\nmay want to take a look at them\nit'll give you a better understanding\nabout the processes going on during installation.\n\n\nWell, now we can reboot the system to see what was installed. For the AI\nboot image login is \nroot\n, password is \nsolaris\n.\n\n\n \nai-client console login:\nai-client console login: \nroot\n\nPassword: solaris\nSep 16 21:40:07 ai-client login: ROOT LOGIN /dev/console\nOracle Corporation      SunOS 5.11      11.3    August 2015\nroot@ai-client:~# \nreboot\n\nSep 16 21:40:11 ai-client reboot: initiated by root on /dev/console\n. . . . . .\n\n\n\n\nAfter several familiar messages you will come to the System\nConfiguration screen. From here you define the hostname, network\nparameters such as IP address, router IP, etc., time zone, root and user\npasswords and others. Pretty standard procedure, so go ahead and enter\nthe following:\n\n\n\nComputer name: \nai-client\n\nNetwork configuration: \nManually\n\nIP address: \n(provided by the instructor)\n\nNetmask: \n255.255.254.0\n\nRouter: \n10.80.11.254\n\nDo not configure DNS\nAlternate Name Service: \nNone\n\nTime Zone: Americas -> United States -> \nyour time zone\n\nLanguage: \nyour preferred language\n\nTerritory: \nyour territory\n\nDate and Time: accept the default by pressing \nF2\n\nKeyboard: \nyour preferred keyboard\n\nRoot password: \nsolaris1\n\nYour real name: \nLab User\n\nUsername: \nlab\n\nUser password: \noracle1\n\n\n\n\n\nOn the final screens just press \nF2\n to continue.\n\n\nAfter a while you will see the login prompt. Use \nlab/oracle1\n to login:\n\n\n\nai-client console login:\nai-client console login: \nlab\n\nPassword: \noracle1\n\nOracle Corporation      SunOS 5.11      11.3    August 2015\nlab@ai-client:~$\n\n\n\n\nSo, what have we just installed?\n\n\nlab@ai-client:~$ uname -a\nSunOS ai-client 5.11 11.3 sun4v sparc sun4v\nlab@ai-client:~$ pkg list entire\nNAME (PUBLISHER)                                  VERSION                    IFO\nentire                                            0.5.11-0.175.3.0.0.29.0    i--\nlab@ai-client:~$ pkg list | wc\n     581    1744   47084\nlab@ai-client:~$ pkg list | grep group\ngroup/system/management/rad/rad-server-interfaces 0.5.11-0.175.3.0.0.29.1    i--\ngroup/system/solaris-core-platform                0.5.11-0.175.3.0.0.29.1    i--\ngroup/system/solaris-large-server                 0.5.11-0.175.3.0.0.29.0    i--\n\n\n\nCongratulations, we have just created an install server and installed\nour first client from it. We used the simplest possible configuration:\nby default we installed the latest available release of Solaris in you\nrepository, default manifest specifies that we install\n\nsolaris-large-server\n group and also we used interactive System\nConfiguration tool to provide hostname, IP address, etc. Of course, all\nthat can be tuned to your particular situation. And this is what we are\ngoing to do in the rest of this lab.",
            "title": "Simple Installation"
        },
        {
            "location": "/ai/ai_another_ver/index.html",
            "text": "Of course, there are situations when you want to install a different\nversion of Oracle Solaris. For instance, in our lab repository we have\nversions 11 Express, 11.0, 11.1, 11.2 with their support SRUs. How can\nwe specify a different version for installation?\n\n\nIn this case we have to create a separate install service. So, the more\ndifferent versions and CPU architectures you want to install, the more\ninstall services you have. Let's figure out what's available. We list 10\nmost recent updates here:\n\n\nroot@solarislab:~# pkg list -avf  install-image/solaris-auto-install | head    \nFMRI                                                                                        IFO\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.3.0.0.29.1:20150817T163032Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.3.0.0.28.0:20150803T161232Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.14.0.5.0:20150910T165659Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.13.0.6.0:20150810T175313Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.12.0.6.0:20150715T155314Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.12.0.5.0:20150701T004857Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.11.0.5.0:20150610T201614Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.10.0.5.0:20150506T023353Z ---\npkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.9.0.5.0:20150404T222307Z ---\n\n\n\nHere we use \n-v\n option here to print full FMRIs (Fault Managed Resource\nIndicator) for packages: we will need it later.\n\n\nWell, we already have created an install service for Solaris 11.3, now\nlet's create another one for Solaris 11.2, SRU 11, for example.\n\n\nroot@solarislab:~# installadm create-service -s pkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.11.0.5.0\nOK to use subdir of /export/auto_install to store image? [y|N]: y\n  0% : Service svc:/network/dns/multicast:default is not online.  Installation services will not be advertised via multicast DNS.\n  0% : Creating service from: pkg://solaris/install-image/solaris-auto-install@5.11,5.11-0.175.2.11.0.5.0\n  0% : Using publisher(s):\n  0% :     solaris: http://10.80.11.34:81/\n  5% : Refreshing Publisher(s)\n 15% : Planning Phase\n 24% : Download Phase\n 62% : Actions Phase\n 91% : Finalize Phase\n 91% : Creating sparc service: solaris11_2_11_5_0-sparc\n 91% : Image path: /export/auto_install/solaris11_2_11_5_0-sparc\n 91% : Setting \nsolaris\n publisher URL in default manifest to:\n 91% :  http://10.80.11.34:81/\n100% : Created Service: 'solaris11_2_11_5_0-sparc'\n100% : Refreshing SMF service svc:/system/install/server:default\n100% : Warning: mDNS registry of service 'solaris11_2_11_5_0-sparc' could not be verified.\n\n\n\nOK, done. Now let's make it the default SPARC service.\n\n\nroot@solarislab:~# installadm delete-service -n default-sparc\nWARNING: The service you are deleting, or a dependent alias, is the\nalias for the default sparc service. Without the 'default-sparc'\nservice, sparc clients will fail to boot unless explicitly assigned to\na service using the create-client subcommand.\nAre you sure you want to delete this alias? [y|N]: y\nWarning: mDNS registry of service default-sparc could not be verified.\nDeleted Service: 'default-sparc'\nroot@solarislab:~# installadm create-service -t solaris11_2_11_5_0-sparc -n default-sparc\n  0% : Service svc:/network/dns/multicast:default is not online.  Installation services will not be advertised via multicast DNS.\n  0% : Creating sparc alias: default-sparc\n  0% : Setting \nsolaris\n publisher URL in default manifest to:\n  0% :  http://10.80.11.34:81/\n100% : Created Service: 'default-sparc'\n100% : Refreshing SMF service svc:/system/install/server:default\n100% : Enabling SMF service svc:/network/dhcp/server:ipv4\n100% : Warning: mDNS registry of service 'default-sparc' could not be verified.\nroot@solarislab:~# svcadm disable dhcp/server:ipv4\n\n\n\nAnd now you can try to install the same client again to check if this\nservice installs the version we have created it with, namely, Solaris\n11.2.11.\n\n\nSpoiler Alert:\n We can already tell you what is going to happen\nduring this test install. If you want to learn it yourself, go ahead and\ninstall the \nai-client\n LDom again using the instructions above and then\ncheck what Solaris version was installed with \npkg list entire\n. For\nthose of you who want to save time and skip this test installation step,\nhere is what's going to happen:\n\n\nThe client system will boot from the install image, which is \n11.2.11\n,\nbut it will install the latest available Solaris SRU, which is \n11.2.14\n\nin our default repository. Why so? Because it uses the default manifest\nand by default it installs the latest SRU available. If we want to\ninstall that particular SRU, we have to create a new manifest. It's time\nto learn about AI manifests.\n\n\nFirst, let's see what we've got already:\n\n\nroot@solarislab:~# installadm list -m\nService Name             Manifest Name Type    Status  Criteria\n------------             ------------- ----    ------  --------\ndefault-sparc            orig_default  derived default none\nsolaris11_2_11_5_0-sparc orig_default  derived default none\nsolaris11_3-sparc        orig_default  derived default none\n\n\n\nNote that we have two services (one per each Solaris update version)\nplus one alias. Each service has one default manifest. Our plan is to\nmodify the default manifest and add it to the default service. First, we\ncopy the manifest XML file from its default location to the home\ndirectory (actually, it can be any directory).\n\n\nroot@solarislab:~# cp /usr/share/auto_install/manifest/default.xml s11.2.11-manifest.xml\n\n\n\nNow we can edit this copy. So we want to specify which SRU we are going\nto install. Let's list what's available.\n\n\nroot@solarislab:~# pkg list -af entire | head\nNAME (PUBLISHER)                                  VERSION                    IFO\nentire                                            0.5.11-0.175.3.0.0.29.0    ---\nentire                                            0.5.11-0.175.3.0.0.28.0    ---\nentire                                            0.5.11-0.175.2.14.0.5.0    ---\nentire                                            0.5.11-0.175.2.13.0.6.0    ---\nentire                                            0.5.11-0.175.2.12.0.6.0    i--\nentire                                            0.5.11-0.175.2.12.0.5.0    ---\nentire                                            0.5.11-0.175.2.11.0.5.0    ---\nentire                                            0.5.11-0.175.2.10.0.5.0    ---\nentire                                            0.5.11-0.175.2.9.0.5.0     ---\n\n\n\nSuppose we decided to install version \n11.2.11\n. Take a note of the\nversion string: \n0.5.11-0.175.2.11.0.5.0\n -- you will need it when\nediting the manifest. Now start your favorite text editor and open the\nmanifest file:\n\n\nroot@solarislab:~# vi s11.2.11-manifest.xml\n\n\n\nScroll to almost the end of the file and find the following lines:\n\n\n\n        For instance, to specify a particular build of S11.2, the\n        following should be used:\n\n            \nname\npkg:/entire@0.5.11,5.11-0.175.2.0.0.build\n/name\n\n      --\n\n      \nsoftware_data action=\ninstall\n\n        \nname\npkg:/entire@0.5.11-0.175.2\n/name\n\n        \nname\npkg:/group/system/solaris-large-server\n/name\n\n      \n/software_data\n\n\n\n\n\nChange them to tell AI server that you want that specific SRU of\nSolaris. Also we can change the default 'solaris-large-server' group to\n'solaris-small-server' one. After editing it should look like this\n(changes are in \nbold\n):\n\n\n\n        For instance, to specify a particular build of S11.2, the\n        following should be used:\n\n            \nname\npkg:/entire@0.5.11,5.11-0.175.2.0.0.build\n/name\n\n      --\n\n      \nsoftware_data action=\ninstall\n\n        \nname\npkg:/entire@0.5.11-0.175.2\n.11.0.5.0\n/name\n\n        \nname\npkg:/group/system/solaris-\nsmall\n-server\n/name\n\n      \n/software_data\n\n\n\n\n\nNow we have to install this manifest in the default SPARC service.\n\n\nroot@solarislab:~# installadm create-manifest -n default-sparc -m s11.2.11 -f ./s11.2.11-manifest.xml\nCreated Manifest: 's11.2.11'\nroot@solarislab:~# installadm list -m\nService Name             Manifest Name Type    Status   Criteria\n------------             ------------- ----    ------   --------\ndefault-sparc            orig_default  derived default  none\n                         s11.2.11      derived inactive none\nsolaris11_2_11_5_0-sparc orig_default  derived default  none\nsolaris11_3-sparc        orig_default  derived default  none\n\n\n\nOK, we have amended the manifest and installed it onto the server, but\nhow can we use it? How can we let the install server know that which\nmanifest to use? Well, we can specify a set of criteria to decide which\nmanifest to apply. We can include IP address, hostname, MAC address in\nthis criteria and the install server will use certain manifests for\ndifferent clients. If the client doesn't fit any criteria, then the\ndefault manifest is applied. Let's define such criteria for our client.\n\n\nIf you remember, before booting the client from OpenBoot prompt we\nassigned several boot parameters to it: IP address, hostname, wanboot\nscript location, etc. So now we can use, for example, its hostname\n\nai-client\n as a criteria.\n\n\nroot@solarislab:~# installadm set-criteria -n default-sparc -m s11.2.11 -c hostname=ai-client\nChanged Manifest: 's11.2.11'\nroot@solarislab:~# installadm list -m\nService Name             Manifest Name Type    Status  Criteria\n------------             ------------- ----    ------  --------\ndefault-sparc            s11.2.11      derived active  hostname = ai-client\n                         orig_default  derived default none\nsolaris11_2_11_5_0-sparc orig_default  derived default none\nsolaris11_3-sparc        orig_default  derived default none\n\n\n\nAnd now we can try to install the client. Login to the console with\n\ntelnet localhost 5000\n (using the port number provided by your\ninstructor) and repeat the installation process described above.\n\n\nAfter you passed all the installation and configuration steps, login\ninto the client and check it's version:\n\n\nroot@ai-client:~# pkg list entire\nNAME (PUBLISHER)                                  VERSION                    IFO\nentire                                            0.5.11-0.175.2.11.0.5.0    i--\n\n\n\nSuccess! Let's explain what we have observed here.\n\n\n\n\nBy default install service is always created from the latest\n    available version of Solaris\n\n\nFor each Solaris Update (i.e. \n11.1\n, \n11.2\n, \n11.3\n) we need a separate\n    install service\n\n\nBy default install service installs the latest available SRU for the\n    Solaris Update this service is created for (in our case the service\n    created from Solaris \n11.2.11\n will install \n11.2.14\n unless we\n    specify otherwise)\n\n\nTo specify a particular SRU which you want to install you have to\n    edit and install AI manifest into the install service\n\n\n\n\nNow we will learn how to customize installation process even further. We\nstart with adding other software packages.",
            "title": "Another Version of Oracle Solaris"
        },
        {
            "location": "/ai/ai_packages/index.html",
            "text": "Most likely you want not just plain vanilla \nsmall-server\n Solaris\ninstallation, but you are interested in some additional packages that\ncan be installed. Instead of installing them manually or via custom\nscripts, why don't we specify them in the manifest? As you might have\nguessed already, if we can specify the version of \nentire\n package in\nthe \nsoftware_data\n part of the manifest, we can add more packages the\nsame way, by using their FMRI names. Let's imagine we want to add a new\nmanagement platform Puppet which was added recently to Oracle Solaris\n11.2. If Puppet is installed from the beginning in the system, it can\nmake system management much easier. But that's a topic for another\nhands-on lab.\n\n\nFirst, we have to find Puppet package we want to install.\n\n\nroot@solarislab:~# pkg list -av '*puppet*'\nFMRI                                                                         IFO\npkg://solaris/system/management/puppet@3.6.2,5.11-0.175.2.5.0.2.0:20141114T212642Z ---\npkg://solaris/system/management/puppet-19@3.6.2,5.11-0.175.2.5.0.2.0:20141114T212630Z ---\n\n\n\nCopy the first package's FMRI until the \n@\n sign into the clipboard or\nNotepad. Open the manifest file and edit it. After the line\nwith \nsolaris-small-server\n FMRI insert the line with Puppet's FMRI\n(shown in bold).\n\n\n\n  \nsoftware_data action=\ninstall\n\n    \nname\npkg:/entire@0.5.11-0.175.2.11.0.5.0\n/name\n\n    \nname\npkg:/group/system/solaris-small-server\n/name\n\n    \nname\npkg://solaris/system/management/puppet\n/name\n\n  \n/software_data\n\n\n\n\n\nDon't forget to update the manifest after changing the XML file!\n\n\nroot@solarislab:~# installadm update-manifest -n default-sparc -m s11.2.11 -f ./s11.2.11-manifest.xml\nChanged Manifest: 's11.2.11'\n\n\n\nThe rest is easy. Get back to OpenBoot prompt on the client (use \ninit\n0\n for that) and install it again (with \nboot net - install\n). After\nconfiguring the system (hostname, IP, time zone, etc.) login via console\nand check if you have installed the right version of Solaris and the\nPuppet package.\n\n\n\n    ai-client console login: \nroot\n\n    Password: \nsolaris1\n\n    Sep 22 17:43:17 ai-client login: ROOT LOGIN /dev/console\n    Oracle Corporation      SunOS 5.11      11.2    May 2015\n    root@ai-client:~# \npkg list entire\n\n    NAME (PUBLISHER)                                  VERSION                    IFO\n    entire                                            0.5.11-0.175.2.11.0.5.0    i--\n    root@ai-client:~# \npkg list '*puppet*'\n\n    NAME (PUBLISHER)                                  VERSION                    IFO\n    system/management/puppet                          3.6.2-0.175.2.5.0.2.0      i--\n    system/management/puppet-19                       3.6.2-0.175.2.5.0.2.0      i--\n\n\n\n\nSuccess! Let's summarize what we have learned in this exercise:\n\n\n\n\nAI manifest controls what software is going to be installed on\n    the client. You can add packages that are available from your\n    IPS repository.\n\n\nDon't forget to update the manifest in the install service after you\n    have edited the XML file.\n\n\nYou can specify which clients should use each particular manifest by\n    creating criteria based on hostname, IP address, MAC address, etc.\n    IMPORTANT: in this criteria AI uses the hostname and IP address that\n    were set in \nnetwork-boot-parameters\n by OpenBoot, not the\n    hostname/IP you \nintend\n to use after installation. Usually they are\n    the same, but could be different.",
            "title": "Different Set of Packages"
        },
        {
            "location": "/ai/ai_profiles/index.html",
            "text": "Are you tired already of typing in hostname, IP address, root and user\npasswords every time you do a system installation? I am! We can solve\nthis problem by using System Configuration Profiles. We can specify all\nthose parameters in another XML file (or several files) and use them\nduring AI installation. This way the system will be usable right after\ninstallation and we won't have that interactive stage at the first boot.\n\n\nWe start with a simple profile and use the same configuration parameters\nthat we used during interactive configuration.\n\n\nroot@solarislab:~# sysconfig create-profile -o ai-client\n\n\n\nYou will see the familiar configuration screens. Use the same parameters\n(hostname, IP address, router address, user and root passwords, etc.)\nthat you used before. After you are finished with the tool, you'll see\nthe following message:\n\n\n    SC profile successfully generated as:\n    ./ai-client/sc_profile.xml\n\n    Exiting System Configuration Tool. Log is available at:\n    /system/volatile/sysconfig/sysconfig.log.26544\n\n\n\nYou see that the tool has created a directory named \nai-client\n and\nplaced the \nsc_profile.xml\n file there. We will need this location in\nthe next command. You can take a look at the resulting XML file to see\nits structure and content. Most of things in it are self-explanatory.\nNow we have to add this profile to the install service and specify again\nthe criteria when it should be used. Here is the command for that:\n\n\nroot@solarislab:~# installadm create-profile -n default-sparc -p ai-client -c hostname=ai-client -f ./ai-client/sc_profile.xml\nCreated Profile: 'ai-client'\nroot@solarislab:~# installadm list -p\nService Name  Profile Name Environment Criteria\n------------  ------------ ----------- --------\ndefault-sparc ai-client    system      hostname = ai-client\n\n\n\nNow try to install again our client (\ninit 0\n, \nboot net - install\n). You\nwill see that the installation will run the same way, but after reboot\nyou won't get the configuration screen. You system will get its hostname\nand network parameters automatically and you'll see the console prompt:\n\nai-client console login:\n. That means your system was installed\ncompletely without intervention from your side.\n\n\nNow you may say: \"Well, should I create a separate profile for each and\nevery system I want to install? They all have different IPs and\nhostnames!\". You are right\nall system have different identities. Oracle\nSolaris engineers know about that and they feel your pain. So they\ncreated a system of templates that you can use to avoid creating\nthousands of separate profiles. Let's learn about them.\n\n\nTake a look at the following file:\n\n\nroot@solarislab:~# cat /usr/share/auto_install/sc_profiles/template_var\n#\n# Copyright (c) 2013, Oracle and/or its affiliates. All rights reserved.\n#\n#\n# This file provides a list of supported profile template variables and\n# sample values for current version of Solaris.\n#\nAI_ARCH=i86pc\nAI_CPU=i386\nAI_HOSTNAME=solaris\nAI_IPV4=192.168.56.2\nAI_IPV4_PREFIXLEN=24\nAI_MAC=01:01:01:01:01:01\nAI_MEM=2048\nAI_NETWORK=10.0.0.0\nAI_NETLINK_DEVICE=e1000g0\nAI_NETLINK_VANITY=net0\nAI_ROUTER=10.0.0.1\nAI_SERVICE=default-i386\nAI_ZONENAME=testzone\n\n\n\nThese are the variables that you can use in system configuration\nprofiles: IP address, hostname and others. It's very important to notice\nthat those are system parameters \nat the time it boots with the AI\nimage\n. In case of SPARC systems those parameters as set in OpenBoot\nwith \nsetenv network-boot-parameters\n command. Usually (but not\nnecessarily) those are parameters you want to see in your system after\ninstallation. That's why we can use them in SC profiles and get what we\nexpected.\n\n\nLet's copy the SC profile XML file that we just created with the\ninteractive tool and edit it.\n\n\nroot@solarislab:~# cp ai-client/sc_profile.xml ./sc_template.xml\nroot@solarislab:~# vi ./sc_template.xml\n\n\n\nInstead of giving you step by step instructions on what to change, it\nseems to be easier to provide you with the output of \ndiff(1)\n command.\nIt's pretty easy to figure out the changes that were made.\n\n\n\nroot@solarislab:~# \ndiff ai-client/sc_profile.xml sc_template.xml\n\n8c8\n\n         \npropval type=\nastring\n name=\nnodename\n value=\nai-client\n/\n\n---\n\n         \npropval type=\nastring\n name=\nnodename\n value=\n{{AI_HOSTNAME}}\n/\n\n21c21\n\n         \npropval type=\nnet_address_v4\n name=\nstatic_address\n value=\n10.80.10.92/23\n/\n\n---\n\n         \npropval type=\nnet_address_v4\n name=\nstatic_address\n value=\n{{AI_IPV4}}/{{AI_IPV4_PREFIXLEN}}\n/\n\n24c24\n\n         \npropval type=\nnet_address_v4\n name=\ndefault_route\n value=\n10.80.11.254\n/\n\n---\n\n         \npropval type=\nnet_address_v4\n name=\ndefault_route\n value=\n{{AI_ROUTER}}\n/\n\n\n\n\n\nAfter you are done with editing, install this profile in the install\nservice and remove the old one. Note that this time we don't specify any\ncriteria, which means that this profile will be applied to all clients.\n\n\nroot@solarislab:~# installadm create-profile -n default-sparc -p sc-template  -f ./sc_template.xml\nCreated Profile: 'sc-template'\nroot@solarislab:~# installadm list -p\nService Name  Profile Name Environment Criteria\n------------  ------------ ----------- --------\ndefault-sparc ai-client    system      hostname = ai-client\n              sc-template  system      none\nroot@solarislab:~# installadm delete-profile -n default-sparc -p ai-client\nDeleted Profile: 'ai-client'\nroot@solarislab:~# installadm list -p\nService Name  Profile Name Environment Criteria\n------------  ------------ ----------- --------\ndefault-sparc sc-template  system      none\n\n\n\nAnd now it's time to test this profile. First, try to install your\nclient system with the current settings. If it works OK, then change the\nsettings with \nsetenv network-boot-parameters\n to a different hostname\nand IP and test again. Here is the example. Your instructor will give\nyou the necessary information about available IP addresses (please,\ndon't \"invent\" them yourself, we have very strict rules in the\nlaboratory!).\n\n\n\n{0} ok \nprintenv network-boot-arguments\n\nnetwork-boot-arguments =  hostname=ai-client,host-ip=10.80.10.92,router-ip=10.80.11.254,subnet-mask=255.255.254.0,file=http://10.80.11.34:5555/cgi-bin/wanboot-cgi\n{0} ok \nsetenv network-boot-arguments hostname=ai-client1,host-ip=10.80.10.91,router-ip=10.80.11.254,subnet-mask=255.255.254.0,file=http://10.80.11.34:5555/cgi-bin/wanboot-cgi\n\nnetwork-boot-arguments =  hostname=ai-client1,host-ip=10.80.10.91,router-ip=10.80.11.254,subnet-mask=255.255.254.0,file=http://10.80.11.34:5555/cgi-bin/wanboot-cgi\n{0} ok\n\n\n\n\nAfter the installation reboot the system and check if it set the\nparameters you expected.\n\n\nai-client1 console login: lab\nPassword:\nOracle Corporation      SunOS 5.11      11.2    August 2015\nlab@ai-client1:~$ ipadm\nNAME              CLASS/TYPE STATE        UNDER      ADDR\nlo0               loopback   ok           --         --\n   lo0/v4         static     ok           --         127.0.0.1/8\n   lo0/v6         static     ok           --         ::1/128\nnet0              ip         ok           --         --\n   net0/v4        static     ok           --         10.80.10.91/23\n   net0/v6        addrconf   ok           --         fe80::214:4fff:fef8:5f6/10\n   net0/v6        addrconf   ok           --         2606:b400:410:831:214:4fff:fef8:5f6/64\n\n\n\nSuccess! Again, let's summarize what we have learned in this exercise.\n\n\n\n\nSystem parameters which we usually set at the first boot, can be\n    configured and set via System Configuration Profiles (SC profiles).\n\n\nSC profiles can be created using interactive SC tool\n    (\nsysconfig(1M)\n) or by copying and editing XML files.\n\n\nYou can use variables in profiles to install systems with different\n    hostnames, IP addresses, etc.",
            "title": "System Configuration Profiles"
        },
        {
            "location": "/ai/ai_zones/index.html",
            "text": "In the beginning of this lab we also mentioned that it's possible to\ninstall zones together with the host installation. It's time to learn\nhow to do that.\n\n\nFirst of all, we have to configure the zone we are going to install.\nLet's create the simplest possible configuration with \nzonecfg(1M)\n.\n\n\nroot@solarislab:~# zonecfg -z ai-zone create\n\n\n\nThis command creates a non-global zone with all default parameters. Now\nwe have to export this configuration into a file:\n\n\nroot@solarislab:~# zonecfg -z ai-zone export -f ai-zone.cfg\n\n\n\nTake a look at the resulting file:\n\n\nroot@solarislab:~# cat ai-zone.cfg\ncreate -b\nset brand=solaris\nset zonepath=/system/zones/%{zonename}\nset autoboot=false\nset autoshutdown=shutdown\nset ip-type=exclusive\nadd anet\nset linkname=net0\nset lower-link=auto\nset configure-allowed-address=true\nset link-protection=mac-nospoof\nset mac-address=auto\nend\n\n\n\nNow we have to make it visible from the client we are about to install.\nThe best way to do it is to place it on a web server inside the local\nnetwork. Luckily, we have a running web server as part of the AI server\ninstallation. We just have to configure the directory where to store\nfiles on this web server. Let's use \nsvccfg(1M)\n for that:\n\n\nroot@solarislab:~# svccfg -s svc:/system/install/server:default listprop\n(that means the directory is not configured yet)\nroot@solarislab:~# mkdir /var/ai/ai-files\nroot@solarislab:~# chown -R webservd:webservd /var/ai/ai-files/\nroot@solarislab:~# svccfg -s svc:/system/install/server:default setprop all_services/webserver_files_dir=/var/ai/ai-files\nroot@solarislab:~# svccfg -s svc:/system/install/server:default listprop  all_services/webserver_files_dir\nall_services/webserver_files_dir astring     /var/ai/ai-files\nroot@solarislab:~# svcadm refresh svc:/system/install/server:default\n\n\n\nNow copy the zone configuration file to this place:\n\n\nroot@solarislab:~# cp ai-zone.cfg /var/ai/ai-files\n\n\n\nNow, which file should we use to tell AI server to install the zone?\nWhat would be your guess? Of course, it's the manifest file! Everything\nthat has to be \ninstalled\n is configured in the manifest! We will use\nthe same XML file where we configured the Solaris version.\n\n\nroot@solarislab:~# vi s11.2.11-manifest.xml\n\n\n\nScroll down to the end. Insert the following line (marked bold) after\nthe \nsoftware\n tag:\n\n\n\n      \nsoftware_data action=\ninstall\n\n        \nname\npkg:/entire@0.5.11-0.175.2.11.0.5.0\n/name\n\n        \nname\npkg:/group/system/solaris-small-server\n/name\n\n        \nname\npkg://solaris/system/management/puppet\n/name\n\n      \n/software_data\n\n    \n/software\n\n\n    \nconfiguration type=\nzone\n name=\nai-zone\n source=\nhttp://10.80.11.34:5555/files/ai-zone.cfg\n/\n\n  \n/ai_instance\n\n\n/auto_install\n\n\n\n\n\nAnd, of course, after editing the XML file, don't forget to update the\nmanifest:\n\n\nroot@solarislab:~# installadm update-manifest -n default-sparc -m s11.2.11 -f ./s11.2.11-manifest.xml\n\n\n\nNow everything is ready. Once again login into the client domain and\nperform the familiar procedure: halt the system (\ninit 0\n) to get to\nOpenBoot \nok\n prompt, boot it from AI server (\nboot net - install\n).\nWatch the messages on the screen. At some point you'll see the following\nlines:\n\n\n21:40:43    Zone name: ai-zone\n21:40:43       source: http://10.80.11.34:5555/files/ai-zone.cfg\n\n\n\nThat means everything is good. Wait until the installation finishes,\nreboot and login into the client system. First, let's check if the zone\nis installed:\n\n\nroot@ai-client1:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n\n\n\nHmm... Nothing. Where is our zone? We were told it's going to be\ninstalled at the first boot! Let's investigate further. Check the status\nof the following service (it's responsible for zone installation):\n\n\nroot@ai-client1:~# svcs zones-install\nSTATE          STIME    FMRI\noffline*       17:52:37 svc:/system/zones-install:default\nroot@ai-client1:~# svcs -x\nsvc:/system/zones-install:default (Zones auto-install)\n State: offline* transitioning to online since September 24, 2015 05:52:37 PM EDT\nReason: Start method is running.\n   See: http://support.oracle.com/msg/SMF-8000-C4\n   See: /var/svc/log/system-zones-install:default.log\nImpact: This service is not running.\n\n\n\nWell, it's not running, but it's \"transitioning to online\". That's a\ngood sign. Also we can take a look at its log file:\n\n\nroot@ai-client1:~# tail  /var/svc/log/system-zones-install:default.log\nzoneadm: ai-zone: No such zone configured\nzoneadm: ai-zone: No such zone configured\nConfiguring zone ai-zone\nInstalling zone ai-zone\nzoneadm -z ai-zone install  -m /usr/share/auto_install/manifest/zone_default.xml -c /usr/share/auto_install/sc_profiles/enable_sci.xml\nThe following ZFS file system(s) have been created:\n    rpool/VARSHARE/zones/ai-zone\nProgress being logged to /var/log/zones/zoneadm.20150924T215408Z.ai-zone.install\n       Image: Preparing at /system/zones/ai-zone/root.\n\n\n\nA-ha! Something is going on! It looks like our zone is being installed\nright now. Check the default \nzonepath\n location:\n\n\nroot@ai-client1:~# ls /system/zones\nai-zone\n\n\n\nIndeed! The file system for the zone is created already. Check the log\nand the service again:\n\n\nroot@ai-client1:~# tail  /var/svc/log/system-zones-install:default.log\n         Startup: Refreshing catalog 'solaris' ... Done\n        Planning: Solver setup ... Done\n        Planning: Running solver ... Done\n        Planning: Finding local manifests ... Done\n        Planning: Fetching manifests:   0/280  0% complete\n        Planning: Fetching manifests: 280/280  100% complete\n        Planning: Package planning ... Done\n        Planning: Merging actions ... Done\n        Planning: Checking for conflicting actions ... Done\nroot@ai-client1:~# tail  /var/svc/log/system-zones-install:default.log\n        Download:  4644/53146 items   32.9/374.3MB  8% complete (2.4M/s)\n        Download:  6193/53146 items   58.5/374.3MB  15% complete (3.7M/s)\n        Download:  7479/53146 items   94.5/374.3MB  25% complete (6.1M/s)\n        Download:  9006/53146 items  111.6/374.3MB  29% complete (5.4M/s)\n        Download: 10556/53146 items  125.9/374.3MB  33% complete (3.2M/s)\n        Download: 12752/53146 items  127.9/374.3MB  34% complete (1.6M/s)\n        Download: 14934/53146 items  142.4/374.3MB  38% complete (1.7M/s)\n        Download: 16263/53146 items  156.6/374.3MB  41% complete (2.8M/s)\n        Download: 17750/53146 items  175.2/374.3MB  46% complete (3.2M/s)\nroot@ai-client1:~# tail  /var/svc/log/system-zones-install:default.log\n         Actions: 14096/71080 actions (Installing new actions)\n         Actions: 17986/71080 actions (Installing new actions)\n         Actions: 21739/71080 actions (Installing new actions)\n         Actions: 25137/71080 actions (Installing new actions)\n         Actions: 28848/71080 actions (Installing new actions)\n         Actions: 32244/71080 actions (Installing new actions)\n         Actions: 35602/71080 actions (Installing new actions)\n         Actions: 39909/71080 actions (Installing new actions)\n         Actions: 43946/71080 actions (Installing new actions)\nroot@ai-client1:~# svcs zones-install\nSTATE          STIME    FMRI\noffline*       17:52:37 svc:/system/zones-install:default\nroot@ai-client1:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n   - ai-zone          installed   /system/zones/ai-zone        solaris    excl\nroot@ai-client1:~# svcs zones-install\nSTATE          STIME    FMRI\nonline         18:00:31 svc:/system/zones-install:default\n\n\n\nGreat! It is installed. Now you can boot it, login via the console\n(\nzlogin -C ai-zone\n), configure its profile and start using it. As you\nmight have guessed, you can configure the zone with manifests and system\nprofiles the same way we did it for AI clients. But we leave this\nexercise for your homework.",
            "title": "Zone Installation with AI"
        },
        {
            "location": "/ai/ai_os_clients/index.html",
            "text": "If you remember, first we have created an install service with the\nlatest Oracle Solaris version available in our repository, which was\nversion \n11.3\n. Then we changed the default to \n11.2.11\n, but what if we\nwanted to install a client with version \n11.3\n? We now have two install\nservices configured, one for \n11.2.11\n (which is the default) and another\nfor \n11.3\n. How can we specify that we want to install \n11.3\n instead of\n\n11.2\n? You may think we must use the same mechanism we used to install a\nspecific Solaris SRU, namely manifest. Well, you may try to create a\nmanifest XML file and specify \n11.3\n instead of \n11.2\n, but we can tell you\n(SPOILER ALERT!!!) that this won't work. The rule is that changing the\nmanifest is not enough if you want to install a different Solaris\n\nupdate\n (i.e. second digit as in \n11.1\n, \n11.2\n, \n11.3\n...). In this case you\nshould use a different \ninstall service\n. Luckily, we have one installed\nalready, its name is \nsolaris11_3-sparc\n.\n\n\nIf we want to specify the client that we want to install \n11.3\n on, we\nhave to use \ncreate-client\n command. With this command we can\nassociate specific clients with certain install services. Clients should\nbe specified using their MAC addresses. Login into the client system and\ndiscover its MAC address:\n\n\nroot@ai-client1:~# dladm show-phys -m\nLINK                SLOT     ADDRESS            INUSE CLIENT\nnet0                primary  0:14:4f:f8:5:f6    yes   net0\n                    1        0:14:4f:f8:3:5c    no    --\n                    2        0:14:4f:f8:ee:26   no    --\n                    3        0:14:4f:fa:c4:6f   no    --\n\n\n\nThe primary MAC address is \n0:14:4f:f8:5:f6\n. Now switch to the AI server\nwindow and use this address to associate it with the install service\n\nsolaris11_3-sparc\n.\n\n\nroot@solarislab:~# installadm create-client -e 0:14:4f:f8:5:f6 -n solaris11_3-sparc\nCreated Client: '00:14:4F:F8:05:F6'\nroot@solarislab:~# installadm list -c\nService Name      Client Address    Arch  Secure Custom Args Custom Grub\n------------      --------------    ----  ------ ----------- -----------\nsolaris11_3-sparc 00:14:4F:F8:05:F6 sparc no     no          no\n\n\n\nAnd now, again the familiar procedure of shutting down the client and\ninstalling it from network.\n\n\nWhen this newly installed system reboots, you will notice that you get\nthe system configuration screen again and you have to specify hostname,\nIP address, and other parameters again. Why? We have configured the\nsystem profile already to automate this process, why are we seeing this\nagain? Take a look at the profiles list:\n\n\nroot@solarislab:~# installadm list -p\nService Name  Profile Name Environment Criteria\n------------  ------------ ----------- --------\ndefault-sparc sc-template  system      none\n\n\n\nWe have created the profile \nsc-template\n and associated it with the\n\ndefault-sparc\n install service. And \ndefault-sparc\n install service\nis an alias of the \ns11.2.11-sparc\n service. So, profiles are\nassociated with install services. As soon as we are using a different\ninstall service, we are using it's default profile, not the one we\ncreated earlier. You can use the XML file \nsc_template.xml\n and create a\nnew profile with 11.3 service\nyou know how to do this already.\n\n\nOne more question: what if we don't have Solaris installed on the\nclient? Like if we just have created a guest logical domain and it's\ncompletely empty? How can we figure out its MAC address?\n\n\nWell, it takes a couple of steps. Halt the client system and get to\nOpenBoot prompt. Enter the following command:\n\n\n{0} ok show-devs\n/cpu@7\n/cpu@6\n/cpu@5\n/cpu@4\n/cpu@3\n/cpu@2\n/cpu@1\n/cpu@0\n/virtual-devices@100\n/iscsi-hba\n/virtual-memory\n/memory@m0,80000000\n/aliases\n/options\n/openprom\n/chosen\n/packages\n/virtual-devices@100/channel-devices@200\n/virtual-devices@100/console@1\n/virtual-devices@100/random-number-generator@e\n/virtual-devices@100/flashprom@0\n/virtual-devices@100/channel-devices@200/virtual-domain-service@0\n/virtual-devices@100/channel-devices@200/pciv-communication@0\n/virtual-devices@100/channel-devices@200/disk@0\n/virtual-devices@100/channel-devices@200/network@0\n/iscsi-hba/disk\n/openprom/client-services\n/packages/obp-tftp\n/packages/kbd-translator\n/packages/SUNW,asr\n/packages/dropins\n/packages/terminal-emulator\n/packages/disk-label\n/packages/deblocker\n/packages/SUNW,builtin-drivers\n{0} ok\n\n\n\nFind the line that represents the network interface. In our case it's\n\n/virtual-devices@100/channel-devices@200/network@0\n. Then 'change\ndirectory' (\ncd\n) into it and type \n.properties\n:\n\n\n{0} ok cd /virtual-devices@100/channel-devices@200/network@0\n{0} ok .properties\nlocal-mac-address        00 14 4f f8 05 f6\nmax-frame-size           00004000\naddress-bits             00000030\nreg                      00000000\ncompatible               SUNW,sun4v-network\ndevice_type              network\nname                     network\n{0} ok\n\n\n\nGot it! Here is our client's MAC address: \n00:14:4f:f8:05:f6\n. You can use\nit with \ncreate-client\n command now and install the system from scratch.",
            "title": "Different OS Versions on Clients"
        },
        {
            "location": "/ai/ai_uar/index.html",
            "text": "One of the very important additions to Oracle Solaris in version \n11.2\n\nwas Unified Archives. It gives a lot of options on systems backup and\nrestore, systems cloning including various use cases: with or without\nzones, from physical to virtual and back, etc. In the following exercise\nwe will use a UAR (Unified ARchive) file with Oracle Solaris\ndistribution which is normally available for download from \noracle.com\n.\nAlternatively, you can create you own archive from the existing system\nusing \ninstalladm(1M)\n. Unified archive preserves everything that was\ninstalled in the system, including zones, so it's a perfect way of\ncloning systems with applications.\n\n\nThe plan is pretty simple:\n\n\n\n\nCopy the UAR file to the AI webserver's files location (it should be\n    visible by the client)\n\n\nCopy and edit the \ndefault_archive.xml\n manifest file to show the\n    archive's location (in URL format)\n\n\nInstall the manifest into one of the install services\n\n\nDefine the client criteria to install from this archive (or make\n    it default)\n\n\n\n\nWe have several UAR files in \n/share1/uar\n:\n\n\nroot@solarislab:~# ls -lh /share1/uar\ntotal 15405066\n-rw-r--r--   1 nobody   nobody      4.7G Jun 29  2014 sol-11_2-openstack-sparc.uar\n-rw-r--r--   1 nobody   nobody      1.3G Jul 18  2014 sol-11_2-sparc.uar\n-rw-r--r--   1 nobody   nobody      887M Sep 28 09:57 sol-11_3_1_3_0-sparc.uar\n-rw-r--r--   1 nobody   nobody      916M Jul  8 16:01 sol-11-3-beta.uar\n\n\n\nRemember, we have configured the AI web server directory to store files,\nit's \n/var/ai/ai-files/\n. Copy one of the UAR files to that directory:\n\n\nroot@solarislab:~# cp /share1/uar/sol-11_2-sparc.uar /var/ai/ai-files\nroot@solarislab:~# chown webservd:webservd /var/ai/ai-files/sol-11_2-sparc.uar\n\n\n\nNow we have to edit the manifest XML. Remember, the local web server\ndirectory is \n/var/ai/ai-files\n, but from outside it looks like\n\nhttp://10.80.11.34:5555/files\n. Copy the default archive manifest file\nto the current directory and edit it.\n\n\nroot@solarislab:~# cp /usr/share/auto_install/manifest/default_archive.xml ./\nroot@solarislab:~# chmod 644 default_archive.xml\nroot@solarislab:~# vi default_archive.xml\n\n\n\nFind the line with UAR location: \nfile uri=\nfile:///.cdrom/archive.uar\n/\n and replace is with the following:\n\n\n        \nfile uri=\nhttp://10.80.11.34:5555/files/sol-11_2-sparc.uar\n/\n\n\n\n\nNow add this manifest to the existing install service:\n\n\nroot@solarislab:~# installadm create-manifest -n default-sparc -m archive -c hostname=ai-client1 -f ./default_archive.xml\nCreated Manifest: 'archive'\nroot@solarislab:~# installadm list -m\nService Name             Manifest Name Type    Status   Criteria\n------------             ------------- ----    ------   --------\ndefault-sparc            archive       xml     active   hostname = ai-client1\n                         s11.2.11      xml     default  none\n                         orig_default  derived inactive none\nsolaris11_2_11_5_0-sparc orig_default  derived default  none\nsolaris11_3-sparc        orig_default  derived default  none\n\n\n\nIn the command above we also specified the client criteria that defines\nwhich client(s) will be installed with this manifest. We use the same\nclient system and if you remember, last time we specified its hostname\nin \nnetwork-boot-parameters\n as \nai-client1\n. You may also remember\nthat we specified its MAC address with \ncreate-client\n command and\nassigned it to use Solaris \n11.3\n install service. This time we want to\ninstall from \ndefault-sparc\n install service (as soon as we have\ninstalled the manifest into it) so we have to remove this client\nassociation.\n\n\nroot@solarislab:~# installadm list -c\nService Name      Client Address    Arch  Secure Custom Args Custom Grub\n------------      --------------    ----  ------ ----------- -----------\nsolaris11_3-sparc 00:14:4F:F8:05:F6 sparc no     no          no\nroot@solarislab:~# installadm delete-client -e 00:14:4F:F8:05:F6\nDeleted Client: '00:14:4F:F8:05:F6'\nroot@solarislab:~# installadm list -c\nThere are no clients configured for local services.\n\n\n\nEverything is ready. Now we can try to install the client and check\nwhich Solaris version was installed. If everything goes well it should\nbe \n11.2.0\n (a.k.a. General Availability release).\n\n\nai-client1 console login: lab\nPassword: oracle1\nlab@ai-client1:~$ pkg list entire\nNAME (PUBLISHER)                                  VERSION                    IFO\nentire                                            0.5.11-0.175.2.0.0.42.0    i--\n\n\n\nSuccess! Did you notice that it was also faster that installing from a\nrepository? That's because we are sending a long stream of data instead\nof installing the system package by package. Again, let's wrap up what\nwe have learned.\n\n\n\n\nWe can install systems not only from the repository, but also from\n    Unified Archives, essentially cloning the servers.\n\n\nInstallation from UAR usually goes faster than from\n    package repository.\n\n\nDon't forget to specify criteria that define which clients will\n    be installed. You can specify it either during manifest\n    creation/update or as a separate command.",
            "title": "Installing from Unified Archives"
        },
        {
            "location": "/ai/ai_summary/index.html",
            "text": "We have completed this Automated Installer Hands-on Lab. Let's wrap up\nwhat we did today:\n\n\n\n\nWe installed our AI client system \nseven times\n (at least)!\n\n\nWe learned how to perform simple default installations\n\n\nWe learned how to define which Solaris version will be installed on\n    which clients\n\n\nWe configured packages to be installed in addition to the standard\n    group\n\n\nWe used profiles and profile templates to configure system's\n    identity including hostnames, IP addresses and users\n\n\nWe used Unified Archives to clone systems and found out that in most\n    times it's a faster and more convenient way to install systems\n\n\nWe automatically installed zones inside the client system\n\n\n\n\nThere are other AI features that we didn't cover in this lab. You may\nwant to add some other capabilities to your AI server, such as: first\nboot script, x86 architecture service, derived manifests and others. You\nmay want to try AI Manifest Wizard that helps you to create XML\nmanifests using web interface. Also it might be a good idea to configure\na special role for AI administration (using Solaris Role-Based Access\nControl) to avoid using \nroot\n role for this task.\n\n\nYou will find more information about this in the official Oracle Solaris\ndocumentation here: \nhttp://docs.oracle.com/cd/E36784_01/pdf/E36800.pdf\n\nor here:\n\nhttp://docs.oracle.com/cd/E36784_01/html/E36800/useaipart.html#scrolltoc\n\n(for Oracle Solaris 11.2) and also on Oracle Technology Network (OTN):\n\nhttp://www.oracle.com/technetwork/server-storage/solaris11/technologies/lifecycle-management-2237945.html",
            "title": "AI Lab Summary"
        },
        {
            "location": "/kz/kz/index.html",
            "text": "There is a new virtualization technology called \"Kernel Zones\" which\nbecame available in Oracle Solaris 11.2. In this lab we will explore its\nnew features and compare it to other Oracle virtualization technologies.\n\n\n\n\n60 seconds of theory\n\n\nSolaris Zones were first announced in 2005 in\nSolaris 10 and then became a popular way of managing application\nworkloads. They are very flexible, extremely lightweight and provide\nvery good isolation of resources. But Solaris Zones require that all\napplications in every zone run under the same version of OS Solaris,\nwhich runs in the global zone. In some situations it might become an\nunnecessary restriction. To answer this limitation, Solaris engineers\ncreated a new virtualization technology which allows you to run zones\nwith different versions of Solaris under one global zone. This\ntechnology is now called \"kernel zones\" because they run their own\nkernels. We continue calling the earlier Solaris Zones technology\n\"non-global\" zones.\n\n\n\n\nIn this lab we will compare various features of kernel zones and\nnon-global zones side-by-side and analyze how and where we can use them.\nPlease pay special attention to the command prompts: they indicate where\nwe perform the operation. Our global zone is called \nsolarislab\n;\nkernel zones will be called \nkzone1\n, \nkzone2\n, etc.; traditional\nnon-global zones are called \nzone1\n, \nzone2\n, etc.",
            "title": "Kernel Zones Intro"
        },
        {
            "location": "/kz/kz_basic/index.html",
            "text": "Zone creation\n\n\nNon-global zone:\n\n\nroot@solarislab:~# zonecfg -z zone1 create\n\n\n\nKernel zone:\n\n\nroot@solarislab:~# zonecfg -z kzone1 create -t SYSsolaris-kz\n\n\n\nThat's it! We just use a special \nkernel zone\n template to create a\nzone. Actually, in the first command we use the default \nSYSsolaris\n\nprofile as soon as we omitted that parameter. If you are curious, take a\nlook at the \n/etc/zones\n directory to compare different profiles. You\nwill also find that there is a special profile for Solaris 10 zones.\n\n\nDefault configuration\n\n\nNon-global zone:\n\n\nroot@solarislab:/etc/zones# zonecfg -z zone1 info\nzonename: zone1\nzonepath: /system/zones/zone1\nbrand: solaris\nautoboot: false\nautoshutdown: shutdown\nbootargs:\nfile-mac-profile:\npool:\nlimitpriv:\nscheduling-class:\nip-type: exclusive\nhostid:\ntenant:\nfs-allowed:\nanet:\n        linkname: net0\n        lower-link: auto\n        allowed-address not specified\n        configure-allowed-address: true\n        defrouter not specified\n        allowed-dhcp-cids not specified\n        link-protection: mac-nospoof\n        mac-address: auto\n        mac-prefix not specified\n        mac-slot not specified\n        vlan-id not specified\n        priority not specified\n        rxrings not specified\n        txrings not specified\n        mtu not specified\n        maxbw not specified\n        bwshare not specified\n        rxfanout not specified\n        vsi-typeid not specified\n        vsi-vers not specified\n        vsi-mgrid not specified\n        etsbw-lcl not specified\n        cos not specified\n        pkey not specified\n        linkmode not specified\n        evs not specified\n        vport not specified\n\n\n\nKernel zone:\n\n\nroot@solarislab:/etc/zones# zonecfg -z kzone1 info\nzonename: kzone1\nbrand: solaris-kz\nautoboot: false\nautoshutdown: shutdown\nbootargs:\npool:\nscheduling-class:\nhostid: 0x2e7d2173\ntenant:\nanet:\n        lower-link: auto\n        allowed-address not specified\n        configure-allowed-address: true\n        defrouter not specified\n        allowed-dhcp-cids not specified\n        link-protection: mac-nospoof\n        mac-address: auto\n        mac-prefix not specified\n        mac-slot not specified\n        vlan-id not specified\n        priority not specified\n        rxrings not specified\n        txrings not specified\n        mtu not specified\n        maxbw not specified\n        bwshare not specified\n        rxfanout not specified\n        vsi-typeid not specified\n        vsi-vers not specified\n        vsi-mgrid not specified\n        etsbw-lcl not specified\n        cos not specified\n        evs not specified\n        vport not specified\n        iov: off\n        lro: auto\n        id: 0\ndevice:\n        match not specified\n        storage: dev:/dev/zvol/dsk/rpool/VARSHARE/zones/kzone1/disk0\n        id: 0\n        bootpri: 0\ncapped-memory:\n        physical: 4G\n\n\n\nHere we see more difference between the two. First, we notice that the\nkernel zone has a different \nbrand\n. It's a signal to the Solaris kernel to\ntreat this zone differently from the default \nsolaris\n brand. Also we\nsee that the \nzonepath\n parameter disappeared in the kernel zone. Where are\nwe going to store the zone's root? Scroll down a little bit and find the\n\ndevice:\n section. What do you see? Now you see that kernel zones keep\ntheir root directories not in a ZFS \nfile system\n, but rather in a ZFS\n\nvolume\n which looks like a block device. By default, Solaris creates a\n16 Gigabyte volume for that in the \nrpool\n ZFS pool. Of course, you\ncan change the size of the volume during installation.\n\n\nAlso take a look at the \nzoneadm list -cv\n output:\n\n\nroot@solarislab:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n   - zone1            configured  /system/zones/zone1          solaris    excl\n   - kzone1           configured  -                            solaris-kz excl\n\n\n\nAgain, you see that we don't have a file system path specified for the\nkernel zone.\n\n\nZone installation\n\n\nNow it's time to install both zones. We use exactly the same command for\nboth kernel zones and non-global zones. Start with the kernel zone:\n\n\nroot@solarislab:~# zoneadm -z kzone1 install\nProgress being logged to /var/log/zones/zoneadm.20150615T163032Z.kzone1.install\npkg cache: Using /var/pkg/publisher.\n Install Log: /system/volatile/install.17385/install_log\n AI Manifest: /tmp/zoneadm16918.sKa4xH/devel-ai-manifest.xml\n  SC Profile: /usr/share/auto_install/sc_profiles/enable_sci.xml\nInstallation: Starting ...\n\n        Creating IPS image\n        Installing packages from:\n            solaris\n                origin:  http://ipkg.us.oracle.com/solaris11/support/\n        The following licenses have been accepted and not displayed.\n        Please review the licenses for the following packages post-install:\n          consolidation/osnet/osnet-incorporation\n        Package licenses may be viewed using the command:\n          pkg info --license \n\nDOWNLOAD                                PKGS         FILES    XFER (MB)   SPEED\nCompleted                            451/451   63995/63995  598.3/598.3  2.2M/s\n\nPHASE                                          ITEMS\nInstalling new actions                   87551/87551\nUpdating package state database                 Done\nUpdating package cache                           0/0\nUpdating image state                            Done\nCreating fast lookup database                   Done\nInstallation: Succeeded\n        Done: Installation completed in 716.730 seconds.\n\n\n\nWhat if we want to install a different version of Solaris into the\nkernel zone? In the previous example we used the so called \"direct\ninstallation\" method. In other words, we used the same package\nrepository that is configured in the global zone. To install a different\nversion of Solaris, we have to use a separate installation media. For\nexample, we can use Oracle Solaris 11.3 beta DVD ISO for that:\n\n\nroot@solarislab:~# zoneadm -z kzone1 install -b /share1/ISOs/sol-11_3-25-text-sparc.iso\n\n\n\nWhat do you think is going to happen in this case? You guessed it right:\nthe kernel zone will boot from this DVD and the usual installation\nprocess will begin. You will go through the familiar questions:\nhostname, IP address, time zone, root password, etc. The process is no\ndifferent from bare metal or logical domain Solaris installation.\n\n\nAnd now install the non-global zone:\n\n\nroot@solarislab:~# zoneadm -z zone1 install\nThe following ZFS file system(s) have been created:\n    rpool/VARSHARE/zones/zone1\nProgress being logged to /var/log/zones/zoneadm.20150615T165123Z.zone1.install\n       Image: Preparing at /system/zones/zone1/root.\n\n Install Log: /system/volatile/install.18535/install_log\n AI Manifest: /tmp/manifest.xml.Z.aylK\n  SC Profile: /usr/share/auto_install/sc_profiles/enable_sci.xml\n    Zonename: zone1\nInstallation: Starting ...\n\n        Creating IPS image\nStartup linked: 1/1 done\n        Installing packages from:\n            solaris\n                origin:  http://ipkg.us.oracle.com/solaris11/support/\nDOWNLOAD                                PKGS         FILES    XFER (MB)   SPEED\nCompleted                            280/280   53151/53151  374.3/374.3  3.4M/s\n\nPHASE                                          ITEMS\nInstalling new actions                   71074/71074\nUpdating package state database                 Done\nUpdating package cache                           0/0\nUpdating image state                            Done\nCreating fast lookup database                   Done\nUpdating package cache                           1/1\nInstallation: Succeeded\n\n        Note: Man pages can be obtained by installing pkg:/system/manual\n\n done.\n\n        Done: Installation completed in 485.377 seconds.\n\n\n  Next Steps: Boot the zone, then log into the zone console (zlogin -C)\n\n              to complete the configuration process.\n\nLog saved in non-global zone as /system/zones/zone1/root/var/log/zones/zoneadm.20150615T165123Z.zone1.install\n\n\n\nLet's look at what's different between these two listings. First, in the\nkernel zone we install more packages (451 vs. 280). Also, if you look at\nthe progress line during the installation you may notice that packages\nlike hardware drivers are being installed--that's a big difference from\nnon-global zones.\n\n\nNow we can boot both zones and complete the installation by configuring\nhost names, root passwords, time zones, etc. (in case of installing the\nkernel zone from media, we have done that already). There is almost no\ndifference in these processes, they are all very familiar. To do that\nlogin into each zone's console and fill the screens that follow.\n\n\nroot@solarislab:~# zlogin -C zone1\n\n\n\nand then:\n\n\n\n\nroot@solarislab:~# zlogin -C kzone1\n\n\n\nAll the necessary information will be provided by your instructor. Make\nsure you choose \nManual\n network configuration, not \nAutomatic\n, which\nis the default. You will need: IP address, netmask, default router\naddress. Set the time zone and root password to the same values as in\nthe global zone.",
            "title": "Basic Configuration and Installation"
        },
        {
            "location": "/kz/kz_basic/index.html#zone-creation",
            "text": "Non-global zone:  root@solarislab:~# zonecfg -z zone1 create  Kernel zone:  root@solarislab:~# zonecfg -z kzone1 create -t SYSsolaris-kz  That's it! We just use a special  kernel zone  template to create a\nzone. Actually, in the first command we use the default  SYSsolaris \nprofile as soon as we omitted that parameter. If you are curious, take a\nlook at the  /etc/zones  directory to compare different profiles. You\nwill also find that there is a special profile for Solaris 10 zones.",
            "title": "Zone creation"
        },
        {
            "location": "/kz/kz_basic/index.html#default-configuration",
            "text": "Non-global zone:  root@solarislab:/etc/zones# zonecfg -z zone1 info\nzonename: zone1\nzonepath: /system/zones/zone1\nbrand: solaris\nautoboot: false\nautoshutdown: shutdown\nbootargs:\nfile-mac-profile:\npool:\nlimitpriv:\nscheduling-class:\nip-type: exclusive\nhostid:\ntenant:\nfs-allowed:\nanet:\n        linkname: net0\n        lower-link: auto\n        allowed-address not specified\n        configure-allowed-address: true\n        defrouter not specified\n        allowed-dhcp-cids not specified\n        link-protection: mac-nospoof\n        mac-address: auto\n        mac-prefix not specified\n        mac-slot not specified\n        vlan-id not specified\n        priority not specified\n        rxrings not specified\n        txrings not specified\n        mtu not specified\n        maxbw not specified\n        bwshare not specified\n        rxfanout not specified\n        vsi-typeid not specified\n        vsi-vers not specified\n        vsi-mgrid not specified\n        etsbw-lcl not specified\n        cos not specified\n        pkey not specified\n        linkmode not specified\n        evs not specified\n        vport not specified  Kernel zone:  root@solarislab:/etc/zones# zonecfg -z kzone1 info\nzonename: kzone1\nbrand: solaris-kz\nautoboot: false\nautoshutdown: shutdown\nbootargs:\npool:\nscheduling-class:\nhostid: 0x2e7d2173\ntenant:\nanet:\n        lower-link: auto\n        allowed-address not specified\n        configure-allowed-address: true\n        defrouter not specified\n        allowed-dhcp-cids not specified\n        link-protection: mac-nospoof\n        mac-address: auto\n        mac-prefix not specified\n        mac-slot not specified\n        vlan-id not specified\n        priority not specified\n        rxrings not specified\n        txrings not specified\n        mtu not specified\n        maxbw not specified\n        bwshare not specified\n        rxfanout not specified\n        vsi-typeid not specified\n        vsi-vers not specified\n        vsi-mgrid not specified\n        etsbw-lcl not specified\n        cos not specified\n        evs not specified\n        vport not specified\n        iov: off\n        lro: auto\n        id: 0\ndevice:\n        match not specified\n        storage: dev:/dev/zvol/dsk/rpool/VARSHARE/zones/kzone1/disk0\n        id: 0\n        bootpri: 0\ncapped-memory:\n        physical: 4G  Here we see more difference between the two. First, we notice that the\nkernel zone has a different  brand . It's a signal to the Solaris kernel to\ntreat this zone differently from the default  solaris  brand. Also we\nsee that the  zonepath  parameter disappeared in the kernel zone. Where are\nwe going to store the zone's root? Scroll down a little bit and find the device:  section. What do you see? Now you see that kernel zones keep\ntheir root directories not in a ZFS  file system , but rather in a ZFS volume  which looks like a block device. By default, Solaris creates a\n16 Gigabyte volume for that in the  rpool  ZFS pool. Of course, you\ncan change the size of the volume during installation.  Also take a look at the  zoneadm list -cv  output:  root@solarislab:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n   - zone1            configured  /system/zones/zone1          solaris    excl\n   - kzone1           configured  -                            solaris-kz excl  Again, you see that we don't have a file system path specified for the\nkernel zone.",
            "title": "Default configuration"
        },
        {
            "location": "/kz/kz_basic/index.html#zone-installation",
            "text": "Now it's time to install both zones. We use exactly the same command for\nboth kernel zones and non-global zones. Start with the kernel zone:  root@solarislab:~# zoneadm -z kzone1 install\nProgress being logged to /var/log/zones/zoneadm.20150615T163032Z.kzone1.install\npkg cache: Using /var/pkg/publisher.\n Install Log: /system/volatile/install.17385/install_log\n AI Manifest: /tmp/zoneadm16918.sKa4xH/devel-ai-manifest.xml\n  SC Profile: /usr/share/auto_install/sc_profiles/enable_sci.xml\nInstallation: Starting ...\n\n        Creating IPS image\n        Installing packages from:\n            solaris\n                origin:  http://ipkg.us.oracle.com/solaris11/support/\n        The following licenses have been accepted and not displayed.\n        Please review the licenses for the following packages post-install:\n          consolidation/osnet/osnet-incorporation\n        Package licenses may be viewed using the command:\n          pkg info --license \n\nDOWNLOAD                                PKGS         FILES    XFER (MB)   SPEED\nCompleted                            451/451   63995/63995  598.3/598.3  2.2M/s\n\nPHASE                                          ITEMS\nInstalling new actions                   87551/87551\nUpdating package state database                 Done\nUpdating package cache                           0/0\nUpdating image state                            Done\nCreating fast lookup database                   Done\nInstallation: Succeeded\n        Done: Installation completed in 716.730 seconds.  What if we want to install a different version of Solaris into the\nkernel zone? In the previous example we used the so called \"direct\ninstallation\" method. In other words, we used the same package\nrepository that is configured in the global zone. To install a different\nversion of Solaris, we have to use a separate installation media. For\nexample, we can use Oracle Solaris 11.3 beta DVD ISO for that:  root@solarislab:~# zoneadm -z kzone1 install -b /share1/ISOs/sol-11_3-25-text-sparc.iso  What do you think is going to happen in this case? You guessed it right:\nthe kernel zone will boot from this DVD and the usual installation\nprocess will begin. You will go through the familiar questions:\nhostname, IP address, time zone, root password, etc. The process is no\ndifferent from bare metal or logical domain Solaris installation.  And now install the non-global zone:  root@solarislab:~# zoneadm -z zone1 install\nThe following ZFS file system(s) have been created:\n    rpool/VARSHARE/zones/zone1\nProgress being logged to /var/log/zones/zoneadm.20150615T165123Z.zone1.install\n       Image: Preparing at /system/zones/zone1/root.\n\n Install Log: /system/volatile/install.18535/install_log\n AI Manifest: /tmp/manifest.xml.Z.aylK\n  SC Profile: /usr/share/auto_install/sc_profiles/enable_sci.xml\n    Zonename: zone1\nInstallation: Starting ...\n\n        Creating IPS image\nStartup linked: 1/1 done\n        Installing packages from:\n            solaris\n                origin:  http://ipkg.us.oracle.com/solaris11/support/\nDOWNLOAD                                PKGS         FILES    XFER (MB)   SPEED\nCompleted                            280/280   53151/53151  374.3/374.3  3.4M/s\n\nPHASE                                          ITEMS\nInstalling new actions                   71074/71074\nUpdating package state database                 Done\nUpdating package cache                           0/0\nUpdating image state                            Done\nCreating fast lookup database                   Done\nUpdating package cache                           1/1\nInstallation: Succeeded\n\n        Note: Man pages can be obtained by installing pkg:/system/manual\n\n done.\n\n        Done: Installation completed in 485.377 seconds.\n\n\n  Next Steps: Boot the zone, then log into the zone console (zlogin -C)\n\n              to complete the configuration process.\n\nLog saved in non-global zone as /system/zones/zone1/root/var/log/zones/zoneadm.20150615T165123Z.zone1.install  Let's look at what's different between these two listings. First, in the\nkernel zone we install more packages (451 vs. 280). Also, if you look at\nthe progress line during the installation you may notice that packages\nlike hardware drivers are being installed--that's a big difference from\nnon-global zones.  Now we can boot both zones and complete the installation by configuring\nhost names, root passwords, time zones, etc. (in case of installing the\nkernel zone from media, we have done that already). There is almost no\ndifference in these processes, they are all very familiar. To do that\nlogin into each zone's console and fill the screens that follow.  root@solarislab:~# zlogin -C zone1  and then:  root@solarislab:~# zlogin -C kzone1  All the necessary information will be provided by your instructor. Make\nsure you choose  Manual  network configuration, not  Automatic , which\nis the default. You will need: IP address, netmask, default router\naddress. Set the time zone and root password to the same values as in\nthe global zone.",
            "title": "Zone installation"
        },
        {
            "location": "/kz/inside_outside/index.html",
            "text": "Now as we have installed and configured our zones, let's login into both\nand see how they look from inside. Remember: in the listings below the\nkernel zone's hostname is \nkzone1\n, non-global zone's one is\n\nzone1\n.\n\n\nProcesses\n\n\nWe start with some basic process-related commands. For example, how many\nprocesses are running just after the zone is booted?\n\n\nroot@kzone1:~# ps -ef | wc -l\n      56\n\n\n\nroot@zone1:~# ps -ef | wc -l\n      36\n\n\n\nYou see, there some extra processes running in the kernel zone. Let's\nsort them by PID and look at the first 20 of them:\n\n\nroot@kzone1:~# ps -ef | sort -k 2 | head -20\n    root     0     0   0 15:16:13 ?           0:00 sched\n    root     1     0   0 15:16:13 ?           0:00 /usr/sbin/init\n    root     2     0   0 15:16:13 ?           0:00 pageout\n    root     3     0   0 15:16:13 ?           0:01 fsflush\n    root     5     0   0 15:16:11 ?           0:03 zpool-rpool\n    root     6     0   0 15:16:13 ?           0:00 kmem_task\n    root     7     0   0 15:16:13 ?           0:00 intrd\n    root     8     0   0 15:16:13 ?           0:00 vmtasks\n    root     9     0   0 15:16:13 ?           0:00 postwaittq\n    root    13     1   0 15:16:16 ?           0:10 /lib/svc/bin/svc.startd\n    root    15     1   0 15:16:16 ?           2:03 /lib/svc/bin/svc.configd\n  netcfg    50     1   0 15:17:45 ?           0:00 /lib/inet/netcfgd\n   dladm    65     1   0 15:17:54 ?           0:00 /usr/sbin/dlmgmtd\n  netadm    84     1   0 15:18:06 ?           0:00 /lib/inet/ipmgmtd\n    root    97     1   0 15:18:08 ?           0:00 /lib/inet/in.mpathd\n    root   122     1   0 15:18:12 ?           0:00 /usr/lib/pfexecd\n    root   165     1   0 15:18:25 ?           0:00 /usr/lib/sysevent/syseventd\n  daemon   202     1   0 15:18:26 ?           0:00 /usr/lib/utmpd\n    root   217     1   0 15:18:27 ?           0:00 /usr/lib/zones/zonestatd\n    root   221     1   0 15:18:27 ?           0:00 /usr/lib/rad/rad -sp\n\n\n\nroot@zone1:~# ps -ef | sort -k 2 | head -20\n     UID   PID  PPID   C    STIME TTY         TIME CMD\n    root 21253 21253   0 14:28:07 ?           0:00 zsched\n    root 21971 21253   0 14:28:16 ?           0:00 /usr/sbin/init\n    root 22151 21253   0 14:28:16 ?           0:04 /lib/svc/bin/svc.startd\n    root 22153 21253   0 14:28:16 ?           1:06 /lib/svc/bin/svc.configd\n  netcfg 22186 21253   0 14:29:12 ?           0:00 /lib/inet/netcfgd\n  daemon 22229 21253   0 14:29:15 ?           0:00 /lib/crypto/kcfd\n    root 22236 21253   0 14:29:16 ?           0:00 /usr/lib/pfexecd\n  netadm 22252 21253   0 14:29:17 ?           0:00 /lib/inet/ipmgmtd\n    root 22266 21253   0 14:29:17 ?           0:00 /lib/inet/in.mpathd\n  daemon 22272 21253   0 14:29:18 ?           0:00 /usr/lib/utmpd\n    root 22287 21253   0 14:29:18 ?           0:00 /usr/lib/rad/rad -sp\n    root 22333 21253   0 14:29:18 ?           0:00 /usr/lib/dbus-daemon --system\n  netadm 22771 21253   0 14:30:43 ?           0:00 /lib/inet/nwamd\n    root 22892 21253   0 14:30:46 ?           0:00 /usr/lib/zones/zoneproxy-client -s localhost:1008\n    root 22899 21253   0 14:30:47 ?           0:00 /usr/sbin/nscd\n    root 22955 21253   0 14:30:49 ?           0:00 /usr/sbin/cron\n  daemon 23010 21253   0 14:30:51 ?           0:00 /usr/sbin/rpcbind\n    root 23040 21253   0 14:30:51 ?           0:00 /usr/lib/inet/in.ndpd\n    root 23048 21253   0 14:30:52 ?           0:00 /usr/lib/autofs/automountd\n\n\n\nNow you see the big difference between kernel zones and non-global\nzones. In non-global zones all the processes are part of the global\nzone's process space, so their PIDs start with some big number. In\nkernel zones they start with \n0\n and the list is not much different from\nthe process list in the global zone.\n\n\nNow let's look at them from the global zone's perspective. Use the \n-Z\n\nswitch for the \nps\n command to see zone's names next to the processes.\n\n\nroot@solarislab:~# ps -efZ\n    ZONE      UID   PID  PPID   C    STIME TTY         TIME CMD\n  global     root     0     0   0   Jul 29 ?           0:10 sched\n  global     root     5     0   0   Jul 29 ?           7:26 zpool-rpool\n  global     root     6     0   0   Jul 29 ?           0:31 kmem_task\n  global     root     1     0   0   Jul 29 ?           0:01 /usr/sbin/init\n  global     root     2     0   0   Jul 29 ?           0:00 pageout\n  global     root     3     0   0   Jul 29 ?          11:32 fsflush\n  global     root     7     0   0   Jul 29 ?           0:12 intrd\n  global     root     8     0   0   Jul 29 ?           1:02 vmtasks\n. . . (skipped) . . .\n   zone1     root 21253     1   0 12:28:07 ?           0:00 zsched\n  global     root  1214     1   0   Jul 29 ?           0:03 /usr/lib/ep/eptelemon\n  global     root  1116     1   0   Jul 29 ?           0:21 /usr/lib/ep/epdetector dcdc cpu0\n  kzone1     root 19817     1   0 11:55:11 ?           0:00 zsched\n. . . (skipped) . . .\n  kzone1     root 19995 19817   0 11:55:12 ?           6:23 /usr/lib/kzhost\n  global  pkg5srv 20737     1   0 12:19:48 ?           0:16 /usr/apache2/2.2/bin/htcacheclean -d20160 -i -l 2048M -n -p /var/cache/pkg/sysr\n  global  pkg5srv 20742 20739   0 12:19:49 ?           0:21 /usr/apache2/2.2/bin/64/httpd.worker -f /system/volatile/pkg/sysrepo/sysrepo_ht\n  global  pkg5srv 20743 20739   0 12:19:49 ?           0:22 /usr/apache2/2.2/bin/64/httpd.worker -f /system/volatile/pkg/sysrepo/sysrepo_ht\n   zone1     root 22151     1   0 12:28:16 ?           0:04 /lib/svc/bin/svc.startd\n   zone1   netcfg 22186     1   0 12:29:12 ?           0:00 /lib/inet/netcfgd\n  global  pkg5srv 20746     1   0 12:19:50 ?           0:00 /usr/lib/zones/zoneproxyd\n   zone1     root 22153     1   0 12:28:16 ?           1:06 /lib/svc/bin/svc.configd\n  global     root 21061     1   0 12:28:04 ?           0:01 zoneadmd -z zone1\n   zone1     root 21971 21253   0 12:28:16 ?           0:00 /usr/sbin/init\n   zone1     root 22266     1   0 12:29:17 ?           0:00 /lib/inet/in.mpathd\n. . . (skipped) . . .\n  global     root 23168 14852   0 12:32:01 pts/2       0:00 ps -efZ\n\n\n\nNow this gets interesting! We see a lot of processes in \nzone1\n, and (no\nsurprise here) their PIDs are all the same that we just saw from inside\nthe zone. As for \nkzone1\n, we see only 2 processes. Just to check, let's\nuse \n-z zonename\n parameter:\n\n\nroot@solarislab:~# ps -f -Z -z kzone1\n    ZONE      UID   PID  PPID   C    STIME TTY         TIME CMD\n  kzone1     root 19817     1   0 11:55:11 ?           0:00 zsched\n  kzone1     root 19995 19817   0 11:55:12 ?           6:28 /usr/lib/kzhost\n\n\n\nThat's right! Just two \nkzone1\n's processes are visible from the global\nzone. Actually, kzhost is just a big process that keeps all the kernel\nzone's processes inside. How big is is? Yes, you guessed it right\nagain\nit takes the amount of memory that is configured for \nkzone1\n.\nYou can check it with the following \nps\n command:\n\n\nroot@solarislab:~# ps -o pid,vsz,rss,comm -z kzone1\n  PID  VSZ  RSS COMMAND\n19817    0    0 zsched\n19995 2120528 2119512 /usr/lib/kzhost\n\n\n\nThe bottom line for these experiments:\n\n\n\n\nnon-global zone's processes are visible from the global zone and\n    belong to its \nprocess space\n\n\nkernel zone's processes are visible only inside the zone and they\n    have \ntheir own\n process space\n\n\n\n\nWhen choosing between non-global zones and kernel zones, please keep in\nmind this major difference.\n\n\nProcessors and Memory\n\n\nLet's move on and figure out what resources are available in the zones.\nWhat is the output of \npsrinfo(1M)\n?\n\n\nroot@kzone1:~# psrinfo\n0       on-line   since 06/15/2015 15:16:12\n\n\n\nroot@zone1:~# psrinfo\n0       on-line   since 06/10/2015 17:30:04\n1       on-line   since 06/10/2015 17:30:07\n2       on-line   since 06/10/2015 17:30:07\n3       on-line   since 06/10/2015 17:30:07\n4       on-line   since 06/10/2015 17:30:07\n5       on-line   since 06/10/2015 17:30:07\n6       on-line   since 06/10/2015 17:30:07\n7       on-line   since 06/10/2015 17:30:07\n\n\n\nYou see the difference? By default Solaris 11.2 assigns only one virtual\nCPU to a kernel zone (you can change that with \nzonecfg\n command). In\nSolaris 11.3 this default has been changed to 4 virtual CPUs. A\nnon-global zone by default can use all the CPUs available in the global\nzone. Run the same \npsrinfo\n command in the global zone and compare the\nresults.\n\n\nWhat does it mean for us? Non-global zones have more flexibility in\nterms of resource control. By default you give them what's available and\nthey share all the resources with the global zone. Later you can \nlimit\n\ntheir resource usage or assign specific groups of CPUs. It's completely\nopposite in kernel zones. By default they get the minimum, just one\nvirtual CPU. You can change that if/when you need more, even during the\ninitial configuration phase.\n\n\nWhat about memory? How much is available? As you can see below, the\nsituation is very similar.\n\n\nroot@kzone1:~# prtconf | grep Memory\nMemory size: 2048 Megabytes\n\n\n\nroot@zone1:~# prtconf | grep Memory\nprtconf: devinfo facility not available\nMemory size: 16384 Megabytes\n\n\n\nroot@solarislab:~# prtconf | grep Memory\nMemory size: 16384 Megabytes\n\n\n\nThe same thing here: the non-global zone by default can use the whole\nphysical memory available in the global zone. For the kernel zone we\nhave to specify how much memory we want to assign to it. By default it's\n2 Gigabytes in \n11.2\n and 4 Gigabytes in \n11.3\n.\n\n\nVirtualization support\n\n\nAlso it's interesting to take a look at what \nvirtinfo(1M)\n reports in\neach case: kernel zone, non-global zone and global zone.\n\n\nroot@kzone1:~# virtinfo\nNAME            CLASS\nkernel-zone     current\nlogical-domain  parent\nnon-global-zone supported\n\n\n\nroot@zone1:~# virtinfo\nNAME            CLASS\nnon-global-zone current\nlogical-domain  parent\nlogical-domain  supported\n\n\n\nroot@solarislab:~# virtinfo\nNAME            CLASS\nlogical-domain  current\nnon-global-zone supported\nkernel-zone     supported\nlogical-domain  supported\n\n\n\nYou can see that you can even create non-global zones inside kernel\nzones! You can do that, but remember: \"Don't over-virtualize!\"\n\n\nAnother interesting test: try to run the \nzonename\n command in the\nglobal zone, kernel zone and non-global zone. What does it show in each\nof them?\n\n\nroot@solarislab:~# zonename\nglobal\n\n\n\nroot@kzone1:~# zonename\nglobal\n\n\n\nroot@zone1:~# zonename\nzone1\n\n\n\nInteresting! That means it's really hard to distinguish a kernel zone\nfrom a \"true\" global zone. Your applications will never notice that they\nare running in a virtualized environment.\n\n\nPackages\n\n\nOne of the great things about non-global zones is that you can manage\nsoftware packages in them both from inside and outside. You can install\npackages being logged in a non-global zone, but more importantly, you\ncan update, install and uninstall packages from the global zone.\nNormally, when you do \npkg update\n in the global zone, all installed\nnon-global zones are being updated too. You can install packages\nrecursively into the zones by using \npkg install -r\n command. Even more,\nyou can name the specific zones you want to install the packages into,\nwith \n-z \nzonename\n parameter, or exclude some zones with \n-Z\n\nzonename\n. What's important, you can do that even when the zones are\nnot running. That's a big advantage of non-global zones in software\nmanagement.\n\n\nAs you might have guessed already, it's different in kernel zones. As\nsoon as they have a separate block device with a file system in it, \nthat is not available from the global zone, we can't update their \npackages without logging into the kernel zone. All package-related \ncommands\ninstall, update, uninstall\nshould be performed \nfrom within the kernel zone.\n\n\nComparing software management operations in non-global zones and kernel\nzones, we can see that because non-global zones are more tightly\nintegrated with the global zone, it gives us more flexibility in package\noperations. Kernel zones are more isolated by design (separate kernel,\nZFS pool and process space), so they are treated as separate Solaris\ninstances. Which approach is better\nyou decide, depending on your\nparticular situation and requirements.",
            "title": "Solaris Zones Inside and Outside"
        },
        {
            "location": "/kz/inside_outside/index.html#processes",
            "text": "We start with some basic process-related commands. For example, how many\nprocesses are running just after the zone is booted?  root@kzone1:~# ps -ef | wc -l\n      56  root@zone1:~# ps -ef | wc -l\n      36  You see, there some extra processes running in the kernel zone. Let's\nsort them by PID and look at the first 20 of them:  root@kzone1:~# ps -ef | sort -k 2 | head -20\n    root     0     0   0 15:16:13 ?           0:00 sched\n    root     1     0   0 15:16:13 ?           0:00 /usr/sbin/init\n    root     2     0   0 15:16:13 ?           0:00 pageout\n    root     3     0   0 15:16:13 ?           0:01 fsflush\n    root     5     0   0 15:16:11 ?           0:03 zpool-rpool\n    root     6     0   0 15:16:13 ?           0:00 kmem_task\n    root     7     0   0 15:16:13 ?           0:00 intrd\n    root     8     0   0 15:16:13 ?           0:00 vmtasks\n    root     9     0   0 15:16:13 ?           0:00 postwaittq\n    root    13     1   0 15:16:16 ?           0:10 /lib/svc/bin/svc.startd\n    root    15     1   0 15:16:16 ?           2:03 /lib/svc/bin/svc.configd\n  netcfg    50     1   0 15:17:45 ?           0:00 /lib/inet/netcfgd\n   dladm    65     1   0 15:17:54 ?           0:00 /usr/sbin/dlmgmtd\n  netadm    84     1   0 15:18:06 ?           0:00 /lib/inet/ipmgmtd\n    root    97     1   0 15:18:08 ?           0:00 /lib/inet/in.mpathd\n    root   122     1   0 15:18:12 ?           0:00 /usr/lib/pfexecd\n    root   165     1   0 15:18:25 ?           0:00 /usr/lib/sysevent/syseventd\n  daemon   202     1   0 15:18:26 ?           0:00 /usr/lib/utmpd\n    root   217     1   0 15:18:27 ?           0:00 /usr/lib/zones/zonestatd\n    root   221     1   0 15:18:27 ?           0:00 /usr/lib/rad/rad -sp  root@zone1:~# ps -ef | sort -k 2 | head -20\n     UID   PID  PPID   C    STIME TTY         TIME CMD\n    root 21253 21253   0 14:28:07 ?           0:00 zsched\n    root 21971 21253   0 14:28:16 ?           0:00 /usr/sbin/init\n    root 22151 21253   0 14:28:16 ?           0:04 /lib/svc/bin/svc.startd\n    root 22153 21253   0 14:28:16 ?           1:06 /lib/svc/bin/svc.configd\n  netcfg 22186 21253   0 14:29:12 ?           0:00 /lib/inet/netcfgd\n  daemon 22229 21253   0 14:29:15 ?           0:00 /lib/crypto/kcfd\n    root 22236 21253   0 14:29:16 ?           0:00 /usr/lib/pfexecd\n  netadm 22252 21253   0 14:29:17 ?           0:00 /lib/inet/ipmgmtd\n    root 22266 21253   0 14:29:17 ?           0:00 /lib/inet/in.mpathd\n  daemon 22272 21253   0 14:29:18 ?           0:00 /usr/lib/utmpd\n    root 22287 21253   0 14:29:18 ?           0:00 /usr/lib/rad/rad -sp\n    root 22333 21253   0 14:29:18 ?           0:00 /usr/lib/dbus-daemon --system\n  netadm 22771 21253   0 14:30:43 ?           0:00 /lib/inet/nwamd\n    root 22892 21253   0 14:30:46 ?           0:00 /usr/lib/zones/zoneproxy-client -s localhost:1008\n    root 22899 21253   0 14:30:47 ?           0:00 /usr/sbin/nscd\n    root 22955 21253   0 14:30:49 ?           0:00 /usr/sbin/cron\n  daemon 23010 21253   0 14:30:51 ?           0:00 /usr/sbin/rpcbind\n    root 23040 21253   0 14:30:51 ?           0:00 /usr/lib/inet/in.ndpd\n    root 23048 21253   0 14:30:52 ?           0:00 /usr/lib/autofs/automountd  Now you see the big difference between kernel zones and non-global\nzones. In non-global zones all the processes are part of the global\nzone's process space, so their PIDs start with some big number. In\nkernel zones they start with  0  and the list is not much different from\nthe process list in the global zone.  Now let's look at them from the global zone's perspective. Use the  -Z \nswitch for the  ps  command to see zone's names next to the processes.  root@solarislab:~# ps -efZ\n    ZONE      UID   PID  PPID   C    STIME TTY         TIME CMD\n  global     root     0     0   0   Jul 29 ?           0:10 sched\n  global     root     5     0   0   Jul 29 ?           7:26 zpool-rpool\n  global     root     6     0   0   Jul 29 ?           0:31 kmem_task\n  global     root     1     0   0   Jul 29 ?           0:01 /usr/sbin/init\n  global     root     2     0   0   Jul 29 ?           0:00 pageout\n  global     root     3     0   0   Jul 29 ?          11:32 fsflush\n  global     root     7     0   0   Jul 29 ?           0:12 intrd\n  global     root     8     0   0   Jul 29 ?           1:02 vmtasks\n. . . (skipped) . . .\n   zone1     root 21253     1   0 12:28:07 ?           0:00 zsched\n  global     root  1214     1   0   Jul 29 ?           0:03 /usr/lib/ep/eptelemon\n  global     root  1116     1   0   Jul 29 ?           0:21 /usr/lib/ep/epdetector dcdc cpu0\n  kzone1     root 19817     1   0 11:55:11 ?           0:00 zsched\n. . . (skipped) . . .\n  kzone1     root 19995 19817   0 11:55:12 ?           6:23 /usr/lib/kzhost\n  global  pkg5srv 20737     1   0 12:19:48 ?           0:16 /usr/apache2/2.2/bin/htcacheclean -d20160 -i -l 2048M -n -p /var/cache/pkg/sysr\n  global  pkg5srv 20742 20739   0 12:19:49 ?           0:21 /usr/apache2/2.2/bin/64/httpd.worker -f /system/volatile/pkg/sysrepo/sysrepo_ht\n  global  pkg5srv 20743 20739   0 12:19:49 ?           0:22 /usr/apache2/2.2/bin/64/httpd.worker -f /system/volatile/pkg/sysrepo/sysrepo_ht\n   zone1     root 22151     1   0 12:28:16 ?           0:04 /lib/svc/bin/svc.startd\n   zone1   netcfg 22186     1   0 12:29:12 ?           0:00 /lib/inet/netcfgd\n  global  pkg5srv 20746     1   0 12:19:50 ?           0:00 /usr/lib/zones/zoneproxyd\n   zone1     root 22153     1   0 12:28:16 ?           1:06 /lib/svc/bin/svc.configd\n  global     root 21061     1   0 12:28:04 ?           0:01 zoneadmd -z zone1\n   zone1     root 21971 21253   0 12:28:16 ?           0:00 /usr/sbin/init\n   zone1     root 22266     1   0 12:29:17 ?           0:00 /lib/inet/in.mpathd\n. . . (skipped) . . .\n  global     root 23168 14852   0 12:32:01 pts/2       0:00 ps -efZ  Now this gets interesting! We see a lot of processes in  zone1 , and (no\nsurprise here) their PIDs are all the same that we just saw from inside\nthe zone. As for  kzone1 , we see only 2 processes. Just to check, let's\nuse  -z zonename  parameter:  root@solarislab:~# ps -f -Z -z kzone1\n    ZONE      UID   PID  PPID   C    STIME TTY         TIME CMD\n  kzone1     root 19817     1   0 11:55:11 ?           0:00 zsched\n  kzone1     root 19995 19817   0 11:55:12 ?           6:28 /usr/lib/kzhost  That's right! Just two  kzone1 's processes are visible from the global\nzone. Actually, kzhost is just a big process that keeps all the kernel\nzone's processes inside. How big is is? Yes, you guessed it right\nagain it takes the amount of memory that is configured for  kzone1 .\nYou can check it with the following  ps  command:  root@solarislab:~# ps -o pid,vsz,rss,comm -z kzone1\n  PID  VSZ  RSS COMMAND\n19817    0    0 zsched\n19995 2120528 2119512 /usr/lib/kzhost  The bottom line for these experiments:   non-global zone's processes are visible from the global zone and\n    belong to its  process space  kernel zone's processes are visible only inside the zone and they\n    have  their own  process space   When choosing between non-global zones and kernel zones, please keep in\nmind this major difference.",
            "title": "Processes"
        },
        {
            "location": "/kz/inside_outside/index.html#processors-and-memory",
            "text": "Let's move on and figure out what resources are available in the zones.\nWhat is the output of  psrinfo(1M) ?  root@kzone1:~# psrinfo\n0       on-line   since 06/15/2015 15:16:12  root@zone1:~# psrinfo\n0       on-line   since 06/10/2015 17:30:04\n1       on-line   since 06/10/2015 17:30:07\n2       on-line   since 06/10/2015 17:30:07\n3       on-line   since 06/10/2015 17:30:07\n4       on-line   since 06/10/2015 17:30:07\n5       on-line   since 06/10/2015 17:30:07\n6       on-line   since 06/10/2015 17:30:07\n7       on-line   since 06/10/2015 17:30:07  You see the difference? By default Solaris 11.2 assigns only one virtual\nCPU to a kernel zone (you can change that with  zonecfg  command). In\nSolaris 11.3 this default has been changed to 4 virtual CPUs. A\nnon-global zone by default can use all the CPUs available in the global\nzone. Run the same  psrinfo  command in the global zone and compare the\nresults.  What does it mean for us? Non-global zones have more flexibility in\nterms of resource control. By default you give them what's available and\nthey share all the resources with the global zone. Later you can  limit \ntheir resource usage or assign specific groups of CPUs. It's completely\nopposite in kernel zones. By default they get the minimum, just one\nvirtual CPU. You can change that if/when you need more, even during the\ninitial configuration phase.  What about memory? How much is available? As you can see below, the\nsituation is very similar.  root@kzone1:~# prtconf | grep Memory\nMemory size: 2048 Megabytes  root@zone1:~# prtconf | grep Memory\nprtconf: devinfo facility not available\nMemory size: 16384 Megabytes  root@solarislab:~# prtconf | grep Memory\nMemory size: 16384 Megabytes  The same thing here: the non-global zone by default can use the whole\nphysical memory available in the global zone. For the kernel zone we\nhave to specify how much memory we want to assign to it. By default it's\n2 Gigabytes in  11.2  and 4 Gigabytes in  11.3 .",
            "title": "Processors and Memory"
        },
        {
            "location": "/kz/inside_outside/index.html#virtualization-support",
            "text": "Also it's interesting to take a look at what  virtinfo(1M)  reports in\neach case: kernel zone, non-global zone and global zone.  root@kzone1:~# virtinfo\nNAME            CLASS\nkernel-zone     current\nlogical-domain  parent\nnon-global-zone supported  root@zone1:~# virtinfo\nNAME            CLASS\nnon-global-zone current\nlogical-domain  parent\nlogical-domain  supported  root@solarislab:~# virtinfo\nNAME            CLASS\nlogical-domain  current\nnon-global-zone supported\nkernel-zone     supported\nlogical-domain  supported  You can see that you can even create non-global zones inside kernel\nzones! You can do that, but remember: \"Don't over-virtualize!\"  Another interesting test: try to run the  zonename  command in the\nglobal zone, kernel zone and non-global zone. What does it show in each\nof them?  root@solarislab:~# zonename\nglobal  root@kzone1:~# zonename\nglobal  root@zone1:~# zonename\nzone1  Interesting! That means it's really hard to distinguish a kernel zone\nfrom a \"true\" global zone. Your applications will never notice that they\nare running in a virtualized environment.",
            "title": "Virtualization support"
        },
        {
            "location": "/kz/inside_outside/index.html#packages",
            "text": "One of the great things about non-global zones is that you can manage\nsoftware packages in them both from inside and outside. You can install\npackages being logged in a non-global zone, but more importantly, you\ncan update, install and uninstall packages from the global zone.\nNormally, when you do  pkg update  in the global zone, all installed\nnon-global zones are being updated too. You can install packages\nrecursively into the zones by using  pkg install -r  command. Even more,\nyou can name the specific zones you want to install the packages into,\nwith  -z  zonename  parameter, or exclude some zones with  -Z zonename . What's important, you can do that even when the zones are\nnot running. That's a big advantage of non-global zones in software\nmanagement.  As you might have guessed already, it's different in kernel zones. As\nsoon as they have a separate block device with a file system in it, \nthat is not available from the global zone, we can't update their \npackages without logging into the kernel zone. All package-related \ncommands install, update, uninstall should be performed \nfrom within the kernel zone.  Comparing software management operations in non-global zones and kernel\nzones, we can see that because non-global zones are more tightly\nintegrated with the global zone, it gives us more flexibility in package\noperations. Kernel zones are more isolated by design (separate kernel,\nZFS pool and process space), so they are treated as separate Solaris\ninstances. Which approach is better you decide, depending on your\nparticular situation and requirements.",
            "title": "Packages"
        },
        {
            "location": "/kz/kz_suspend/index.html",
            "text": "One of the coolest Kernel Zones features is their ability to suspend and\nresume operations. You can use this feature either to \nwarm migrate\n a\nkernel zone from one system to another, or to preserve your\napplication's state while rebooting the global zone.\n\n\nTo be able to use this capability we have to configure the storage space\nwhere we keep the suspended zone. First check if it's configured already\nand then add the path to file's location.\n\n\nroot@solarislab:~# zonecfg -z kzone1 info suspend\nroot@solarislab:~# zonecfg -z kzone1\nzonecfg:kzone1\n add suspend\nzonecfg:kzone1:suspend\n set path=/system/zones/kzone1/suspend\nzonecfg:kzone1:suspend\n end\nzonecfg:kzone1\n exit\nroot@solarislab:~# zonecfg -z kzone1 info suspend\nsuspend:\n        path: /system/zones/kzone1/suspend\n        storage not specified\n\n\n\nGood. Now we can try to suspend the kernel zone.\n\n\nroot@solarislab:~# zoneadm -z kzone1 suspend\nroot@solarislab:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n   5 zone1            running     /system/zones/zone1          solaris    excl\n   - kzone1           installed   -                            solaris-kz excl\n\n\n\nSo what? It doesn't look very spectacular, does it? Note that \nkzone1\n\nis not in any special \nsuspended\n status, it's just \ninstalled\n, like if\nwe just shut it down. How can we resume it? OK, according to the zones'\nlifecycle, from the \ninstalled\n state we can \nboot\n the zone. There is\nno special \nresume\n command.\n\n\nroot@solarislab:~# zoneadm -z kzone1 boot\nroot@solarislab:~# zoneadm list -cv\n  ID NAME             STATUS      PATH                         BRAND      IP\n   0 global           running     /                            solaris    shared\n   5 zone1            running     /system/zones/zone1          solaris    excl\n   7 kzone1           running     -                            solaris-kz excl\n\n\n\nAgain, nothing spectacular. We need something to see how the zone is\ngetting suspended and resumed. Open another terminal window, login to\nthe host (global zone) and then to \nkzone1\n. Inside \nkzone1\n start a\nsimple script that prints out the current timestamp every second.\n\n\nroot@solarislab:~# zlogin -C kzone1\n[Connected to zone 'kzone1' console]\n\nkzone11-3 console login: lab\nPassword: oracle1 (not shown)\nLast login: Tue Aug  4 15:09:04 2015 on console\nOracle Corporation      SunOS 5.11      11.3    June 2015\nlab@kzone11-3:~$ while true; do date ; sleep 1 ; done\nWednesday, August  5, 2015 02:35:46 PM EDT\nWednesday, August  5, 2015 02:35:47 PM EDT\nWednesday, August  5, 2015 02:35:48 PM EDT\nWednesday, August  5, 2015 02:35:49 PM EDT\nWednesday, August  5, 2015 02:35:50 PM EDT\nWednesday, August  5, 2015 02:35:51 PM EDT\n. . . (it goes on and on and on)\n\n\n\nNow switch to the previous terminal session in the global zone and\nsuspend and resume \nkzone1\n again, while watching what's going on in the\nsecond window (in the \nkzone1\n).\n\n\nroot@solarislab:~# zoneadm -z kzone1 suspend\nroot@solarislab:~# zoneadm -z kzone1 boot\n\n\n\nIn the second window at the same time:\n\n\nlab@kzone1:~$ while true; do date ; sleep 1 ; done\nWednesday, August  5, 2015 02:42:04 PM EDT\nWednesday, August  5, 2015 02:42:05 PM EDT\nWednesday, August  5, 2015 02:42:06 PM EDT\nWednesday, August  5, 2015 02:42:07 PM EDT\nWednesday, August  5, 2015 02:42:08 PM EDT\nWednesday, August  5, 2015 02:42:09 PM EDT\nWednesday, August  5, 2015 02:42:10 PM EDT\n\n[NOTICE: Zone suspending]\nOSQ: suspending devices...\n        suspending ORCL,kz-zvterm@0 (aka zvterm)\n        suspending ORCL,kz-zvblk@0 (aka zvblk)\n        suspending zvcntrl@0\n        suspending zvnet@0\n        suspending zvsdir@0\n        suspending ORCL,kz-extenders@ff (aka zvnex)\n\n[NOTICE: Zone halted]\n\n[NOTICE: Zone resuming]\n        resuming ORCL,kz-extenders@ff (aka zvnex)\n        resuming zvsdir@0\n        resuming zvnet@0\n        resuming zvcntrl@0\n        resuming ORCL,kz-zvblk@0 (aka zvblk)\n        resuming ORCL,kz-zvterm@0 (aka zvterm)\nOSQ: resume devices completed\nWednesday, August  5, 2015 02:42:11 PM EDT\nWednesday, August  5, 2015 02:42:12 PM EDT\nWednesday, August  5, 2015 02:42:13 PM EDT\nWednesday, August  5, 2015 02:42:14 PM EDT\nWednesday, August  5, 2015 02:42:15 PM EDT\nWednesday, August  5, 2015 02:42:16 PM EDT\n^C\n\n\n\nThis is really interesting! Now we see that the zone has been suspended\nand then resumed and continued the script from the point where it was\nleft. That was definitely not possible with traditional non-global zones\nand now we can do this with kernel zones. Think about it: how can you\nuse it with your applications? What might be a good test to see if they\nare really suspended and resumed?",
            "title": "Kernel Zones Suspend and Resume"
        },
        {
            "location": "/kz/kz_clone/index.html",
            "text": "Sometimes we want to create a zone, install and configure applications\nin it and use it as a template: whether it's for scaling out the\napplication (e.g. load balancing), or for distributing it to other\nlocations, or for using it for testing and training purposes. There are\nseveral different ways to do that. You can use different technologies\nfor that:\n\n\n\n\nzone cloning feature (\nzoneadm clone\n command)\n\n\nAutomated Installer mechanisms (manifests and profiles)\n\n\nUnified Archives (available since \n11.2\n)\n\n\n\n\nIn this lab we will explore the third method as it's the most flexible one.\n\n\nSo, we have a kernel zone already and we want to create a clone of it.\nFirst, we create a Unified Archive:\n\n\nroot@solarislab:~# archiveadm create /share1/uar/kzone2.uar -z kzone2\n\n\n\nThen we create a new kernel zone with zonecfg, but we use the UAR as a\ntemplate:\n\n\nroot@solarislab:~# zonecfg -z kzone3 create -a /share1/uar/kzone2.uar -z kzone2\n\n\n\nAnd then install the zone from that UAR:\n\n\nroot@solarislab:~# zoneadm -z kzone3 install -a /share1/uar/kzone2.uar -z kzone2\n\n\n\nThat's it! Now you are going to boot the zone, login to its console and\ngo through the initial configuration process. Remember, when installing\nfrom a UAR, we remove all individual parameters like hostname, IP\naddress and root password. Sometimes it's more convenient to create this\nprofile beforehand with \nsysconfig create-profile\n and then use it\nduring installation. In that case, your new zone will be ready to use\nafter the first boot.",
            "title": "Kernel Zone Cloning"
        }
    ]
}